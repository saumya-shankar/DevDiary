{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XRJyQOnOKi05"
   },
   "outputs": [],
   "source": [
    "##This notebook is based on https://keras.io/examples/vision/siamese_network/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ft813bZmcZBu",
    "outputId": "c2afc538-957e-43db-b0e2-d4393957d865"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Apr  5 14:07:06 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   37C    P8     9W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "gpu_info = !nvidia-smi\n",
    "gpu_info = '\\n'.join(gpu_info)\n",
    "if gpu_info.find('failed') >= 0:\n",
    "  print('Not connected to a GPU')\n",
    "else:\n",
    "  print(gpu_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AjuA_pPmXFU1"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-2XGD84hcarD",
    "outputId": "c6a319a3-546f-4bfc-8a04-72129c15cfef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "#mounting google drive on colab\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Oqzndv0MbGbD"
   },
   "outputs": [],
   "source": [
    "train_dataset=pd.read_csv('./drive/MyDrive/best_model_28_representation_train_040422.csv').iloc[:,1:]\n",
    "val_dataset=pd.read_csv('./drive/MyDrive/best_model_28_representation_val_040422.csv').iloc[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "id": "pSUr9l6aZu0u",
    "outputId": "84876d60-a8a0-4630-b161-4222c6a03899"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-c5d1bf26-d94d-41a5-a567-622e826d77f0\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>2039</th>\n",
       "      <th>2040</th>\n",
       "      <th>2041</th>\n",
       "      <th>2042</th>\n",
       "      <th>2043</th>\n",
       "      <th>2044</th>\n",
       "      <th>2045</th>\n",
       "      <th>2046</th>\n",
       "      <th>2047</th>\n",
       "      <th>category_class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.313336</td>\n",
       "      <td>0.025133</td>\n",
       "      <td>0.229388</td>\n",
       "      <td>0.136946</td>\n",
       "      <td>0.034979</td>\n",
       "      <td>0.120104</td>\n",
       "      <td>0.196935</td>\n",
       "      <td>0.023299</td>\n",
       "      <td>0.166517</td>\n",
       "      <td>...</td>\n",
       "      <td>0.561781</td>\n",
       "      <td>0.001802</td>\n",
       "      <td>0.210695</td>\n",
       "      <td>0.161759</td>\n",
       "      <td>0.020894</td>\n",
       "      <td>0.524201</td>\n",
       "      <td>0.353692</td>\n",
       "      <td>0.020171</td>\n",
       "      <td>0.403670</td>\n",
       "      <td>338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.093444</td>\n",
       "      <td>0.316758</td>\n",
       "      <td>0.170155</td>\n",
       "      <td>0.137241</td>\n",
       "      <td>0.259935</td>\n",
       "      <td>0.474877</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.194750</td>\n",
       "      <td>0.067905</td>\n",
       "      <td>0.051878</td>\n",
       "      <td>...</td>\n",
       "      <td>1.161999</td>\n",
       "      <td>0.064962</td>\n",
       "      <td>0.178196</td>\n",
       "      <td>0.366895</td>\n",
       "      <td>0.073961</td>\n",
       "      <td>0.799406</td>\n",
       "      <td>0.238650</td>\n",
       "      <td>0.013355</td>\n",
       "      <td>0.226344</td>\n",
       "      <td>338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.005281</td>\n",
       "      <td>0.349727</td>\n",
       "      <td>0.094547</td>\n",
       "      <td>0.376952</td>\n",
       "      <td>0.304110</td>\n",
       "      <td>0.114644</td>\n",
       "      <td>0.173896</td>\n",
       "      <td>0.426651</td>\n",
       "      <td>0.051706</td>\n",
       "      <td>0.172470</td>\n",
       "      <td>...</td>\n",
       "      <td>0.607481</td>\n",
       "      <td>0.092371</td>\n",
       "      <td>0.017004</td>\n",
       "      <td>0.155562</td>\n",
       "      <td>0.018352</td>\n",
       "      <td>0.468657</td>\n",
       "      <td>0.072951</td>\n",
       "      <td>0.036000</td>\n",
       "      <td>0.496107</td>\n",
       "      <td>338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.089383</td>\n",
       "      <td>0.107351</td>\n",
       "      <td>0.075641</td>\n",
       "      <td>0.013774</td>\n",
       "      <td>0.096981</td>\n",
       "      <td>0.001354</td>\n",
       "      <td>0.072457</td>\n",
       "      <td>0.125850</td>\n",
       "      <td>0.032342</td>\n",
       "      <td>0.126957</td>\n",
       "      <td>...</td>\n",
       "      <td>0.416517</td>\n",
       "      <td>0.064198</td>\n",
       "      <td>0.024564</td>\n",
       "      <td>0.143060</td>\n",
       "      <td>0.013859</td>\n",
       "      <td>0.805468</td>\n",
       "      <td>0.205996</td>\n",
       "      <td>0.024247</td>\n",
       "      <td>0.788354</td>\n",
       "      <td>338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.013804</td>\n",
       "      <td>0.034383</td>\n",
       "      <td>0.114037</td>\n",
       "      <td>0.031434</td>\n",
       "      <td>0.449520</td>\n",
       "      <td>0.003940</td>\n",
       "      <td>0.158426</td>\n",
       "      <td>0.118010</td>\n",
       "      <td>0.004489</td>\n",
       "      <td>0.047437</td>\n",
       "      <td>...</td>\n",
       "      <td>0.264046</td>\n",
       "      <td>0.103955</td>\n",
       "      <td>0.075154</td>\n",
       "      <td>0.008518</td>\n",
       "      <td>0.005747</td>\n",
       "      <td>0.603387</td>\n",
       "      <td>0.416539</td>\n",
       "      <td>0.005948</td>\n",
       "      <td>0.242247</td>\n",
       "      <td>338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39995</th>\n",
       "      <td>0.008469</td>\n",
       "      <td>0.153707</td>\n",
       "      <td>0.295961</td>\n",
       "      <td>0.640903</td>\n",
       "      <td>0.147320</td>\n",
       "      <td>0.107286</td>\n",
       "      <td>0.033159</td>\n",
       "      <td>0.055960</td>\n",
       "      <td>0.008775</td>\n",
       "      <td>0.000215</td>\n",
       "      <td>...</td>\n",
       "      <td>0.466908</td>\n",
       "      <td>0.157604</td>\n",
       "      <td>0.001789</td>\n",
       "      <td>0.313070</td>\n",
       "      <td>0.021882</td>\n",
       "      <td>0.128249</td>\n",
       "      <td>0.420876</td>\n",
       "      <td>0.125631</td>\n",
       "      <td>0.086492</td>\n",
       "      <td>591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39996</th>\n",
       "      <td>0.010601</td>\n",
       "      <td>0.295052</td>\n",
       "      <td>0.014810</td>\n",
       "      <td>0.608878</td>\n",
       "      <td>0.032543</td>\n",
       "      <td>0.013807</td>\n",
       "      <td>0.200572</td>\n",
       "      <td>0.015779</td>\n",
       "      <td>0.207144</td>\n",
       "      <td>0.473168</td>\n",
       "      <td>...</td>\n",
       "      <td>0.044090</td>\n",
       "      <td>0.042290</td>\n",
       "      <td>0.077591</td>\n",
       "      <td>0.028302</td>\n",
       "      <td>0.023190</td>\n",
       "      <td>0.081915</td>\n",
       "      <td>0.164003</td>\n",
       "      <td>0.035183</td>\n",
       "      <td>0.097715</td>\n",
       "      <td>591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39997</th>\n",
       "      <td>0.192546</td>\n",
       "      <td>0.433518</td>\n",
       "      <td>0.085116</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.819737</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.049733</td>\n",
       "      <td>0.018173</td>\n",
       "      <td>0.015861</td>\n",
       "      <td>0.194425</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004095</td>\n",
       "      <td>0.006790</td>\n",
       "      <td>0.154554</td>\n",
       "      <td>0.153198</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.522131</td>\n",
       "      <td>0.000262</td>\n",
       "      <td>0.001606</td>\n",
       "      <td>591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39998</th>\n",
       "      <td>0.052466</td>\n",
       "      <td>1.688614</td>\n",
       "      <td>0.070324</td>\n",
       "      <td>0.110022</td>\n",
       "      <td>0.126059</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.095915</td>\n",
       "      <td>0.001023</td>\n",
       "      <td>0.010808</td>\n",
       "      <td>0.105967</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.058327</td>\n",
       "      <td>0.070632</td>\n",
       "      <td>0.421258</td>\n",
       "      <td>0.018547</td>\n",
       "      <td>0.004673</td>\n",
       "      <td>0.220298</td>\n",
       "      <td>0.007178</td>\n",
       "      <td>0.071261</td>\n",
       "      <td>591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39999</th>\n",
       "      <td>0.025741</td>\n",
       "      <td>0.752354</td>\n",
       "      <td>0.080995</td>\n",
       "      <td>0.015271</td>\n",
       "      <td>0.802826</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000496</td>\n",
       "      <td>0.077876</td>\n",
       "      <td>0.074441</td>\n",
       "      <td>0.062494</td>\n",
       "      <td>...</td>\n",
       "      <td>0.019828</td>\n",
       "      <td>0.070384</td>\n",
       "      <td>0.113285</td>\n",
       "      <td>0.291323</td>\n",
       "      <td>0.541946</td>\n",
       "      <td>0.457819</td>\n",
       "      <td>0.277326</td>\n",
       "      <td>0.044750</td>\n",
       "      <td>0.076066</td>\n",
       "      <td>591</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>40000 rows × 2049 columns</p>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c5d1bf26-d94d-41a5-a567-622e826d77f0')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-c5d1bf26-d94d-41a5-a567-622e826d77f0 button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-c5d1bf26-d94d-41a5-a567-622e826d77f0');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "              0         1         2         3         4         5         6  \\\n",
       "0      0.033333  0.313336  0.025133  0.229388  0.136946  0.034979  0.120104   \n",
       "1      0.093444  0.316758  0.170155  0.137241  0.259935  0.474877  0.000000   \n",
       "2      0.005281  0.349727  0.094547  0.376952  0.304110  0.114644  0.173896   \n",
       "3      0.089383  0.107351  0.075641  0.013774  0.096981  0.001354  0.072457   \n",
       "4      0.013804  0.034383  0.114037  0.031434  0.449520  0.003940  0.158426   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "39995  0.008469  0.153707  0.295961  0.640903  0.147320  0.107286  0.033159   \n",
       "39996  0.010601  0.295052  0.014810  0.608878  0.032543  0.013807  0.200572   \n",
       "39997  0.192546  0.433518  0.085116  0.000000  0.819737  0.000000  0.049733   \n",
       "39998  0.052466  1.688614  0.070324  0.110022  0.126059  0.000000  0.095915   \n",
       "39999  0.025741  0.752354  0.080995  0.015271  0.802826  0.000000  0.000496   \n",
       "\n",
       "              7         8         9  ...      2039      2040      2041  \\\n",
       "0      0.196935  0.023299  0.166517  ...  0.561781  0.001802  0.210695   \n",
       "1      0.194750  0.067905  0.051878  ...  1.161999  0.064962  0.178196   \n",
       "2      0.426651  0.051706  0.172470  ...  0.607481  0.092371  0.017004   \n",
       "3      0.125850  0.032342  0.126957  ...  0.416517  0.064198  0.024564   \n",
       "4      0.118010  0.004489  0.047437  ...  0.264046  0.103955  0.075154   \n",
       "...         ...       ...       ...  ...       ...       ...       ...   \n",
       "39995  0.055960  0.008775  0.000215  ...  0.466908  0.157604  0.001789   \n",
       "39996  0.015779  0.207144  0.473168  ...  0.044090  0.042290  0.077591   \n",
       "39997  0.018173  0.015861  0.194425  ...  0.000000  0.004095  0.006790   \n",
       "39998  0.001023  0.010808  0.105967  ...  0.000000  0.058327  0.070632   \n",
       "39999  0.077876  0.074441  0.062494  ...  0.019828  0.070384  0.113285   \n",
       "\n",
       "           2042      2043      2044      2045      2046      2047  \\\n",
       "0      0.161759  0.020894  0.524201  0.353692  0.020171  0.403670   \n",
       "1      0.366895  0.073961  0.799406  0.238650  0.013355  0.226344   \n",
       "2      0.155562  0.018352  0.468657  0.072951  0.036000  0.496107   \n",
       "3      0.143060  0.013859  0.805468  0.205996  0.024247  0.788354   \n",
       "4      0.008518  0.005747  0.603387  0.416539  0.005948  0.242247   \n",
       "...         ...       ...       ...       ...       ...       ...   \n",
       "39995  0.313070  0.021882  0.128249  0.420876  0.125631  0.086492   \n",
       "39996  0.028302  0.023190  0.081915  0.164003  0.035183  0.097715   \n",
       "39997  0.154554  0.153198  0.000000  0.522131  0.000262  0.001606   \n",
       "39998  0.421258  0.018547  0.004673  0.220298  0.007178  0.071261   \n",
       "39999  0.291323  0.541946  0.457819  0.277326  0.044750  0.076066   \n",
       "\n",
       "       category_class  \n",
       "0                 338  \n",
       "1                 338  \n",
       "2                 338  \n",
       "3                 338  \n",
       "4                 338  \n",
       "...               ...  \n",
       "39995             591  \n",
       "39996             591  \n",
       "39997             591  \n",
       "39998             591  \n",
       "39999             591  \n",
       "\n",
       "[40000 rows x 2049 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "id": "4fvcX-0OZwkm",
    "outputId": "5545bbc1-f7e4-4a6e-c799-e135dbb52c4c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-6d030b29-1790-48d5-b5e3-d130ae160cfe\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>2039</th>\n",
       "      <th>2040</th>\n",
       "      <th>2041</th>\n",
       "      <th>2042</th>\n",
       "      <th>2043</th>\n",
       "      <th>2044</th>\n",
       "      <th>2045</th>\n",
       "      <th>2046</th>\n",
       "      <th>2047</th>\n",
       "      <th>category_class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.037247</td>\n",
       "      <td>0.084723</td>\n",
       "      <td>0.067953</td>\n",
       "      <td>0.148163</td>\n",
       "      <td>0.190729</td>\n",
       "      <td>0.034427</td>\n",
       "      <td>0.010865</td>\n",
       "      <td>0.190755</td>\n",
       "      <td>0.040758</td>\n",
       "      <td>0.071579</td>\n",
       "      <td>...</td>\n",
       "      <td>0.413018</td>\n",
       "      <td>0.040352</td>\n",
       "      <td>0.029557</td>\n",
       "      <td>0.017327</td>\n",
       "      <td>0.082976</td>\n",
       "      <td>0.208654</td>\n",
       "      <td>0.213978</td>\n",
       "      <td>0.007254</td>\n",
       "      <td>0.369544</td>\n",
       "      <td>338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.006566</td>\n",
       "      <td>0.147232</td>\n",
       "      <td>0.117783</td>\n",
       "      <td>0.476772</td>\n",
       "      <td>0.533292</td>\n",
       "      <td>0.588703</td>\n",
       "      <td>0.131454</td>\n",
       "      <td>0.441281</td>\n",
       "      <td>0.010310</td>\n",
       "      <td>0.016984</td>\n",
       "      <td>...</td>\n",
       "      <td>0.224854</td>\n",
       "      <td>0.045422</td>\n",
       "      <td>0.151592</td>\n",
       "      <td>0.346858</td>\n",
       "      <td>0.000291</td>\n",
       "      <td>0.781311</td>\n",
       "      <td>0.047355</td>\n",
       "      <td>0.008029</td>\n",
       "      <td>0.237799</td>\n",
       "      <td>338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.210000</td>\n",
       "      <td>0.083216</td>\n",
       "      <td>0.107389</td>\n",
       "      <td>0.373491</td>\n",
       "      <td>0.139399</td>\n",
       "      <td>0.228461</td>\n",
       "      <td>0.224466</td>\n",
       "      <td>0.400738</td>\n",
       "      <td>0.035322</td>\n",
       "      <td>0.065952</td>\n",
       "      <td>...</td>\n",
       "      <td>0.340572</td>\n",
       "      <td>0.006806</td>\n",
       "      <td>0.127561</td>\n",
       "      <td>0.045913</td>\n",
       "      <td>0.742372</td>\n",
       "      <td>0.063517</td>\n",
       "      <td>0.105330</td>\n",
       "      <td>0.058667</td>\n",
       "      <td>0.092687</td>\n",
       "      <td>338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.089227</td>\n",
       "      <td>0.028722</td>\n",
       "      <td>0.023529</td>\n",
       "      <td>0.516543</td>\n",
       "      <td>0.357591</td>\n",
       "      <td>1.165964</td>\n",
       "      <td>0.149411</td>\n",
       "      <td>0.075871</td>\n",
       "      <td>0.022748</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.272945</td>\n",
       "      <td>0.010554</td>\n",
       "      <td>1.335347</td>\n",
       "      <td>0.072965</td>\n",
       "      <td>0.002853</td>\n",
       "      <td>0.189264</td>\n",
       "      <td>0.011270</td>\n",
       "      <td>0.085637</td>\n",
       "      <td>0.058687</td>\n",
       "      <td>338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.005663</td>\n",
       "      <td>0.135440</td>\n",
       "      <td>0.112919</td>\n",
       "      <td>0.115065</td>\n",
       "      <td>0.094258</td>\n",
       "      <td>0.071284</td>\n",
       "      <td>0.024563</td>\n",
       "      <td>0.129603</td>\n",
       "      <td>0.003601</td>\n",
       "      <td>0.212501</td>\n",
       "      <td>...</td>\n",
       "      <td>0.402290</td>\n",
       "      <td>0.103481</td>\n",
       "      <td>0.176257</td>\n",
       "      <td>0.158090</td>\n",
       "      <td>0.010789</td>\n",
       "      <td>0.546413</td>\n",
       "      <td>0.319652</td>\n",
       "      <td>0.083233</td>\n",
       "      <td>0.016363</td>\n",
       "      <td>338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>0.000638</td>\n",
       "      <td>0.113700</td>\n",
       "      <td>0.108743</td>\n",
       "      <td>0.090572</td>\n",
       "      <td>0.055815</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.082937</td>\n",
       "      <td>0.025062</td>\n",
       "      <td>0.003649</td>\n",
       "      <td>0.045059</td>\n",
       "      <td>...</td>\n",
       "      <td>0.713404</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009588</td>\n",
       "      <td>0.051383</td>\n",
       "      <td>0.411699</td>\n",
       "      <td>0.089952</td>\n",
       "      <td>0.282519</td>\n",
       "      <td>0.000132</td>\n",
       "      <td>0.007038</td>\n",
       "      <td>591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>0.059231</td>\n",
       "      <td>0.211131</td>\n",
       "      <td>0.201713</td>\n",
       "      <td>0.032353</td>\n",
       "      <td>0.048716</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.077469</td>\n",
       "      <td>0.013222</td>\n",
       "      <td>0.060250</td>\n",
       "      <td>...</td>\n",
       "      <td>0.411932</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.028635</td>\n",
       "      <td>0.034902</td>\n",
       "      <td>0.292674</td>\n",
       "      <td>0.112189</td>\n",
       "      <td>0.008453</td>\n",
       "      <td>0.048728</td>\n",
       "      <td>591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>0.001761</td>\n",
       "      <td>1.463821</td>\n",
       "      <td>0.011850</td>\n",
       "      <td>0.032598</td>\n",
       "      <td>0.937367</td>\n",
       "      <td>0.021707</td>\n",
       "      <td>0.103139</td>\n",
       "      <td>0.006572</td>\n",
       "      <td>0.019002</td>\n",
       "      <td>0.021300</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009892</td>\n",
       "      <td>0.071254</td>\n",
       "      <td>0.661938</td>\n",
       "      <td>0.006536</td>\n",
       "      <td>0.265005</td>\n",
       "      <td>0.174313</td>\n",
       "      <td>0.000492</td>\n",
       "      <td>0.299318</td>\n",
       "      <td>591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>0.026339</td>\n",
       "      <td>0.870153</td>\n",
       "      <td>0.044403</td>\n",
       "      <td>0.028958</td>\n",
       "      <td>0.081213</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.152911</td>\n",
       "      <td>0.002433</td>\n",
       "      <td>0.006991</td>\n",
       "      <td>0.132442</td>\n",
       "      <td>...</td>\n",
       "      <td>0.021862</td>\n",
       "      <td>0.001303</td>\n",
       "      <td>0.022319</td>\n",
       "      <td>0.043793</td>\n",
       "      <td>0.260402</td>\n",
       "      <td>0.007712</td>\n",
       "      <td>0.034186</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.067226</td>\n",
       "      <td>591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>0.028445</td>\n",
       "      <td>0.396974</td>\n",
       "      <td>0.129699</td>\n",
       "      <td>0.304854</td>\n",
       "      <td>0.032658</td>\n",
       "      <td>0.027572</td>\n",
       "      <td>0.009164</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.114894</td>\n",
       "      <td>0.589555</td>\n",
       "      <td>...</td>\n",
       "      <td>0.069370</td>\n",
       "      <td>0.034535</td>\n",
       "      <td>0.012728</td>\n",
       "      <td>0.155390</td>\n",
       "      <td>0.008744</td>\n",
       "      <td>0.136618</td>\n",
       "      <td>0.080729</td>\n",
       "      <td>0.018973</td>\n",
       "      <td>0.069819</td>\n",
       "      <td>591</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 2049 columns</p>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6d030b29-1790-48d5-b5e3-d130ae160cfe')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-6d030b29-1790-48d5-b5e3-d130ae160cfe button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-6d030b29-1790-48d5-b5e3-d130ae160cfe');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "             0         1         2         3         4         5         6  \\\n",
       "0     0.037247  0.084723  0.067953  0.148163  0.190729  0.034427  0.010865   \n",
       "1     0.006566  0.147232  0.117783  0.476772  0.533292  0.588703  0.131454   \n",
       "2     0.210000  0.083216  0.107389  0.373491  0.139399  0.228461  0.224466   \n",
       "3     0.089227  0.028722  0.023529  0.516543  0.357591  1.165964  0.149411   \n",
       "4     0.005663  0.135440  0.112919  0.115065  0.094258  0.071284  0.024563   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "9995  0.000638  0.113700  0.108743  0.090572  0.055815  0.000000  0.082937   \n",
       "9996  0.059231  0.211131  0.201713  0.032353  0.048716  0.000000  0.000000   \n",
       "9997  0.001761  1.463821  0.011850  0.032598  0.937367  0.021707  0.103139   \n",
       "9998  0.026339  0.870153  0.044403  0.028958  0.081213  0.000000  0.152911   \n",
       "9999  0.028445  0.396974  0.129699  0.304854  0.032658  0.027572  0.009164   \n",
       "\n",
       "             7         8         9  ...      2039      2040      2041  \\\n",
       "0     0.190755  0.040758  0.071579  ...  0.413018  0.040352  0.029557   \n",
       "1     0.441281  0.010310  0.016984  ...  0.224854  0.045422  0.151592   \n",
       "2     0.400738  0.035322  0.065952  ...  0.340572  0.006806  0.127561   \n",
       "3     0.075871  0.022748  0.000000  ...  0.272945  0.010554  1.335347   \n",
       "4     0.129603  0.003601  0.212501  ...  0.402290  0.103481  0.176257   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "9995  0.025062  0.003649  0.045059  ...  0.713404  0.000000  0.009588   \n",
       "9996  0.077469  0.013222  0.060250  ...  0.411932  0.000000  0.000043   \n",
       "9997  0.006572  0.019002  0.021300  ...  0.000000  0.009892  0.071254   \n",
       "9998  0.002433  0.006991  0.132442  ...  0.021862  0.001303  0.022319   \n",
       "9999  0.000000  0.114894  0.589555  ...  0.069370  0.034535  0.012728   \n",
       "\n",
       "          2042      2043      2044      2045      2046      2047  \\\n",
       "0     0.017327  0.082976  0.208654  0.213978  0.007254  0.369544   \n",
       "1     0.346858  0.000291  0.781311  0.047355  0.008029  0.237799   \n",
       "2     0.045913  0.742372  0.063517  0.105330  0.058667  0.092687   \n",
       "3     0.072965  0.002853  0.189264  0.011270  0.085637  0.058687   \n",
       "4     0.158090  0.010789  0.546413  0.319652  0.083233  0.016363   \n",
       "...        ...       ...       ...       ...       ...       ...   \n",
       "9995  0.051383  0.411699  0.089952  0.282519  0.000132  0.007038   \n",
       "9996  0.028635  0.034902  0.292674  0.112189  0.008453  0.048728   \n",
       "9997  0.661938  0.006536  0.265005  0.174313  0.000492  0.299318   \n",
       "9998  0.043793  0.260402  0.007712  0.034186  0.000000  0.067226   \n",
       "9999  0.155390  0.008744  0.136618  0.080729  0.018973  0.069819   \n",
       "\n",
       "      category_class  \n",
       "0                338  \n",
       "1                338  \n",
       "2                338  \n",
       "3                338  \n",
       "4                338  \n",
       "...              ...  \n",
       "9995             591  \n",
       "9996             591  \n",
       "9997             591  \n",
       "9998             591  \n",
       "9999             591  \n",
       "\n",
       "[10000 rows x 2049 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "htXFIOPMba2B",
    "outputId": "8628abc2-2354-4b45-af6a-31e1f3876782"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "len(Counter(train_dataset['category_class']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "paE4cJ0BTxoL"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "def make_pairs(x, y):\n",
    "    \"\"\"Creates a tuple containing image pairs with corresponding label.\n",
    "\n",
    "    Arguments:\n",
    "        x: List containing images, each index in this list corresponds to one image.\n",
    "        y: List containing labels, each label with datatype of `int`.\n",
    "\n",
    "    Returns:\n",
    "        Tuple containing two numpy arrays as (pairs_of_samples, labels),\n",
    "        where pairs_of_samples' shape is (2len(x), 2,n_features_dims) and\n",
    "        labels are a binary array of shape (2len(x)).\n",
    "    \"\"\"\n",
    "\n",
    "    num_classes = max(y) + 1\n",
    "    digit_indices = [np.where(y == i)[0] for i in range(num_classes)]\n",
    "\n",
    "    pairs = []\n",
    "    labels = []\n",
    "\n",
    "    for idx1 in range(len(x)):\n",
    "        #####Add a positive example\n",
    "        x1 = x[idx1]\n",
    "        label1 = y[idx1]\n",
    "        idx2 = random.choice(digit_indices[label1])\n",
    "        x2 = x[idx2]\n",
    "\n",
    "        #####Add a negative example\n",
    "        label2 = random.randint(0, num_classes - 1)\n",
    "        while label2 == label1:\n",
    "            label2 = random.randint(0, num_classes - 1)\n",
    "\n",
    "        idx3 = random.choice(digit_indices[label2])\n",
    "        x3 = x[idx3]\n",
    "\n",
    "        pairs+=[[x1,x2,x3]]\n",
    "\n",
    "    return np.array(pairs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9jTt0fa0T_N0"
   },
   "outputs": [],
   "source": [
    "x_train=train_dataset.iloc[:,:-1].values\n",
    "y_train=train_dataset.iloc[:,-1].values\n",
    "x_val=val_dataset.iloc[:,:-1].values\n",
    "y_val=val_dataset.iloc[:,-1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "evmuCLs5UaEx",
    "outputId": "ad57c9c2-22f0-43cb-9642-3bc83dad08cc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([338, 338, 338, ..., 591, 591, 591])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fNMYvzQ_a2jY",
    "outputId": "1d2688ac-6d39-4ebe-ee9f-9a0536e5bcc8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([7770, 7771, 7772, 7773, 7774, 7775, 7776, 7777, 7778, 7779]),\n",
       " array([9500, 9501, 9502, 9503, 9504, 9505, 9506, 9507, 9508, 9509]),\n",
       " array([4790, 4791, 4792, 4793, 4794, 4795, 4796, 4797, 4798, 4799]),\n",
       " array([7240, 7241, 7242, 7243, 7244, 7245, 7246, 7247, 7248, 7249]),\n",
       " array([9720, 9721, 9722, 9723, 9724, 9725, 9726, 9727, 9728, 9729]),\n",
       " array([7110, 7111, 7112, 7113, 7114, 7115, 7116, 7117, 7118, 7119]),\n",
       " array([3620, 3621, 3622, 3623, 3624, 3625, 3626, 3627, 3628, 3629]),\n",
       " array([1930, 1931, 1932, 1933, 1934, 1935, 1936, 1937, 1938, 1939]),\n",
       " array([2220, 2221, 2222, 2223, 2224, 2225, 2226, 2227, 2228, 2229]),\n",
       " array([1530, 1531, 1532, 1533, 1534, 1535, 1536, 1537, 1538, 1539]),\n",
       " array([6320, 6321, 6322, 6323, 6324, 6325, 6326, 6327, 6328, 6329]),\n",
       " array([9530, 9531, 9532, 9533, 9534, 9535, 9536, 9537, 9538, 9539]),\n",
       " array([620, 621, 622, 623, 624, 625, 626, 627, 628, 629]),\n",
       " array([3120, 3121, 3122, 3123, 3124, 3125, 3126, 3127, 3128, 3129]),\n",
       " array([7230, 7231, 7232, 7233, 7234, 7235, 7236, 7237, 7238, 7239]),\n",
       " array([3050, 3051, 3052, 3053, 3054, 3055, 3056, 3057, 3058, 3059]),\n",
       " array([750, 751, 752, 753, 754, 755, 756, 757, 758, 759]),\n",
       " array([5930, 5931, 5932, 5933, 5934, 5935, 5936, 5937, 5938, 5939]),\n",
       " array([740, 741, 742, 743, 744, 745, 746, 747, 748, 749]),\n",
       " array([7040, 7041, 7042, 7043, 7044, 7045, 7046, 7047, 7048, 7049]),\n",
       " array([700, 701, 702, 703, 704, 705, 706, 707, 708, 709]),\n",
       " array([2300, 2301, 2302, 2303, 2304, 2305, 2306, 2307, 2308, 2309]),\n",
       " array([7210, 7211, 7212, 7213, 7214, 7215, 7216, 7217, 7218, 7219]),\n",
       " array([4740, 4741, 4742, 4743, 4744, 4745, 4746, 4747, 4748, 4749]),\n",
       " array([6830, 6831, 6832, 6833, 6834, 6835, 6836, 6837, 6838, 6839]),\n",
       " array([300, 301, 302, 303, 304, 305, 306, 307, 308, 309]),\n",
       " array([5720, 5721, 5722, 5723, 5724, 5725, 5726, 5727, 5728, 5729]),\n",
       " array([1550, 1551, 1552, 1553, 1554, 1555, 1556, 1557, 1558, 1559]),\n",
       " array([8140, 8141, 8142, 8143, 8144, 8145, 8146, 8147, 8148, 8149]),\n",
       " array([2230, 2231, 2232, 2233, 2234, 2235, 2236, 2237, 2238, 2239]),\n",
       " array([8090, 8091, 8092, 8093, 8094, 8095, 8096, 8097, 8098, 8099]),\n",
       " array([9390, 9391, 9392, 9393, 9394, 9395, 9396, 9397, 9398, 9399]),\n",
       " array([6610, 6611, 6612, 6613, 6614, 6615, 6616, 6617, 6618, 6619]),\n",
       " array([3810, 3811, 3812, 3813, 3814, 3815, 3816, 3817, 3818, 3819]),\n",
       " array([9830, 9831, 9832, 9833, 9834, 9835, 9836, 9837, 9838, 9839]),\n",
       " array([760, 761, 762, 763, 764, 765, 766, 767, 768, 769]),\n",
       " array([8150, 8151, 8152, 8153, 8154, 8155, 8156, 8157, 8158, 8159]),\n",
       " array([9710, 9711, 9712, 9713, 9714, 9715, 9716, 9717, 9718, 9719]),\n",
       " array([490, 491, 492, 493, 494, 495, 496, 497, 498, 499]),\n",
       " array([7250, 7251, 7252, 7253, 7254, 7255, 7256, 7257, 7258, 7259]),\n",
       " array([7690, 7691, 7692, 7693, 7694, 7695, 7696, 7697, 7698, 7699]),\n",
       " array([4860, 4861, 4862, 4863, 4864, 4865, 4866, 4867, 4868, 4869]),\n",
       " array([1150, 1151, 1152, 1153, 1154, 1155, 1156, 1157, 1158, 1159]),\n",
       " array([630, 631, 632, 633, 634, 635, 636, 637, 638, 639]),\n",
       " array([4990, 4991, 4992, 4993, 4994, 4995, 4996, 4997, 4998, 4999]),\n",
       " array([2940, 2941, 2942, 2943, 2944, 2945, 2946, 2947, 2948, 2949]),\n",
       " array([4890, 4891, 4892, 4893, 4894, 4895, 4896, 4897, 4898, 4899]),\n",
       " array([7540, 7541, 7542, 7543, 7544, 7545, 7546, 7547, 7548, 7549]),\n",
       " array([7820, 7821, 7822, 7823, 7824, 7825, 7826, 7827, 7828, 7829]),\n",
       " array([6420, 6421, 6422, 6423, 6424, 6425, 6426, 6427, 6428, 6429]),\n",
       " array([1900, 1901, 1902, 1903, 1904, 1905, 1906, 1907, 1908, 1909]),\n",
       " array([990, 991, 992, 993, 994, 995, 996, 997, 998, 999]),\n",
       " array([4640, 4641, 4642, 4643, 4644, 4645, 4646, 4647, 4648, 4649]),\n",
       " array([7410, 7411, 7412, 7413, 7414, 7415, 7416, 7417, 7418, 7419]),\n",
       " array([1360, 1361, 1362, 1363, 1364, 1365, 1366, 1367, 1368, 1369]),\n",
       " array([8860, 8861, 8862, 8863, 8864, 8865, 8866, 8867, 8868, 8869]),\n",
       " array([6370, 6371, 6372, 6373, 6374, 6375, 6376, 6377, 6378, 6379]),\n",
       " array([4800, 4801, 4802, 4803, 4804, 4805, 4806, 4807, 4808, 4809]),\n",
       " array([330, 331, 332, 333, 334, 335, 336, 337, 338, 339]),\n",
       " array([1060, 1061, 1062, 1063, 1064, 1065, 1066, 1067, 1068, 1069]),\n",
       " array([3820, 3821, 3822, 3823, 3824, 3825, 3826, 3827, 3828, 3829]),\n",
       " array([8660, 8661, 8662, 8663, 8664, 8665, 8666, 8667, 8668, 8669]),\n",
       " array([5550, 5551, 5552, 5553, 5554, 5555, 5556, 5557, 5558, 5559]),\n",
       " array([4950, 4951, 4952, 4953, 4954, 4955, 4956, 4957, 4958, 4959]),\n",
       " array([1560, 1561, 1562, 1563, 1564, 1565, 1566, 1567, 1568, 1569]),\n",
       " array([8790, 8791, 8792, 8793, 8794, 8795, 8796, 8797, 8798, 8799]),\n",
       " array([9850, 9851, 9852, 9853, 9854, 9855, 9856, 9857, 9858, 9859]),\n",
       " array([5670, 5671, 5672, 5673, 5674, 5675, 5676, 5677, 5678, 5679]),\n",
       " array([7310, 7311, 7312, 7313, 7314, 7315, 7316, 7317, 7318, 7319]),\n",
       " array([870, 871, 872, 873, 874, 875, 876, 877, 878, 879]),\n",
       " array([9670, 9671, 9672, 9673, 9674, 9675, 9676, 9677, 9678, 9679]),\n",
       " array([3160, 3161, 3162, 3163, 3164, 3165, 3166, 3167, 3168, 3169]),\n",
       " array([8250, 8251, 8252, 8253, 8254, 8255, 8256, 8257, 8258, 8259]),\n",
       " array([4300, 4301, 4302, 4303, 4304, 4305, 4306, 4307, 4308, 4309]),\n",
       " array([8120, 8121, 8122, 8123, 8124, 8125, 8126, 8127, 8128, 8129]),\n",
       " array([950, 951, 952, 953, 954, 955, 956, 957, 958, 959]),\n",
       " array([9730, 9731, 9732, 9733, 9734, 9735, 9736, 9737, 9738, 9739]),\n",
       " array([8750, 8751, 8752, 8753, 8754, 8755, 8756, 8757, 8758, 8759]),\n",
       " array([4290, 4291, 4292, 4293, 4294, 4295, 4296, 4297, 4298, 4299]),\n",
       " array([8680, 8681, 8682, 8683, 8684, 8685, 8686, 8687, 8688, 8689]),\n",
       " array([9750, 9751, 9752, 9753, 9754, 9755, 9756, 9757, 9758, 9759]),\n",
       " array([7350, 7351, 7352, 7353, 7354, 7355, 7356, 7357, 7358, 7359]),\n",
       " array([7610, 7611, 7612, 7613, 7614, 7615, 7616, 7617, 7618, 7619]),\n",
       " array([7900, 7901, 7902, 7903, 7904, 7905, 7906, 7907, 7908, 7909]),\n",
       " array([4510, 4511, 4512, 4513, 4514, 4515, 4516, 4517, 4518, 4519]),\n",
       " array([320, 321, 322, 323, 324, 325, 326, 327, 328, 329]),\n",
       " array([7700, 7701, 7702, 7703, 7704, 7705, 7706, 7707, 7708, 7709]),\n",
       " array([9540, 9541, 9542, 9543, 9544, 9545, 9546, 9547, 9548, 9549]),\n",
       " array([7460, 7461, 7462, 7463, 7464, 7465, 7466, 7467, 7468, 7469]),\n",
       " array([6440, 6441, 6442, 6443, 6444, 6445, 6446, 6447, 6448, 6449]),\n",
       " array([8520, 8521, 8522, 8523, 8524, 8525, 8526, 8527, 8528, 8529]),\n",
       " array([2430, 2431, 2432, 2433, 2434, 2435, 2436, 2437, 2438, 2439]),\n",
       " array([5340, 5341, 5342, 5343, 5344, 5345, 5346, 5347, 5348, 5349]),\n",
       " array([1750, 1751, 1752, 1753, 1754, 1755, 1756, 1757, 1758, 1759]),\n",
       " array([8280, 8281, 8282, 8283, 8284, 8285, 8286, 8287, 8288, 8289]),\n",
       " array([2270, 2271, 2272, 2273, 2274, 2275, 2276, 2277, 2278, 2279]),\n",
       " array([5270, 5271, 5272, 5273, 5274, 5275, 5276, 5277, 5278, 5279]),\n",
       " array([7480, 7481, 7482, 7483, 7484, 7485, 7486, 7487, 7488, 7489]),\n",
       " array([880, 881, 882, 883, 884, 885, 886, 887, 888, 889]),\n",
       " array([7370, 7371, 7372, 7373, 7374, 7375, 7376, 7377, 7378, 7379]),\n",
       " array([20, 21, 22, 23, 24, 25, 26, 27, 28, 29]),\n",
       " array([5030, 5031, 5032, 5033, 5034, 5035, 5036, 5037, 5038, 5039]),\n",
       " array([4020, 4021, 4022, 4023, 4024, 4025, 4026, 4027, 4028, 4029]),\n",
       " array([3990, 3991, 3992, 3993, 3994, 3995, 3996, 3997, 3998, 3999]),\n",
       " array([7120, 7121, 7122, 7123, 7124, 7125, 7126, 7127, 7128, 7129]),\n",
       " array([5390, 5391, 5392, 5393, 5394, 5395, 5396, 5397, 5398, 5399]),\n",
       " array([820, 821, 822, 823, 824, 825, 826, 827, 828, 829]),\n",
       " array([6070, 6071, 6072, 6073, 6074, 6075, 6076, 6077, 6078, 6079]),\n",
       " array([7990, 7991, 7992, 7993, 7994, 7995, 7996, 7997, 7998, 7999]),\n",
       " array([9140, 9141, 9142, 9143, 9144, 9145, 9146, 9147, 9148, 9149]),\n",
       " array([1080, 1081, 1082, 1083, 1084, 1085, 1086, 1087, 1088, 1089]),\n",
       " array([4500, 4501, 4502, 4503, 4504, 4505, 4506, 4507, 4508, 4509]),\n",
       " array([4090, 4091, 4092, 4093, 4094, 4095, 4096, 4097, 4098, 4099]),\n",
       " array([1940, 1941, 1942, 1943, 1944, 1945, 1946, 1947, 1948, 1949]),\n",
       " array([2710, 2711, 2712, 2713, 2714, 2715, 2716, 2717, 2718, 2719]),\n",
       " array([7810, 7811, 7812, 7813, 7814, 7815, 7816, 7817, 7818, 7819]),\n",
       " array([50, 51, 52, 53, 54, 55, 56, 57, 58, 59]),\n",
       " array([6930, 6931, 6932, 6933, 6934, 6935, 6936, 6937, 6938, 6939]),\n",
       " array([8800, 8801, 8802, 8803, 8804, 8805, 8806, 8807, 8808, 8809]),\n",
       " array([6340, 6341, 6342, 6343, 6344, 6345, 6346, 6347, 6348, 6349]),\n",
       " array([3630, 3631, 3632, 3633, 3634, 3635, 3636, 3637, 3638, 3639]),\n",
       " array([5220, 5221, 5222, 5223, 5224, 5225, 5226, 5227, 5228, 5229]),\n",
       " array([4480, 4481, 4482, 4483, 4484, 4485, 4486, 4487, 4488, 4489]),\n",
       " array([6660, 6661, 6662, 6663, 6664, 6665, 6666, 6667, 6668, 6669]),\n",
       " array([9840, 9841, 9842, 9843, 9844, 9845, 9846, 9847, 9848, 9849]),\n",
       " array([5470, 5471, 5472, 5473, 5474, 5475, 5476, 5477, 5478, 5479]),\n",
       " array([3960, 3961, 3962, 3963, 3964, 3965, 3966, 3967, 3968, 3969]),\n",
       " array([8460, 8461, 8462, 8463, 8464, 8465, 8466, 8467, 8468, 8469]),\n",
       " array([4490, 4491, 4492, 4493, 4494, 4495, 4496, 4497, 4498, 4499]),\n",
       " array([8170, 8171, 8172, 8173, 8174, 8175, 8176, 8177, 8178, 8179]),\n",
       " array([7400, 7401, 7402, 7403, 7404, 7405, 7406, 7407, 7408, 7409]),\n",
       " array([1470, 1471, 1472, 1473, 1474, 1475, 1476, 1477, 1478, 1479]),\n",
       " array([2170, 2171, 2172, 2173, 2174, 2175, 2176, 2177, 2178, 2179]),\n",
       " array([2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019]),\n",
       " array([5260, 5261, 5262, 5263, 5264, 5265, 5266, 5267, 5268, 5269]),\n",
       " array([1030, 1031, 1032, 1033, 1034, 1035, 1036, 1037, 1038, 1039]),\n",
       " array([310, 311, 312, 313, 314, 315, 316, 317, 318, 319]),\n",
       " array([6010, 6011, 6012, 6013, 6014, 6015, 6016, 6017, 6018, 6019]),\n",
       " array([5620, 5621, 5622, 5623, 5624, 5625, 5626, 5627, 5628, 5629]),\n",
       " array([9570, 9571, 9572, 9573, 9574, 9575, 9576, 9577, 9578, 9579]),\n",
       " array([1100, 1101, 1102, 1103, 1104, 1105, 1106, 1107, 1108, 1109]),\n",
       " array([7300, 7301, 7302, 7303, 7304, 7305, 7306, 7307, 7308, 7309]),\n",
       " array([7760, 7761, 7762, 7763, 7764, 7765, 7766, 7767, 7768, 7769]),\n",
       " array([8100, 8101, 8102, 8103, 8104, 8105, 8106, 8107, 8108, 8109]),\n",
       " array([6480, 6481, 6482, 6483, 6484, 6485, 6486, 6487, 6488, 6489]),\n",
       " array([5240, 5241, 5242, 5243, 5244, 5245, 5246, 5247, 5248, 5249]),\n",
       " array([7950, 7951, 7952, 7953, 7954, 7955, 7956, 7957, 7958, 7959]),\n",
       " array([1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019]),\n",
       " array([1620, 1621, 1622, 1623, 1624, 1625, 1626, 1627, 1628, 1629]),\n",
       " array([9090, 9091, 9092, 9093, 9094, 9095, 9096, 9097, 9098, 9099]),\n",
       " array([1490, 1491, 1492, 1493, 1494, 1495, 1496, 1497, 1498, 1499]),\n",
       " array([4590, 4591, 4592, 4593, 4594, 4595, 4596, 4597, 4598, 4599]),\n",
       " array([8310, 8311, 8312, 8313, 8314, 8315, 8316, 8317, 8318, 8319]),\n",
       " array([8330, 8331, 8332, 8333, 8334, 8335, 8336, 8337, 8338, 8339]),\n",
       " array([7270, 7271, 7272, 7273, 7274, 7275, 7276, 7277, 7278, 7279]),\n",
       " array([4130, 4131, 4132, 4133, 4134, 4135, 4136, 4137, 4138, 4139]),\n",
       " array([2050, 2051, 2052, 2053, 2054, 2055, 2056, 2057, 2058, 2059]),\n",
       " array([1320, 1321, 1322, 1323, 1324, 1325, 1326, 1327, 1328, 1329]),\n",
       " array([5180, 5181, 5182, 5183, 5184, 5185, 5186, 5187, 5188, 5189]),\n",
       " array([1160, 1161, 1162, 1163, 1164, 1165, 1166, 1167, 1168, 1169]),\n",
       " array([8690, 8691, 8692, 8693, 8694, 8695, 8696, 8697, 8698, 8699]),\n",
       " array([3650, 3651, 3652, 3653, 3654, 3655, 3656, 3657, 3658, 3659]),\n",
       " array([7980, 7981, 7982, 7983, 7984, 7985, 7986, 7987, 7988, 7989]),\n",
       " array([6200, 6201, 6202, 6203, 6204, 6205, 6206, 6207, 6208, 6209]),\n",
       " array([9320, 9321, 9322, 9323, 9324, 9325, 9326, 9327, 9328, 9329]),\n",
       " array([720, 721, 722, 723, 724, 725, 726, 727, 728, 729]),\n",
       " array([8950, 8951, 8952, 8953, 8954, 8955, 8956, 8957, 8958, 8959]),\n",
       " array([2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009]),\n",
       " array([8650, 8651, 8652, 8653, 8654, 8655, 8656, 8657, 8658, 8659]),\n",
       " array([9070, 9071, 9072, 9073, 9074, 9075, 9076, 9077, 9078, 9079]),\n",
       " array([730, 731, 732, 733, 734, 735, 736, 737, 738, 739]),\n",
       " array([1910, 1911, 1912, 1913, 1914, 1915, 1916, 1917, 1918, 1919]),\n",
       " array([7090, 7091, 7092, 7093, 7094, 7095, 7096, 7097, 7098, 7099]),\n",
       " array([2570, 2571, 2572, 2573, 2574, 2575, 2576, 2577, 2578, 2579]),\n",
       " array([8530, 8531, 8532, 8533, 8534, 8535, 8536, 8537, 8538, 8539]),\n",
       " array([860, 861, 862, 863, 864, 865, 866, 867, 868, 869]),\n",
       " array([8580, 8581, 8582, 8583, 8584, 8585, 8586, 8587, 8588, 8589]),\n",
       " array([9280, 9281, 9282, 9283, 9284, 9285, 9286, 9287, 9288, 9289]),\n",
       " array([9220, 9221, 9222, 9223, 9224, 9225, 9226, 9227, 9228, 9229]),\n",
       " array([6730, 6731, 6732, 6733, 6734, 6735, 6736, 6737, 6738, 6739]),\n",
       " array([9290, 9291, 9292, 9293, 9294, 9295, 9296, 9297, 9298, 9299]),\n",
       " array([1190, 1191, 1192, 1193, 1194, 1195, 1196, 1197, 1198, 1199]),\n",
       " array([810, 811, 812, 813, 814, 815, 816, 817, 818, 819]),\n",
       " array([4750, 4751, 4752, 4753, 4754, 4755, 4756, 4757, 4758, 4759]),\n",
       " array([8770, 8771, 8772, 8773, 8774, 8775, 8776, 8777, 8778, 8779]),\n",
       " array([1850, 1851, 1852, 1853, 1854, 1855, 1856, 1857, 1858, 1859]),\n",
       " array([3270, 3271, 3272, 3273, 3274, 3275, 3276, 3277, 3278, 3279]),\n",
       " array([6710, 6711, 6712, 6713, 6714, 6715, 6716, 6717, 6718, 6719]),\n",
       " array([8510, 8511, 8512, 8513, 8514, 8515, 8516, 8517, 8518, 8519]),\n",
       " array([6630, 6631, 6632, 6633, 6634, 6635, 6636, 6637, 6638, 6639]),\n",
       " array([9440, 9441, 9442, 9443, 9444, 9445, 9446, 9447, 9448, 9449]),\n",
       " array([970, 971, 972, 973, 974, 975, 976, 977, 978, 979]),\n",
       " array([8070, 8071, 8072, 8073, 8074, 8075, 8076, 8077, 8078, 8079]),\n",
       " array([7710, 7711, 7712, 7713, 7714, 7715, 7716, 7717, 7718, 7719]),\n",
       " array([2780, 2781, 2782, 2783, 2784, 2785, 2786, 2787, 2788, 2789]),\n",
       " array([7790, 7791, 7792, 7793, 7794, 7795, 7796, 7797, 7798, 7799]),\n",
       " array([2830, 2831, 2832, 2833, 2834, 2835, 2836, 2837, 2838, 2839]),\n",
       " array([10, 11, 12, 13, 14, 15, 16, 17, 18, 19]),\n",
       " array([2020, 2021, 2022, 2023, 2024, 2025, 2026, 2027, 2028, 2029]),\n",
       " array([6790, 6791, 6792, 6793, 6794, 6795, 6796, 6797, 6798, 6799]),\n",
       " array([3560, 3561, 3562, 3563, 3564, 3565, 3566, 3567, 3568, 3569]),\n",
       " array([8180, 8181, 8182, 8183, 8184, 8185, 8186, 8187, 8188, 8189]),\n",
       " array([1600, 1601, 1602, 1603, 1604, 1605, 1606, 1607, 1608, 1609]),\n",
       " array([400, 401, 402, 403, 404, 405, 406, 407, 408, 409]),\n",
       " array([280, 281, 282, 283, 284, 285, 286, 287, 288, 289]),\n",
       " array([2360, 2361, 2362, 2363, 2364, 2365, 2366, 2367, 2368, 2369]),\n",
       " array([7840, 7841, 7842, 7843, 7844, 7845, 7846, 7847, 7848, 7849]),\n",
       " array([8560, 8561, 8562, 8563, 8564, 8565, 8566, 8567, 8568, 8569]),\n",
       " array([5540, 5541, 5542, 5543, 5544, 5545, 5546, 5547, 5548, 5549]),\n",
       " array([5940, 5941, 5942, 5943, 5944, 5945, 5946, 5947, 5948, 5949]),\n",
       " array([3200, 3201, 3202, 3203, 3204, 3205, 3206, 3207, 3208, 3209]),\n",
       " array([3860, 3861, 3862, 3863, 3864, 3865, 3866, 3867, 3868, 3869]),\n",
       " array([6680, 6681, 6682, 6683, 6684, 6685, 6686, 6687, 6688, 6689]),\n",
       " array([640, 641, 642, 643, 644, 645, 646, 647, 648, 649]),\n",
       " array([1830, 1831, 1832, 1833, 1834, 1835, 1836, 1837, 1838, 1839]),\n",
       " array([9680, 9681, 9682, 9683, 9684, 9685, 9686, 9687, 9688, 9689]),\n",
       " array([8060, 8061, 8062, 8063, 8064, 8065, 8066, 8067, 8068, 8069]),\n",
       " array([4920, 4921, 4922, 4923, 4924, 4925, 4926, 4927, 4928, 4929]),\n",
       " array([7680, 7681, 7682, 7683, 7684, 7685, 7686, 7687, 7688, 7689]),\n",
       " array([420, 421, 422, 423, 424, 425, 426, 427, 428, 429]),\n",
       " array([8020, 8021, 8022, 8023, 8024, 8025, 8026, 8027, 8028, 8029]),\n",
       " array([2240, 2241, 2242, 2243, 2244, 2245, 2246, 2247, 2248, 2249]),\n",
       " array([6160, 6161, 6162, 6163, 6164, 6165, 6166, 6167, 6168, 6169]),\n",
       " array([9790, 9791, 9792, 9793, 9794, 9795, 9796, 9797, 9798, 9799]),\n",
       " array([1400, 1401, 1402, 1403, 1404, 1405, 1406, 1407, 1408, 1409]),\n",
       " array([7800, 7801, 7802, 7803, 7804, 7805, 7806, 7807, 7808, 7809]),\n",
       " array([200, 201, 202, 203, 204, 205, 206, 207, 208, 209]),\n",
       " array([3720, 3721, 3722, 3723, 3724, 3725, 3726, 3727, 3728, 3729]),\n",
       " array([6450, 6451, 6452, 6453, 6454, 6455, 6456, 6457, 6458, 6459]),\n",
       " array([6870, 6871, 6872, 6873, 6874, 6875, 6876, 6877, 6878, 6879]),\n",
       " array([9800, 9801, 9802, 9803, 9804, 9805, 9806, 9807, 9808, 9809]),\n",
       " array([8620, 8621, 8622, 8623, 8624, 8625, 8626, 8627, 8628, 8629]),\n",
       " array([6800, 6801, 6802, 6803, 6804, 6805, 6806, 6807, 6808, 6809]),\n",
       " array([9190, 9191, 9192, 9193, 9194, 9195, 9196, 9197, 9198, 9199]),\n",
       " array([3080, 3081, 3082, 3083, 3084, 3085, 3086, 3087, 3088, 3089]),\n",
       " array([4330, 4331, 4332, 4333, 4334, 4335, 4336, 4337, 4338, 4339]),\n",
       " array([4550, 4551, 4552, 4553, 4554, 4555, 4556, 4557, 4558, 4559]),\n",
       " array([9890, 9891, 9892, 9893, 9894, 9895, 9896, 9897, 9898, 9899]),\n",
       " array([6940, 6941, 6942, 6943, 6944, 6945, 6946, 6947, 6948, 6949]),\n",
       " array([7100, 7101, 7102, 7103, 7104, 7105, 7106, 7107, 7108, 7109]),\n",
       " array([3920, 3921, 3922, 3923, 3924, 3925, 3926, 3927, 3928, 3929]),\n",
       " array([8810, 8811, 8812, 8813, 8814, 8815, 8816, 8817, 8818, 8819]),\n",
       " array([9180, 9181, 9182, 9183, 9184, 9185, 9186, 9187, 9188, 9189]),\n",
       " array([2030, 2031, 2032, 2033, 2034, 2035, 2036, 2037, 2038, 2039]),\n",
       " array([6560, 6561, 6562, 6563, 6564, 6565, 6566, 6567, 6568, 6569]),\n",
       " array([8780, 8781, 8782, 8783, 8784, 8785, 8786, 8787, 8788, 8789]),\n",
       " array([4340, 4341, 4342, 4343, 4344, 4345, 4346, 4347, 4348, 4349]),\n",
       " array([2800, 2801, 2802, 2803, 2804, 2805, 2806, 2807, 2808, 2809]),\n",
       " array([4070, 4071, 4072, 4073, 4074, 4075, 4076, 4077, 4078, 4079]),\n",
       " array([1580, 1581, 1582, 1583, 1584, 1585, 1586, 1587, 1588, 1589]),\n",
       " array([2600, 2601, 2602, 2603, 2604, 2605, 2606, 2607, 2608, 2609]),\n",
       " array([9410, 9411, 9412, 9413, 9414, 9415, 9416, 9417, 9418, 9419]),\n",
       " array([380, 381, 382, 383, 384, 385, 386, 387, 388, 389]),\n",
       " array([5640, 5641, 5642, 5643, 5644, 5645, 5646, 5647, 5648, 5649]),\n",
       " array([8160, 8161, 8162, 8163, 8164, 8165, 8166, 8167, 8168, 8169]),\n",
       " array([8890, 8891, 8892, 8893, 8894, 8895, 8896, 8897, 8898, 8899]),\n",
       " array([9610, 9611, 9612, 9613, 9614, 9615, 9616, 9617, 9618, 9619]),\n",
       " array([3840, 3841, 3842, 3843, 3844, 3845, 3846, 3847, 3848, 3849]),\n",
       " array([3670, 3671, 3672, 3673, 3674, 3675, 3676, 3677, 3678, 3679]),\n",
       " array([7320, 7321, 7322, 7323, 7324, 7325, 7326, 7327, 7328, 7329]),\n",
       " array([2690, 2691, 2692, 2693, 2694, 2695, 2696, 2697, 2698, 2699]),\n",
       " array([5020, 5021, 5022, 5023, 5024, 5025, 5026, 5027, 5028, 5029]),\n",
       " array([5590, 5591, 5592, 5593, 5594, 5595, 5596, 5597, 5598, 5599]),\n",
       " array([2320, 2321, 2322, 2323, 2324, 2325, 2326, 2327, 2328, 2329]),\n",
       " array([4220, 4221, 4222, 4223, 4224, 4225, 4226, 4227, 4228, 4229]),\n",
       " array([3530, 3531, 3532, 3533, 3534, 3535, 3536, 3537, 3538, 3539]),\n",
       " array([7670, 7671, 7672, 7673, 7674, 7675, 7676, 7677, 7678, 7679]),\n",
       " array([8840, 8841, 8842, 8843, 8844, 8845, 8846, 8847, 8848, 8849]),\n",
       " array([2990, 2991, 2992, 2993, 2994, 2995, 2996, 2997, 2998, 2999]),\n",
       " array([7050, 7051, 7052, 7053, 7054, 7055, 7056, 7057, 7058, 7059]),\n",
       " array([7660, 7661, 7662, 7663, 7664, 7665, 7666, 7667, 7668, 7669]),\n",
       " array([4610, 4611, 4612, 4613, 4614, 4615, 4616, 4617, 4618, 4619]),\n",
       " array([600, 601, 602, 603, 604, 605, 606, 607, 608, 609]),\n",
       " array([1520, 1521, 1522, 1523, 1524, 1525, 1526, 1527, 1528, 1529]),\n",
       " array([4060, 4061, 4062, 4063, 4064, 4065, 4066, 4067, 4068, 4069]),\n",
       " array([5770, 5771, 5772, 5773, 5774, 5775, 5776, 5777, 5778, 5779]),\n",
       " array([9950, 9951, 9952, 9953, 9954, 9955, 9956, 9957, 9958, 9959]),\n",
       " array([6900, 6901, 6902, 6903, 6904, 6905, 6906, 6907, 6908, 6909]),\n",
       " array([7020, 7021, 7022, 7023, 7024, 7025, 7026, 7027, 7028, 7029]),\n",
       " array([4570, 4571, 4572, 4573, 4574, 4575, 4576, 4577, 4578, 4579]),\n",
       " array([8010, 8011, 8012, 8013, 8014, 8015, 8016, 8017, 8018, 8019]),\n",
       " array([4710, 4711, 4712, 4713, 4714, 4715, 4716, 4717, 4718, 4719]),\n",
       " array([5360, 5361, 5362, 5363, 5364, 5365, 5366, 5367, 5368, 5369]),\n",
       " array([2130, 2131, 2132, 2133, 2134, 2135, 2136, 2137, 2138, 2139]),\n",
       " array([3770, 3771, 3772, 3773, 3774, 3775, 3776, 3777, 3778, 3779]),\n",
       " array([1670, 1671, 1672, 1673, 1674, 1675, 1676, 1677, 1678, 1679]),\n",
       " array([70, 71, 72, 73, 74, 75, 76, 77, 78, 79]),\n",
       " array([1250, 1251, 1252, 1253, 1254, 1255, 1256, 1257, 1258, 1259]),\n",
       " array([2350, 2351, 2352, 2353, 2354, 2355, 2356, 2357, 2358, 2359]),\n",
       " array([8980, 8981, 8982, 8983, 8984, 8985, 8986, 8987, 8988, 8989]),\n",
       " array([2280, 2281, 2282, 2283, 2284, 2285, 2286, 2287, 2288, 2289]),\n",
       " array([4440, 4441, 4442, 4443, 4444, 4445, 4446, 4447, 4448, 4449]),\n",
       " array([5230, 5231, 5232, 5233, 5234, 5235, 5236, 5237, 5238, 5239]),\n",
       " array([2820, 2821, 2822, 2823, 2824, 2825, 2826, 2827, 2828, 2829]),\n",
       " array([5990, 5991, 5992, 5993, 5994, 5995, 5996, 5997, 5998, 5999]),\n",
       " array([8480, 8481, 8482, 8483, 8484, 8485, 8486, 8487, 8488, 8489]),\n",
       " array([9350, 9351, 9352, 9353, 9354, 9355, 9356, 9357, 9358, 9359]),\n",
       " array([5880, 5881, 5882, 5883, 5884, 5885, 5886, 5887, 5888, 5889]),\n",
       " array([1980, 1981, 1982, 1983, 1984, 1985, 1986, 1987, 1988, 1989]),\n",
       " array([7010, 7011, 7012, 7013, 7014, 7015, 7016, 7017, 7018, 7019]),\n",
       " array([140, 141, 142, 143, 144, 145, 146, 147, 148, 149]),\n",
       " array([7890, 7891, 7892, 7893, 7894, 7895, 7896, 7897, 7898, 7899]),\n",
       " array([9240, 9241, 9242, 9243, 9244, 9245, 9246, 9247, 9248, 9249]),\n",
       " array([6060, 6061, 6062, 6063, 6064, 6065, 6066, 6067, 6068, 6069]),\n",
       " array([3900, 3901, 3902, 3903, 3904, 3905, 3906, 3907, 3908, 3909]),\n",
       " array([7920, 7921, 7922, 7923, 7924, 7925, 7926, 7927, 7928, 7929]),\n",
       " array([7830, 7831, 7832, 7833, 7834, 7835, 7836, 7837, 7838, 7839]),\n",
       " array([110, 111, 112, 113, 114, 115, 116, 117, 118, 119]),\n",
       " array([1420, 1421, 1422, 1423, 1424, 1425, 1426, 1427, 1428, 1429]),\n",
       " array([3450, 3451, 3452, 3453, 3454, 3455, 3456, 3457, 3458, 3459]),\n",
       " array([6490, 6491, 6492, 6493, 6494, 6495, 6496, 6497, 6498, 6499]),\n",
       " array([1230, 1231, 1232, 1233, 1234, 1235, 1236, 1237, 1238, 1239]),\n",
       " array([5350, 5351, 5352, 5353, 5354, 5355, 5356, 5357, 5358, 5359]),\n",
       " array([6500, 6501, 6502, 6503, 6504, 6505, 6506, 6507, 6508, 6509]),\n",
       " array([7870, 7871, 7872, 7873, 7874, 7875, 7876, 7877, 7878, 7879]),\n",
       " array([2590, 2591, 2592, 2593, 2594, 2595, 2596, 2597, 2598, 2599]),\n",
       " array([9420, 9421, 9422, 9423, 9424, 9425, 9426, 9427, 9428, 9429]),\n",
       " array([940, 941, 942, 943, 944, 945, 946, 947, 948, 949]),\n",
       " array([9650, 9651, 9652, 9653, 9654, 9655, 9656, 9657, 9658, 9659]),\n",
       " array([8290, 8291, 8292, 8293, 8294, 8295, 8296, 8297, 8298, 8299]),\n",
       " array([9250, 9251, 9252, 9253, 9254, 9255, 9256, 9257, 9258, 9259]),\n",
       " array([1200, 1201, 1202, 1203, 1204, 1205, 1206, 1207, 1208, 1209]),\n",
       " array([8000, 8001, 8002, 8003, 8004, 8005, 8006, 8007, 8008, 8009]),\n",
       " array([6570, 6571, 6572, 6573, 6574, 6575, 6576, 6577, 6578, 6579]),\n",
       " array([8600, 8601, 8602, 8603, 8604, 8605, 8606, 8607, 8608, 8609]),\n",
       " array([2970, 2971, 2972, 2973, 2974, 2975, 2976, 2977, 2978, 2979]),\n",
       " array([4400, 4401, 4402, 4403, 4404, 4405, 4406, 4407, 4408, 4409]),\n",
       " array([6750, 6751, 6752, 6753, 6754, 6755, 6756, 6757, 6758, 6759]),\n",
       " array([4240, 4241, 4242, 4243, 4244, 4245, 4246, 4247, 4248, 4249]),\n",
       " array([1870, 1871, 1872, 1873, 1874, 1875, 1876, 1877, 1878, 1879]),\n",
       " array([1510, 1511, 1512, 1513, 1514, 1515, 1516, 1517, 1518, 1519]),\n",
       " array([4940, 4941, 4942, 4943, 4944, 4945, 4946, 4947, 4948, 4949]),\n",
       " array([1110, 1111, 1112, 1113, 1114, 1115, 1116, 1117, 1118, 1119]),\n",
       " array([660, 661, 662, 663, 664, 665, 666, 667, 668, 669]),\n",
       " array([4170, 4171, 4172, 4173, 4174, 4175, 4176, 4177, 4178, 4179]),\n",
       " array([6170, 6171, 6172, 6173, 6174, 6175, 6176, 6177, 6178, 6179]),\n",
       " array([8470, 8471, 8472, 8473, 8474, 8475, 8476, 8477, 8478, 8479]),\n",
       " array([1970, 1971, 1972, 1973, 1974, 1975, 1976, 1977, 1978, 1979]),\n",
       " array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]),\n",
       " array([8880, 8881, 8882, 8883, 8884, 8885, 8886, 8887, 8888, 8889]),\n",
       " array([4410, 4411, 4412, 4413, 4414, 4415, 4416, 4417, 4418, 4419]),\n",
       " array([3420, 3421, 3422, 3423, 3424, 3425, 3426, 3427, 3428, 3429]),\n",
       " array([920, 921, 922, 923, 924, 925, 926, 927, 928, 929]),\n",
       " array([1790, 1791, 1792, 1793, 1794, 1795, 1796, 1797, 1798, 1799]),\n",
       " array([6850, 6851, 6852, 6853, 6854, 6855, 6856, 6857, 6858, 6859]),\n",
       " array([5700, 5701, 5702, 5703, 5704, 5705, 5706, 5707, 5708, 5709]),\n",
       " array([8500, 8501, 8502, 8503, 8504, 8505, 8506, 8507, 8508, 8509]),\n",
       " array([3290, 3291, 3292, 3293, 3294, 3295, 3296, 3297, 3298, 3299]),\n",
       " array([680, 681, 682, 683, 684, 685, 686, 687, 688, 689]),\n",
       " array([9820, 9821, 9822, 9823, 9824, 9825, 9826, 9827, 9828, 9829]),\n",
       " array([5530, 5531, 5532, 5533, 5534, 5535, 5536, 5537, 5538, 5539]),\n",
       " array([1240, 1241, 1242, 1243, 1244, 1245, 1246, 1247, 1248, 1249]),\n",
       " array([3210, 3211, 3212, 3213, 3214, 3215, 3216, 3217, 3218, 3219]),\n",
       " array([6960, 6961, 6962, 6963, 6964, 6965, 6966, 6967, 6968, 6969]),\n",
       " array([3460, 3461, 3462, 3463, 3464, 3465, 3466, 3467, 3468, 3469]),\n",
       " array([5100, 5101, 5102, 5103, 5104, 5105, 5106, 5107, 5108, 5109]),\n",
       " array([840, 841, 842, 843, 844, 845, 846, 847, 848, 849]),\n",
       " array([1070, 1071, 1072, 1073, 1074, 1075, 1076, 1077, 1078, 1079]),\n",
       " array([7140, 7141, 7142, 7143, 7144, 7145, 7146, 7147, 7148, 7149]),\n",
       " array([5050, 5051, 5052, 5053, 5054, 5055, 5056, 5057, 5058, 5059]),\n",
       " array([3680, 3681, 3682, 3683, 3684, 3685, 3686, 3687, 3688, 3689]),\n",
       " array([1840, 1841, 1842, 1843, 1844, 1845, 1846, 1847, 1848, 1849]),\n",
       " array([830, 831, 832, 833, 834, 835, 836, 837, 838, 839]),\n",
       " array([1180, 1181, 1182, 1183, 1184, 1185, 1186, 1187, 1188, 1189]),\n",
       " array([2980, 2981, 2982, 2983, 2984, 2985, 2986, 2987, 2988, 2989]),\n",
       " array([3830, 3831, 3832, 3833, 3834, 3835, 3836, 3837, 3838, 3839]),\n",
       " array([1120, 1121, 1122, 1123, 1124, 1125, 1126, 1127, 1128, 1129]),\n",
       " array([3250, 3251, 3252, 3253, 3254, 3255, 3256, 3257, 3258, 3259]),\n",
       " array([6820, 6821, 6822, 6823, 6824, 6825, 6826, 6827, 6828, 6829]),\n",
       " array([4760, 4761, 4762, 4763, 4764, 4765, 4766, 4767, 4768, 4769]),\n",
       " array([4150, 4151, 4152, 4153, 4154, 4155, 4156, 4157, 4158, 4159]),\n",
       " array([8430, 8431, 8432, 8433, 8434, 8435, 8436, 8437, 8438, 8439]),\n",
       " array([5250, 5251, 5252, 5253, 5254, 5255, 5256, 5257, 5258, 5259]),\n",
       " array([7000, 7001, 7002, 7003, 7004, 7005, 7006, 7007, 7008, 7009]),\n",
       " array([2540, 2541, 2542, 2543, 2544, 2545, 2546, 2547, 2548, 2549]),\n",
       " array([3950, 3951, 3952, 3953, 3954, 3955, 3956, 3957, 3958, 3959]),\n",
       " array([2500, 2501, 2502, 2503, 2504, 2505, 2506, 2507, 2508, 2509]),\n",
       " array([8230, 8231, 8232, 8233, 8234, 8235, 8236, 8237, 8238, 8239]),\n",
       " array([7510, 7511, 7512, 7513, 7514, 7515, 7516, 7517, 7518, 7519]),\n",
       " array([5850, 5851, 5852, 5853, 5854, 5855, 5856, 5857, 5858, 5859]),\n",
       " array([1650, 1651, 1652, 1653, 1654, 1655, 1656, 1657, 1658, 1659]),\n",
       " array([6400, 6401, 6402, 6403, 6404, 6405, 6406, 6407, 6408, 6409]),\n",
       " array([6980, 6981, 6982, 6983, 6984, 6985, 6986, 6987, 6988, 6989]),\n",
       " array([890, 891, 892, 893, 894, 895, 896, 897, 898, 899]),\n",
       " array([2720, 2721, 2722, 2723, 2724, 2725, 2726, 2727, 2728, 2729]),\n",
       " array([8960, 8961, 8962, 8963, 8964, 8965, 8966, 8967, 8968, 8969]),\n",
       " array([9520, 9521, 9522, 9523, 9524, 9525, 9526, 9527, 9528, 9529]),\n",
       " array([4420, 4421, 4422, 4423, 4424, 4425, 4426, 4427, 4428, 4429]),\n",
       " array([3590, 3591, 3592, 3593, 3594, 3595, 3596, 3597, 3598, 3599]),\n",
       " array([5120, 5121, 5122, 5123, 5124, 5125, 5126, 5127, 5128, 5129]),\n",
       " array([4980, 4981, 4982, 4983, 4984, 4985, 4986, 4987, 4988, 4989]),\n",
       " array([7590, 7591, 7592, 7593, 7594, 7595, 7596, 7597, 7598, 7599]),\n",
       " array([1380, 1381, 1382, 1383, 1384, 1385, 1386, 1387, 1388, 1389]),\n",
       " array([2900, 2901, 2902, 2903, 2904, 2905, 2906, 2907, 2908, 2909]),\n",
       " array([9020, 9021, 9022, 9023, 9024, 9025, 9026, 9027, 9028, 9029]),\n",
       " array([8360, 8361, 8362, 8363, 8364, 8365, 8366, 8367, 8368, 8369]),\n",
       " array([5450, 5451, 5452, 5453, 5454, 5455, 5456, 5457, 5458, 5459]),\n",
       " array([5130, 5131, 5132, 5133, 5134, 5135, 5136, 5137, 5138, 5139]),\n",
       " array([2920, 2921, 2922, 2923, 2924, 2925, 2926, 2927, 2928, 2929]),\n",
       " array([4620, 4621, 4622, 4623, 4624, 4625, 4626, 4627, 4628, 4629]),\n",
       " array([3580, 3581, 3582, 3583, 3584, 3585, 3586, 3587, 3588, 3589]),\n",
       " array([1220, 1221, 1222, 1223, 1224, 1225, 1226, 1227, 1228, 1229]),\n",
       " array([6840, 6841, 6842, 6843, 6844, 6845, 6846, 6847, 6848, 6849]),\n",
       " array([2120, 2121, 2122, 2123, 2124, 2125, 2126, 2127, 2128, 2129]),\n",
       " array([8630, 8631, 8632, 8633, 8634, 8635, 8636, 8637, 8638, 8639]),\n",
       " array([1680, 1681, 1682, 1683, 1684, 1685, 1686, 1687, 1688, 1689]),\n",
       " array([3430, 3431, 3432, 3433, 3434, 3435, 3436, 3437, 3438, 3439]),\n",
       " array([2620, 2621, 2622, 2623, 2624, 2625, 2626, 2627, 2628, 2629]),\n",
       " array([8390, 8391, 8392, 8393, 8394, 8395, 8396, 8397, 8398, 8399]),\n",
       " array([5600, 5601, 5602, 5603, 5604, 5605, 5606, 5607, 5608, 5609]),\n",
       " array([8760, 8761, 8762, 8763, 8764, 8765, 8766, 8767, 8768, 8769]),\n",
       " array([5010, 5011, 5012, 5013, 5014, 5015, 5016, 5017, 5018, 5019]),\n",
       " array([2520, 2521, 2522, 2523, 2524, 2525, 2526, 2527, 2528, 2529]),\n",
       " array([9770, 9771, 9772, 9773, 9774, 9775, 9776, 9777, 9778, 9779]),\n",
       " array([6210, 6211, 6212, 6213, 6214, 6215, 6216, 6217, 6218, 6219]),\n",
       " array([5730, 5731, 5732, 5733, 5734, 5735, 5736, 5737, 5738, 5739]),\n",
       " array([6640, 6641, 6642, 6643, 6644, 6645, 6646, 6647, 6648, 6649]),\n",
       " array([7940, 7941, 7942, 7943, 7944, 7945, 7946, 7947, 7948, 7949]),\n",
       " array([180, 181, 182, 183, 184, 185, 186, 187, 188, 189]),\n",
       " array([1960, 1961, 1962, 1963, 1964, 1965, 1966, 1967, 1968, 1969]),\n",
       " array([9970, 9971, 9972, 9973, 9974, 9975, 9976, 9977, 9978, 9979]),\n",
       " array([2580, 2581, 2582, 2583, 2584, 2585, 2586, 2587, 2588, 2589]),\n",
       " array([5830, 5831, 5832, 5833, 5834, 5835, 5836, 5837, 5838, 5839]),\n",
       " array([2910, 2911, 2912, 2913, 2914, 2915, 2916, 2917, 2918, 2919]),\n",
       " array([9370, 9371, 9372, 9373, 9374, 9375, 9376, 9377, 9378, 9379]),\n",
       " array([4810, 4811, 4812, 4813, 4814, 4815, 4816, 4817, 4818, 4819]),\n",
       " array([8640, 8641, 8642, 8643, 8644, 8645, 8646, 8647, 8648, 8649]),\n",
       " array([3060, 3061, 3062, 3063, 3064, 3065, 3066, 3067, 3068, 3069]),\n",
       " array([7530, 7531, 7532, 7533, 7534, 7535, 7536, 7537, 7538, 7539]),\n",
       " array([8110, 8111, 8112, 8113, 8114, 8115, 8116, 8117, 8118, 8119]),\n",
       " array([6740, 6741, 6742, 6743, 6744, 6745, 6746, 6747, 6748, 6749]),\n",
       " array([3640, 3641, 3642, 3643, 3644, 3645, 3646, 3647, 3648, 3649]),\n",
       " array([6920, 6921, 6922, 6923, 6924, 6925, 6926, 6927, 6928, 6929]),\n",
       " array([6690, 6691, 6692, 6693, 6694, 6695, 6696, 6697, 6698, 6699]),\n",
       " array([9600, 9601, 9602, 9603, 9604, 9605, 9606, 9607, 9608, 9609]),\n",
       " array([6990, 6991, 6992, 6993, 6994, 6995, 6996, 6997, 6998, 6999]),\n",
       " array([3730, 3731, 3732, 3733, 3734, 3735, 3736, 3737, 3738, 3739]),\n",
       " array([6250, 6251, 6252, 6253, 6254, 6255, 6256, 6257, 6258, 6259]),\n",
       " array([8370, 8371, 8372, 8373, 8374, 8375, 8376, 8377, 8378, 8379]),\n",
       " array([6110, 6111, 6112, 6113, 6114, 6115, 6116, 6117, 6118, 6119]),\n",
       " array([9740, 9741, 9742, 9743, 9744, 9745, 9746, 9747, 9748, 9749]),\n",
       " array([3400, 3401, 3402, 3403, 3404, 3405, 3406, 3407, 3408, 3409]),\n",
       " array([9870, 9871, 9872, 9873, 9874, 9875, 9876, 9877, 9878, 9879]),\n",
       " array([5820, 5821, 5822, 5823, 5824, 5825, 5826, 5827, 5828, 5829]),\n",
       " array([8130, 8131, 8132, 8133, 8134, 8135, 8136, 8137, 8138, 8139]),\n",
       " array([4830, 4831, 4832, 4833, 4834, 4835, 4836, 4837, 4838, 4839]),\n",
       " array([4160, 4161, 4162, 4163, 4164, 4165, 4166, 4167, 4168, 4169]),\n",
       " array([440, 441, 442, 443, 444, 445, 446, 447, 448, 449]),\n",
       " array([2210, 2211, 2212, 2213, 2214, 2215, 2216, 2217, 2218, 2219]),\n",
       " array([3020, 3021, 3022, 3023, 3024, 3025, 3026, 3027, 3028, 3029]),\n",
       " array([6430, 6431, 6432, 6433, 6434, 6435, 6436, 6437, 6438, 6439]),\n",
       " array([8970, 8971, 8972, 8973, 8974, 8975, 8976, 8977, 8978, 8979]),\n",
       " array([2770, 2771, 2772, 2773, 2774, 2775, 2776, 2777, 2778, 2779]),\n",
       " array([3390, 3391, 3392, 3393, 3394, 3395, 3396, 3397, 3398, 3399]),\n",
       " array([7880, 7881, 7882, 7883, 7884, 7885, 7886, 7887, 7888, 7889]),\n",
       " array([2950, 2951, 2952, 2953, 2954, 2955, 2956, 2957, 2958, 2959]),\n",
       " array([4960, 4961, 4962, 4963, 4964, 4965, 4966, 4967, 4968, 4969]),\n",
       " array([3370, 3371, 3372, 3373, 3374, 3375, 3376, 3377, 3378, 3379]),\n",
       " array([5140, 5141, 5142, 5143, 5144, 5145, 5146, 5147, 5148, 5149]),\n",
       " array([2700, 2701, 2702, 2703, 2704, 2705, 2706, 2707, 2708, 2709]),\n",
       " array([2160, 2161, 2162, 2163, 2164, 2165, 2166, 2167, 2168, 2169]),\n",
       " array([2880, 2881, 2882, 2883, 2884, 2885, 2886, 2887, 2888, 2889]),\n",
       " array([4820, 4821, 4822, 4823, 4824, 4825, 4826, 4827, 4828, 4829]),\n",
       " array([6770, 6771, 6772, 6773, 6774, 6775, 6776, 6777, 6778, 6779]),\n",
       " array([6240, 6241, 6242, 6243, 6244, 6245, 6246, 6247, 6248, 6249]),\n",
       " array([3360, 3361, 3362, 3363, 3364, 3365, 3366, 3367, 3368, 3369]),\n",
       " array([6880, 6881, 6882, 6883, 6884, 6885, 6886, 6887, 6888, 6889]),\n",
       " array([2450, 2451, 2452, 2453, 2454, 2455, 2456, 2457, 2458, 2459]),\n",
       " array([3440, 3441, 3442, 3443, 3444, 3445, 3446, 3447, 3448, 3449]),\n",
       " array([4080, 4081, 4082, 4083, 4084, 4085, 4086, 4087, 4088, 4089]),\n",
       " array([3150, 3151, 3152, 3153, 3154, 3155, 3156, 3157, 3158, 3159]),\n",
       " array([9100, 9101, 9102, 9103, 9104, 9105, 9106, 9107, 9108, 9109]),\n",
       " array([7720, 7721, 7722, 7723, 7724, 7725, 7726, 7727, 7728, 7729]),\n",
       " array([480, 481, 482, 483, 484, 485, 486, 487, 488, 489]),\n",
       " array([8220, 8221, 8222, 8223, 8224, 8225, 8226, 8227, 8228, 8229]),\n",
       " array([3140, 3141, 3142, 3143, 3144, 3145, 3146, 3147, 3148, 3149]),\n",
       " array([7650, 7651, 7652, 7653, 7654, 7655, 7656, 7657, 7658, 7659]),\n",
       " array([1860, 1861, 1862, 1863, 1864, 1865, 1866, 1867, 1868, 1869]),\n",
       " array([4730, 4731, 4732, 4733, 4734, 4735, 4736, 4737, 4738, 4739]),\n",
       " array([9640, 9641, 9642, 9643, 9644, 9645, 9646, 9647, 9648, 9649]),\n",
       " array([800, 801, 802, 803, 804, 805, 806, 807, 808, 809]),\n",
       " array([8610, 8611, 8612, 8613, 8614, 8615, 8616, 8617, 8618, 8619]),\n",
       " array([2400, 2401, 2402, 2403, 2404, 2405, 2406, 2407, 2408, 2409]),\n",
       " array([3510, 3511, 3512, 3513, 3514, 3515, 3516, 3517, 3518, 3519]),\n",
       " array([9810, 9811, 9812, 9813, 9814, 9815, 9816, 9817, 9818, 9819]),\n",
       " array([9340, 9341, 9342, 9343, 9344, 9345, 9346, 9347, 9348, 9349]),\n",
       " array([8080, 8081, 8082, 8083, 8084, 8085, 8086, 8087, 8088, 8089]),\n",
       " array([3910, 3911, 3912, 3913, 3914, 3915, 3916, 3917, 3918, 3919]),\n",
       " array([5410, 5411, 5412, 5413, 5414, 5415, 5416, 5417, 5418, 5419]),\n",
       " array([4660, 4661, 4662, 4663, 4664, 4665, 4666, 4667, 4668, 4669]),\n",
       " array([4100, 4101, 4102, 4103, 4104, 4105, 4106, 4107, 4108, 4109]),\n",
       " array([610, 611, 612, 613, 614, 615, 616, 617, 618, 619]),\n",
       " array([8400, 8401, 8402, 8403, 8404, 8405, 8406, 8407, 8408, 8409]),\n",
       " array([3000, 3001, 3002, 3003, 3004, 3005, 3006, 3007, 3008, 3009]),\n",
       " array([6720, 6721, 6722, 6723, 6724, 6725, 6726, 6727, 6728, 6729]),\n",
       " array([2480, 2481, 2482, 2483, 2484, 2485, 2486, 2487, 2488, 2489]),\n",
       " array([5860, 5861, 5862, 5863, 5864, 5865, 5866, 5867, 5868, 5869]),\n",
       " array([6280, 6281, 6282, 6283, 6284, 6285, 6286, 6287, 6288, 6289]),\n",
       " array([3480, 3481, 3482, 3483, 3484, 3485, 3486, 3487, 3488, 3489]),\n",
       " array([2460, 2461, 2462, 2463, 2464, 2465, 2466, 2467, 2468, 2469]),\n",
       " array([1990, 1991, 1992, 1993, 1994, 1995, 1996, 1997, 1998, 1999]),\n",
       " array([3890, 3891, 3892, 3893, 3894, 3895, 3896, 3897, 3898, 3899]),\n",
       " array([6890, 6891, 6892, 6893, 6894, 6895, 6896, 6897, 6898, 6899]),\n",
       " array([1760, 1761, 1762, 1763, 1764, 1765, 1766, 1767, 1768, 1769]),\n",
       " array([100, 101, 102, 103, 104, 105, 106, 107, 108, 109]),\n",
       " array([910, 911, 912, 913, 914, 915, 916, 917, 918, 919]),\n",
       " array([6350, 6351, 6352, 6353, 6354, 6355, 6356, 6357, 6358, 6359]),\n",
       " array([2860, 2861, 2862, 2863, 2864, 2865, 2866, 2867, 2868, 2869]),\n",
       " array([9310, 9311, 9312, 9313, 9314, 9315, 9316, 9317, 9318, 9319]),\n",
       " array([5210, 5211, 5212, 5213, 5214, 5215, 5216, 5217, 5218, 5219]),\n",
       " array([7730, 7731, 7732, 7733, 7734, 7735, 7736, 7737, 7738, 7739]),\n",
       " array([2670, 2671, 2672, 2673, 2674, 2675, 2676, 2677, 2678, 2679]),\n",
       " array([5950, 5951, 5952, 5953, 5954, 5955, 5956, 5957, 5958, 5959]),\n",
       " array([6270, 6271, 6272, 6273, 6274, 6275, 6276, 6277, 6278, 6279]),\n",
       " array([3880, 3881, 3882, 3883, 3884, 3885, 3886, 3887, 3888, 3889]),\n",
       " array([4140, 4141, 4142, 4143, 4144, 4145, 4146, 4147, 4148, 4149]),\n",
       " array([460, 461, 462, 463, 464, 465, 466, 467, 468, 469]),\n",
       " array([7290, 7291, 7292, 7293, 7294, 7295, 7296, 7297, 7298, 7299]),\n",
       " array([9150, 9151, 9152, 9153, 9154, 9155, 9156, 9157, 9158, 9159]),\n",
       " array([3750, 3751, 3752, 3753, 3754, 3755, 3756, 3757, 3758, 3759]),\n",
       " array([9470, 9471, 9472, 9473, 9474, 9475, 9476, 9477, 9478, 9479]),\n",
       " array([260, 261, 262, 263, 264, 265, 266, 267, 268, 269]),\n",
       " array([9330, 9331, 9332, 9333, 9334, 9335, 9336, 9337, 9338, 9339]),\n",
       " array([2060, 2061, 2062, 2063, 2064, 2065, 2066, 2067, 2068, 2069]),\n",
       " array([7630, 7631, 7632, 7633, 7634, 7635, 7636, 7637, 7638, 7639]),\n",
       " array([2290, 2291, 2292, 2293, 2294, 2295, 2296, 2297, 2298, 2299]),\n",
       " array([2070, 2071, 2072, 2073, 2074, 2075, 2076, 2077, 2078, 2079]),\n",
       " array([500, 501, 502, 503, 504, 505, 506, 507, 508, 509]),\n",
       " array([1920, 1921, 1922, 1923, 1924, 1925, 1926, 1927, 1928, 1929]),\n",
       " array([6460, 6461, 6462, 6463, 6464, 6465, 6466, 6467, 6468, 6469]),\n",
       " array([6290, 6291, 6292, 6293, 6294, 6295, 6296, 6297, 6298, 6299]),\n",
       " array([1050, 1051, 1052, 1053, 1054, 1055, 1056, 1057, 1058, 1059]),\n",
       " array([9630, 9631, 9632, 9633, 9634, 9635, 9636, 9637, 9638, 9639]),\n",
       " array([3930, 3931, 3932, 3933, 3934, 3935, 3936, 3937, 3938, 3939]),\n",
       " array([6520, 6521, 6522, 6523, 6524, 6525, 6526, 6527, 6528, 6529]),\n",
       " array([3130, 3131, 3132, 3133, 3134, 3135, 3136, 3137, 3138, 3139]),\n",
       " array([1710, 1711, 1712, 1713, 1714, 1715, 1716, 1717, 1718, 1719]),\n",
       " array([650, 651, 652, 653, 654, 655, 656, 657, 658, 659]),\n",
       " array([710, 711, 712, 713, 714, 715, 716, 717, 718, 719]),\n",
       " array([3340, 3341, 3342, 3343, 3344, 3345, 3346, 3347, 3348, 3349]),\n",
       " array([9980, 9981, 9982, 9983, 9984, 9985, 9986, 9987, 9988, 9989]),\n",
       " array([5080, 5081, 5082, 5083, 5084, 5085, 5086, 5087, 5088, 5089]),\n",
       " array([6970, 6971, 6972, 6973, 6974, 6975, 6976, 6977, 6978, 6979]),\n",
       " array([9900, 9901, 9902, 9903, 9904, 9905, 9906, 9907, 9908, 9909]),\n",
       " array([8990, 8991, 8992, 8993, 8994, 8995, 8996, 8997, 8998, 8999]),\n",
       " array([5420, 5421, 5422, 5423, 5424, 5425, 5426, 5427, 5428, 5429]),\n",
       " array([8450, 8451, 8452, 8453, 8454, 8455, 8456, 8457, 8458, 8459]),\n",
       " array([8590, 8591, 8592, 8593, 8594, 8595, 8596, 8597, 8598, 8599]),\n",
       " array([560, 561, 562, 563, 564, 565, 566, 567, 568, 569]),\n",
       " array([7150, 7151, 7152, 7153, 7154, 7155, 7156, 7157, 7158, 7159]),\n",
       " array([3070, 3071, 3072, 3073, 3074, 3075, 3076, 3077, 3078, 3079]),\n",
       " array([2650, 2651, 2652, 2653, 2654, 2655, 2656, 2657, 2658, 2659]),\n",
       " array([6390, 6391, 6392, 6393, 6394, 6395, 6396, 6397, 6398, 6399]),\n",
       " array([4580, 4581, 4582, 4583, 4584, 4585, 4586, 4587, 4588, 4589]),\n",
       " array([3970, 3971, 3972, 3973, 3974, 3975, 3976, 3977, 3978, 3979]),\n",
       " array([9050, 9051, 9052, 9053, 9054, 9055, 9056, 9057, 9058, 9059]),\n",
       " array([7340, 7341, 7342, 7343, 7344, 7345, 7346, 7347, 7348, 7349]),\n",
       " array([9660, 9661, 9662, 9663, 9664, 9665, 9666, 9667, 9668, 9669]),\n",
       " array([9160, 9161, 9162, 9163, 9164, 9165, 9166, 9167, 9168, 9169]),\n",
       " array([9450, 9451, 9452, 9453, 9454, 9455, 9456, 9457, 9458, 9459]),\n",
       " array([4700, 4701, 4702, 4703, 4704, 4705, 4706, 4707, 4708, 4709]),\n",
       " array([7860, 7861, 7862, 7863, 7864, 7865, 7866, 7867, 7868, 7869]),\n",
       " array([9940, 9941, 9942, 9943, 9944, 9945, 9946, 9947, 9948, 9949]),\n",
       " array([9200, 9201, 9202, 9203, 9204, 9205, 9206, 9207, 9208, 9209]),\n",
       " array([160, 161, 162, 163, 164, 165, 166, 167, 168, 169]),\n",
       " array([390, 391, 392, 393, 394, 395, 396, 397, 398, 399]),\n",
       " array([9430, 9431, 9432, 9433, 9434, 9435, 9436, 9437, 9438, 9439]),\n",
       " array([8270, 8271, 8272, 8273, 8274, 8275, 8276, 8277, 8278, 8279]),\n",
       " array([3540, 3541, 3542, 3543, 3544, 3545, 3546, 3547, 3548, 3549]),\n",
       " array([6190, 6191, 6192, 6193, 6194, 6195, 6196, 6197, 6198, 6199]),\n",
       " array([2040, 2041, 2042, 2043, 2044, 2045, 2046, 2047, 2048, 2049]),\n",
       " array([9170, 9171, 9172, 9173, 9174, 9175, 9176, 9177, 9178, 9179]),\n",
       " array([4260, 4261, 4262, 4263, 4264, 4265, 4266, 4267, 4268, 4269]),\n",
       " array([9230, 9231, 9232, 9233, 9234, 9235, 9236, 9237, 9238, 9239]),\n",
       " array([2180, 2181, 2182, 2183, 2184, 2185, 2186, 2187, 2188, 2189]),\n",
       " array([7600, 7601, 7602, 7603, 7604, 7605, 7606, 7607, 7608, 7609]),\n",
       " array([5500, 5501, 5502, 5503, 5504, 5505, 5506, 5507, 5508, 5509]),\n",
       " array([550, 551, 552, 553, 554, 555, 556, 557, 558, 559]),\n",
       " array([7360, 7361, 7362, 7363, 7364, 7365, 7366, 7367, 7368, 7369]),\n",
       " array([5090, 5091, 5092, 5093, 5094, 5095, 5096, 5097, 5098, 5099]),\n",
       " array([1690, 1691, 1692, 1693, 1694, 1695, 1696, 1697, 1698, 1699]),\n",
       " array([8260, 8261, 8262, 8263, 8264, 8265, 8266, 8267, 8268, 8269]),\n",
       " array([2850, 2851, 2852, 2853, 2854, 2855, 2856, 2857, 2858, 2859]),\n",
       " array([90, 91, 92, 93, 94, 95, 96, 97, 98, 99]),\n",
       " array([8850, 8851, 8852, 8853, 8854, 8855, 8856, 8857, 8858, 8859]),\n",
       " array([1270, 1271, 1272, 1273, 1274, 1275, 1276, 1277, 1278, 1279]),\n",
       " array([1350, 1351, 1352, 1353, 1354, 1355, 1356, 1357, 1358, 1359]),\n",
       " array([8030, 8031, 8032, 8033, 8034, 8035, 8036, 8037, 8038, 8039]),\n",
       " array([2680, 2681, 2682, 2683, 2684, 2685, 2686, 2687, 2688, 2689]),\n",
       " array([8720, 8721, 8722, 8723, 8724, 8725, 8726, 8727, 8728, 8729]),\n",
       " array([9260, 9261, 9262, 9263, 9264, 9265, 9266, 9267, 9268, 9269]),\n",
       " array([9990, 9991, 9992, 9993, 9994, 9995, 9996, 9997, 9998, 9999]),\n",
       " array([9030, 9031, 9032, 9033, 9034, 9035, 9036, 9037, 9038, 9039]),\n",
       " array([8320, 8321, 8322, 8323, 8324, 8325, 8326, 8327, 8328, 8329]),\n",
       " array([8050, 8051, 8052, 8053, 8054, 8055, 8056, 8057, 8058, 8059]),\n",
       " array([2380, 2381, 2382, 2383, 2384, 2385, 2386, 2387, 2388, 2389]),\n",
       " array([7060, 7061, 7062, 7063, 7064, 7065, 7066, 7067, 7068, 7069]),\n",
       " array([4380, 4381, 4382, 4383, 4384, 4385, 4386, 4387, 4388, 4389]),\n",
       " array([2510, 2511, 2512, 2513, 2514, 2515, 2516, 2517, 2518, 2519]),\n",
       " array([3550, 3551, 3552, 3553, 3554, 3555, 3556, 3557, 3558, 3559]),\n",
       " array([3260, 3261, 3262, 3263, 3264, 3265, 3266, 3267, 3268, 3269]),\n",
       " array([4000, 4001, 4002, 4003, 4004, 4005, 4006, 4007, 4008, 4009]),\n",
       " array([4250, 4251, 4252, 4253, 4254, 4255, 4256, 4257, 4258, 4259]),\n",
       " array([8920, 8921, 8922, 8923, 8924, 8925, 8926, 8927, 8928, 8929]),\n",
       " array([1130, 1131, 1132, 1133, 1134, 1135, 1136, 1137, 1138, 1139]),\n",
       " array([9120, 9121, 9122, 9123, 9124, 9125, 9126, 9127, 9128, 9129]),\n",
       " array([590, 591, 592, 593, 594, 595, 596, 597, 598, 599]),\n",
       " array([4630, 4631, 4632, 4633, 4634, 4635, 4636, 4637, 4638, 4639]),\n",
       " array([7970, 7971, 7972, 7973, 7974, 7975, 7976, 7977, 7978, 7979]),\n",
       " array([4690, 4691, 4692, 4693, 4694, 4695, 4696, 4697, 4698, 4699]),\n",
       " array([9490, 9491, 9492, 9493, 9494, 9495, 9496, 9497, 9498, 9499]),\n",
       " array([9460, 9461, 9462, 9463, 9464, 9465, 9466, 9467, 9468, 9469]),\n",
       " array([9040, 9041, 9042, 9043, 9044, 9045, 9046, 9047, 9048, 9049]),\n",
       " array([4520, 4521, 4522, 4523, 4524, 4525, 4526, 4527, 4528, 4529]),\n",
       " array([5460, 5461, 5462, 5463, 5464, 5465, 5466, 5467, 5468, 5469]),\n",
       " array([2330, 2331, 2332, 2333, 2334, 2335, 2336, 2337, 2338, 2339]),\n",
       " array([9910, 9911, 9912, 9913, 9914, 9915, 9916, 9917, 9918, 9919]),\n",
       " array([1460, 1461, 1462, 1463, 1464, 1465, 1466, 1467, 1468, 1469]),\n",
       " array([270, 271, 272, 273, 274, 275, 276, 277, 278, 279]),\n",
       " array([80, 81, 82, 83, 84, 85, 86, 87, 88, 89]),\n",
       " array([8900, 8901, 8902, 8903, 8904, 8905, 8906, 8907, 8908, 8909]),\n",
       " array([3470, 3471, 3472, 3473, 3474, 3475, 3476, 3477, 3478, 3479]),\n",
       " array([580, 581, 582, 583, 584, 585, 586, 587, 588, 589]),\n",
       " array([6150, 6151, 6152, 6153, 6154, 6155, 6156, 6157, 6158, 6159]),\n",
       " array([3040, 3041, 3042, 3043, 3044, 3045, 3046, 3047, 3048, 3049]),\n",
       " array([1540, 1541, 1542, 1543, 1544, 1545, 1546, 1547, 1548, 1549]),\n",
       " array([230, 231, 232, 233, 234, 235, 236, 237, 238, 239]),\n",
       " array([3380, 3381, 3382, 3383, 3384, 3385, 3386, 3387, 3388, 3389]),\n",
       " array([6020, 6021, 6022, 6023, 6024, 6025, 6026, 6027, 6028, 6029]),\n",
       " array([130, 131, 132, 133, 134, 135, 136, 137, 138, 139]),\n",
       " array([40, 41, 42, 43, 44, 45, 46, 47, 48, 49]),\n",
       " array([3110, 3111, 3112, 3113, 3114, 3115, 3116, 3117, 3118, 3119]),\n",
       " array([2890, 2891, 2892, 2893, 2894, 2895, 2896, 2897, 2898, 2899]),\n",
       " array([5690, 5691, 5692, 5693, 5694, 5695, 5696, 5697, 5698, 5699]),\n",
       " array([3350, 3351, 3352, 3353, 3354, 3355, 3356, 3357, 3358, 3359]),\n",
       " array([3230, 3231, 3232, 3233, 3234, 3235, 3236, 3237, 3238, 3239]),\n",
       " array([7030, 7031, 7032, 7033, 7034, 7035, 7036, 7037, 7038, 7039]),\n",
       " array([5890, 5891, 5892, 5893, 5894, 5895, 5896, 5897, 5898, 5899]),\n",
       " array([290, 291, 292, 293, 294, 295, 296, 297, 298, 299]),\n",
       " array([4010, 4011, 4012, 4013, 4014, 4015, 4016, 4017, 4018, 4019]),\n",
       " array([6230, 6231, 6232, 6233, 6234, 6235, 6236, 6237, 6238, 6239]),\n",
       " array([2530, 2531, 2532, 2533, 2534, 2535, 2536, 2537, 2538, 2539]),\n",
       " array([6550, 6551, 6552, 6553, 6554, 6555, 6556, 6557, 6558, 6559]),\n",
       " array([3570, 3571, 3572, 3573, 3574, 3575, 3576, 3577, 3578, 3579]),\n",
       " array([9400, 9401, 9402, 9403, 9404, 9405, 9406, 9407, 9408, 9409]),\n",
       " array([4270, 4271, 4272, 4273, 4274, 4275, 4276, 4277, 4278, 4279]),\n",
       " array([5440, 5441, 5442, 5443, 5444, 5445, 5446, 5447, 5448, 5449]),\n",
       " array([4470, 4471, 4472, 4473, 4474, 4475, 4476, 4477, 4478, 4479]),\n",
       " array([6050, 6051, 6052, 6053, 6054, 6055, 6056, 6057, 6058, 6059]),\n",
       " array([5740, 5741, 5742, 5743, 5744, 5745, 5746, 5747, 5748, 5749]),\n",
       " array([5400, 5401, 5402, 5403, 5404, 5405, 5406, 5407, 5408, 5409]),\n",
       " array([7580, 7581, 7582, 7583, 7584, 7585, 7586, 7587, 7588, 7589]),\n",
       " array([1950, 1951, 1952, 1953, 1954, 1955, 1956, 1957, 1958, 1959]),\n",
       " array([6780, 6781, 6782, 6783, 6784, 6785, 6786, 6787, 6788, 6789]),\n",
       " array([2630, 2631, 2632, 2633, 2634, 2635, 2636, 2637, 2638, 2639]),\n",
       " array([3490, 3491, 3492, 3493, 3494, 3495, 3496, 3497, 3498, 3499]),\n",
       " array([4530, 4531, 4532, 4533, 4534, 4535, 4536, 4537, 4538, 4539]),\n",
       " array([3710, 3711, 3712, 3713, 3714, 3715, 3716, 3717, 3718, 3719]),\n",
       " array([670, 671, 672, 673, 674, 675, 676, 677, 678, 679]),\n",
       " array([9360, 9361, 9362, 9363, 9364, 9365, 9366, 9367, 9368, 9369]),\n",
       " array([7620, 7621, 7622, 7623, 7624, 7625, 7626, 7627, 7628, 7629]),\n",
       " array([9620, 9621, 9622, 9623, 9624, 9625, 9626, 9627, 9628, 9629]),\n",
       " array([1280, 1281, 1282, 1283, 1284, 1285, 1286, 1287, 1288, 1289]),\n",
       " array([3010, 3011, 3012, 3013, 3014, 3015, 3016, 3017, 3018, 3019]),\n",
       " array([5370, 5371, 5372, 5373, 5374, 5375, 5376, 5377, 5378, 5379]),\n",
       " array([5310, 5311, 5312, 5313, 5314, 5315, 5316, 5317, 5318, 5319]),\n",
       " array([340, 341, 342, 343, 344, 345, 346, 347, 348, 349]),\n",
       " array([5290, 5291, 5292, 5293, 5294, 5295, 5296, 5297, 5298, 5299]),\n",
       " array([150, 151, 152, 153, 154, 155, 156, 157, 158, 159]),\n",
       " array([30, 31, 32, 33, 34, 35, 36, 37, 38, 39]),\n",
       " array([5660, 5661, 5662, 5663, 5664, 5665, 5666, 5667, 5668, 5669]),\n",
       " array([2390, 2391, 2392, 2393, 2394, 2395, 2396, 2397, 2398, 2399]),\n",
       " array([7560, 7561, 7562, 7563, 7564, 7565, 7566, 7567, 7568, 7569]),\n",
       " array([4560, 4561, 4562, 4563, 4564, 4565, 4566, 4567, 4568, 4569]),\n",
       " array([6300, 6301, 6302, 6303, 6304, 6305, 6306, 6307, 6308, 6309]),\n",
       " array([6030, 6031, 6032, 6033, 6034, 6035, 6036, 6037, 6038, 6039]),\n",
       " array([170, 171, 172, 173, 174, 175, 176, 177, 178, 179]),\n",
       " array([4680, 4681, 4682, 4683, 4684, 4685, 4686, 4687, 4688, 4689]),\n",
       " array([5520, 5521, 5522, 5523, 5524, 5525, 5526, 5527, 5528, 5529]),\n",
       " array([2200, 2201, 2202, 2203, 2204, 2205, 2206, 2207, 2208, 2209]),\n",
       " array([4880, 4881, 4882, 4883, 4884, 4885, 4886, 4887, 4888, 4889]),\n",
       " array([9010, 9011, 9012, 9013, 9014, 9015, 9016, 9017, 9018, 9019]),\n",
       " array([4720, 4721, 4722, 4723, 4724, 4725, 4726, 4727, 4728, 4729]),\n",
       " array([6540, 6541, 6542, 6543, 6544, 6545, 6546, 6547, 6548, 6549]),\n",
       " array([5560, 5561, 5562, 5563, 5564, 5565, 5566, 5567, 5568, 5569]),\n",
       " array([1260, 1261, 1262, 1263, 1264, 1265, 1266, 1267, 1268, 1269]),\n",
       " array([9920, 9921, 9922, 9923, 9924, 9925, 9926, 9927, 9928, 9929]),\n",
       " array([7220, 7221, 7222, 7223, 7224, 7225, 7226, 7227, 7228, 7229]),\n",
       " array([2110, 2111, 2112, 2113, 2114, 2115, 2116, 2117, 2118, 2119]),\n",
       " array([3760, 3761, 3762, 3763, 3764, 3765, 3766, 3767, 3768, 3769]),\n",
       " array([8410, 8411, 8412, 8413, 8414, 8415, 8416, 8417, 8418, 8419]),\n",
       " array([7280, 7281, 7282, 7283, 7284, 7285, 7286, 7287, 7288, 7289]),\n",
       " array([7750, 7751, 7752, 7753, 7754, 7755, 7756, 7757, 7758, 7759]),\n",
       " array([8350, 8351, 8352, 8353, 8354, 8355, 8356, 8357, 8358, 8359]),\n",
       " array([220, 221, 222, 223, 224, 225, 226, 227, 228, 229]),\n",
       " array([4050, 4051, 4052, 4053, 4054, 4055, 4056, 4057, 4058, 4059]),\n",
       " array([5150, 5151, 5152, 5153, 5154, 5155, 5156, 5157, 5158, 5159]),\n",
       " array([9110, 9111, 9112, 9113, 9114, 9115, 9116, 9117, 9118, 9119]),\n",
       " array([6180, 6181, 6182, 6183, 6184, 6185, 6186, 6187, 6188, 6189]),\n",
       " array([1340, 1341, 1342, 1343, 1344, 1345, 1346, 1347, 1348, 1349]),\n",
       " array([2550, 2551, 2552, 2553, 2554, 2555, 2556, 2557, 2558, 2559]),\n",
       " array([5870, 5871, 5872, 5873, 5874, 5875, 5876, 5877, 5878, 5879]),\n",
       " array([6120, 6121, 6122, 6123, 6124, 6125, 6126, 6127, 6128, 6129]),\n",
       " array([6000, 6001, 6002, 6003, 6004, 6005, 6006, 6007, 6008, 6009]),\n",
       " array([5170, 5171, 5172, 5173, 5174, 5175, 5176, 5177, 5178, 5179]),\n",
       " array([980, 981, 982, 983, 984, 985, 986, 987, 988, 989]),\n",
       " array([5960, 5961, 5962, 5963, 5964, 5965, 5966, 5967, 5968, 5969]),\n",
       " array([2410, 2411, 2412, 2413, 2414, 2415, 2416, 2417, 2418, 2419]),\n",
       " array([780, 781, 782, 783, 784, 785, 786, 787, 788, 789]),\n",
       " array([2190, 2191, 2192, 2193, 2194, 2195, 2196, 2197, 2198, 2199]),\n",
       " array([2660, 2661, 2662, 2663, 2664, 2665, 2666, 2667, 2668, 2669]),\n",
       " array([7520, 7521, 7522, 7523, 7524, 7525, 7526, 7527, 7528, 7529]),\n",
       " array([9880, 9881, 9882, 9883, 9884, 9885, 9886, 9887, 9888, 9889]),\n",
       " array([5800, 5801, 5802, 5803, 5804, 5805, 5806, 5807, 5808, 5809]),\n",
       " array([8820, 8821, 8822, 8823, 8824, 8825, 8826, 8827, 8828, 8829]),\n",
       " array([7910, 7911, 7912, 7913, 7914, 7915, 7916, 7917, 7918, 7919]),\n",
       " array([5680, 5681, 5682, 5683, 5684, 5685, 5686, 5687, 5688, 5689]),\n",
       " array([4540, 4541, 4542, 4543, 4544, 4545, 4546, 4547, 4548, 4549]),\n",
       " array([7500, 7501, 7502, 7503, 7504, 7505, 7506, 7507, 7508, 7509]),\n",
       " array([430, 431, 432, 433, 434, 435, 436, 437, 438, 439]),\n",
       " array([6620, 6621, 6622, 6623, 6624, 6625, 6626, 6627, 6628, 6629]),\n",
       " array([6380, 6381, 6382, 6383, 6384, 6385, 6386, 6387, 6388, 6389]),\n",
       " array([5480, 5481, 5482, 5483, 5484, 5485, 5486, 5487, 5488, 5489]),\n",
       " array([1770, 1771, 1772, 1773, 1774, 1775, 1776, 1777, 1778, 1779]),\n",
       " array([1780, 1781, 1782, 1783, 1784, 1785, 1786, 1787, 1788, 1789]),\n",
       " array([1020, 1021, 1022, 1023, 1024, 1025, 1026, 1027, 1028, 1029]),\n",
       " array([2870, 2871, 2872, 2873, 2874, 2875, 2876, 2877, 2878, 2879]),\n",
       " array([2150, 2151, 2152, 2153, 2154, 2155, 2156, 2157, 2158, 2159]),\n",
       " array([9690, 9691, 9692, 9693, 9694, 9695, 9696, 9697, 9698, 9699]),\n",
       " array([6950, 6951, 6952, 6953, 6954, 6955, 6956, 6957, 6958, 6959]),\n",
       " array([8940, 8941, 8942, 8943, 8944, 8945, 8946, 8947, 8948, 8949]),\n",
       " array([4320, 4321, 4322, 4323, 4324, 4325, 4326, 4327, 4328, 4329]),\n",
       " array([1410, 1411, 1412, 1413, 1414, 1415, 1416, 1417, 1418, 1419]),\n",
       " array([5910, 5911, 5912, 5913, 5914, 5915, 5916, 5917, 5918, 5919]),\n",
       " array([4120, 4121, 4122, 4123, 4124, 4125, 4126, 4127, 4128, 4129]),\n",
       " array([4650, 4651, 4652, 4653, 4654, 4655, 4656, 4657, 4658, 4659]),\n",
       " array([8730, 8731, 8732, 8733, 8734, 8735, 8736, 8737, 8738, 8739]),\n",
       " array([2790, 2791, 2792, 2793, 2794, 2795, 2796, 2797, 2798, 2799]),\n",
       " array([1570, 1571, 1572, 1573, 1574, 1575, 1576, 1577, 1578, 1579]),\n",
       " array([3320, 3321, 3322, 3323, 3324, 3325, 3326, 3327, 3328, 3329]),\n",
       " array([2470, 2471, 2472, 2473, 2474, 2475, 2476, 2477, 2478, 2479]),\n",
       " array([6910, 6911, 6912, 6913, 6914, 6915, 6916, 6917, 6918, 6919]),\n",
       " array([5760, 5761, 5762, 5763, 5764, 5765, 5766, 5767, 5768, 5769]),\n",
       " array([9380, 9381, 9382, 9383, 9384, 9385, 9386, 9387, 9388, 9389]),\n",
       " array([9510, 9511, 9512, 9513, 9514, 9515, 9516, 9517, 9518, 9519]),\n",
       " array([5000, 5001, 5002, 5003, 5004, 5005, 5006, 5007, 5008, 5009]),\n",
       " array([4900, 4901, 4902, 4903, 4904, 4905, 4906, 4907, 4908, 4909]),\n",
       " array([5810, 5811, 5812, 5813, 5814, 5815, 5816, 5817, 5818, 5819]),\n",
       " array([3170, 3171, 3172, 3173, 3174, 3175, 3176, 3177, 3178, 3179]),\n",
       " array([5610, 5611, 5612, 5613, 5614, 5615, 5616, 5617, 5618, 5619]),\n",
       " array([60, 61, 62, 63, 64, 65, 66, 67, 68, 69]),\n",
       " array([1330, 1331, 1332, 1333, 1334, 1335, 1336, 1337, 1338, 1339]),\n",
       " array([1610, 1611, 1612, 1613, 1614, 1615, 1616, 1617, 1618, 1619]),\n",
       " array([2490, 2491, 2492, 2493, 2494, 2495, 2496, 2497, 2498, 2499]),\n",
       " array([5110, 5111, 5112, 5113, 5114, 5115, 5116, 5117, 5118, 5119]),\n",
       " array([1310, 1311, 1312, 1313, 1314, 1315, 1316, 1317, 1318, 1319]),\n",
       " array([850, 851, 852, 853, 854, 855, 856, 857, 858, 859]),\n",
       " array([1700, 1701, 1702, 1703, 1704, 1705, 1706, 1707, 1708, 1709]),\n",
       " array([7130, 7131, 7132, 7133, 7134, 7135, 7136, 7137, 7138, 7139]),\n",
       " array([2560, 2561, 2562, 2563, 2564, 2565, 2566, 2567, 2568, 2569]),\n",
       " array([6530, 6531, 6532, 6533, 6534, 6535, 6536, 6537, 6538, 6539]),\n",
       " array([9560, 9561, 9562, 9563, 9564, 9565, 9566, 9567, 9568, 9569]),\n",
       " array([4190, 4191, 4192, 4193, 4194, 4195, 4196, 4197, 4198, 4199]),\n",
       " array([4370, 4371, 4372, 4373, 4374, 4375, 4376, 4377, 4378, 4379]),\n",
       " array([120, 121, 122, 123, 124, 125, 126, 127, 128, 129]),\n",
       " array([1730, 1731, 1732, 1733, 1734, 1735, 1736, 1737, 1738, 1739]),\n",
       " array([3090, 3091, 3092, 3093, 3094, 3095, 3096, 3097, 3098, 3099]),\n",
       " array([3030, 3031, 3032, 3033, 3034, 3035, 3036, 3037, 3038, 3039]),\n",
       " array([5320, 5321, 5322, 5323, 5324, 5325, 5326, 5327, 5328, 5329]),\n",
       " array([4030, 4031, 4032, 4033, 4034, 4035, 4036, 4037, 4038, 4039]),\n",
       " array([7390, 7391, 7392, 7393, 7394, 7395, 7396, 7397, 7398, 7399]),\n",
       " array([7960, 7961, 7962, 7963, 7964, 7965, 7966, 7967, 7968, 7969]),\n",
       " array([3980, 3981, 3982, 3983, 3984, 3985, 3986, 3987, 3988, 3989]),\n",
       " array([8300, 8301, 8302, 8303, 8304, 8305, 8306, 8307, 8308, 8309]),\n",
       " array([3690, 3691, 3692, 3693, 3694, 3695, 3696, 3697, 3698, 3699]),\n",
       " array([9700, 9701, 9702, 9703, 9704, 9705, 9706, 9707, 9708, 9709]),\n",
       " array([5790, 5791, 5792, 5793, 5794, 5795, 5796, 5797, 5798, 5799]),\n",
       " array([6140, 6141, 6142, 6143, 6144, 6145, 6146, 6147, 6148, 6149]),\n",
       " array([3870, 3871, 3872, 3873, 3874, 3875, 3876, 3877, 3878, 3879]),\n",
       " array([6410, 6411, 6412, 6413, 6414, 6415, 6416, 6417, 6418, 6419]),\n",
       " array([4930, 4931, 4932, 4933, 4934, 4935, 4936, 4937, 4938, 4939]),\n",
       " array([8700, 8701, 8702, 8703, 8704, 8705, 8706, 8707, 8708, 8709]),\n",
       " array([4230, 4231, 4232, 4233, 4234, 4235, 4236, 4237, 4238, 4239]),\n",
       " array([3240, 3241, 3242, 3243, 3244, 3245, 3246, 3247, 3248, 3249]),\n",
       " array([5630, 5631, 5632, 5633, 5634, 5635, 5636, 5637, 5638, 5639]),\n",
       " array([6650, 6651, 6652, 6653, 6654, 6655, 6656, 6657, 6658, 6659]),\n",
       " array([9960, 9961, 9962, 9963, 9964, 9965, 9966, 9967, 9968, 9969]),\n",
       " array([7550, 7551, 7552, 7553, 7554, 7555, 7556, 7557, 7558, 7559]),\n",
       " array([250, 251, 252, 253, 254, 255, 256, 257, 258, 259]),\n",
       " array([5330, 5331, 5332, 5333, 5334, 5335, 5336, 5337, 5338, 5339]),\n",
       " array([6090, 6091, 6092, 6093, 6094, 6095, 6096, 6097, 6098, 6099]),\n",
       " array([7170, 7171, 7172, 7173, 7174, 7175, 7176, 7177, 7178, 7179]),\n",
       " array([7490, 7491, 7492, 7493, 7494, 7495, 7496, 7497, 7498, 7499]),\n",
       " array([1810, 1811, 1812, 1813, 1814, 1815, 1816, 1817, 1818, 1819]),\n",
       " array([3280, 3281, 3282, 3283, 3284, 3285, 3286, 3287, 3288, 3289]),\n",
       " array([7070, 7071, 7072, 7073, 7074, 7075, 7076, 7077, 7078, 7079]),\n",
       " array([5580, 5581, 5582, 5583, 5584, 5585, 5586, 5587, 5588, 5589]),\n",
       " array([540, 541, 542, 543, 544, 545, 546, 547, 548, 549]),\n",
       " array([1820, 1821, 1822, 1823, 1824, 1825, 1826, 1827, 1828, 1829]),\n",
       " array([4970, 4971, 4972, 4973, 4974, 4975, 4976, 4977, 4978, 4979]),\n",
       " array([2250, 2251, 2252, 2253, 2254, 2255, 2256, 2257, 2258, 2259]),\n",
       " array([3220, 3221, 3222, 3223, 3224, 3225, 3226, 3227, 3228, 3229]),\n",
       " array([8380, 8381, 8382, 8383, 8384, 8385, 8386, 8387, 8388, 8389]),\n",
       " array([1720, 1721, 1722, 1723, 1724, 1725, 1726, 1727, 1728, 1729]),\n",
       " array([5430, 5431, 5432, 5433, 5434, 5435, 5436, 5437, 5438, 5439]),\n",
       " array([9780, 9781, 9782, 9783, 9784, 9785, 9786, 9787, 9788, 9789]),\n",
       " array([4770, 4771, 4772, 4773, 4774, 4775, 4776, 4777, 4778, 4779]),\n",
       " array([3520, 3521, 3522, 3523, 3524, 3525, 3526, 3527, 3528, 3529]),\n",
       " array([5300, 5301, 5302, 5303, 5304, 5305, 5306, 5307, 5308, 5309]),\n",
       " array([6260, 6261, 6262, 6263, 6264, 6265, 6266, 6267, 6268, 6269]),\n",
       " array([7450, 7451, 7452, 7453, 7454, 7455, 7456, 7457, 7458, 7459]),\n",
       " array([7180, 7181, 7182, 7183, 7184, 7185, 7186, 7187, 7188, 7189]),\n",
       " array([3790, 3791, 3792, 3793, 3794, 3795, 3796, 3797, 3798, 3799]),\n",
       " array([7200, 7201, 7202, 7203, 7204, 7205, 7206, 7207, 7208, 7209]),\n",
       " array([3600, 3601, 3602, 3603, 3604, 3605, 3606, 3607, 3608, 3609]),\n",
       " array([4870, 4871, 4872, 4873, 4874, 4875, 4876, 4877, 4878, 4879]),\n",
       " array([410, 411, 412, 413, 414, 415, 416, 417, 418, 419]),\n",
       " array([8930, 8931, 8932, 8933, 8934, 8935, 8936, 8937, 8938, 8939]),\n",
       " array([450, 451, 452, 453, 454, 455, 456, 457, 458, 459]),\n",
       " array([1300, 1301, 1302, 1303, 1304, 1305, 1306, 1307, 1308, 1309]),\n",
       " array([7570, 7571, 7572, 7573, 7574, 7575, 7576, 7577, 7578, 7579]),\n",
       " array([6510, 6511, 6512, 6513, 6514, 6515, 6516, 6517, 6518, 6519]),\n",
       " array([790, 791, 792, 793, 794, 795, 796, 797, 798, 799]),\n",
       " array([7330, 7331, 7332, 7333, 7334, 7335, 7336, 7337, 7338, 7339]),\n",
       " array([5040, 5041, 5042, 5043, 5044, 5045, 5046, 5047, 5048, 5049]),\n",
       " array([3180, 3181, 3182, 3183, 3184, 3185, 3186, 3187, 3188, 3189]),\n",
       " array([2340, 2341, 2342, 2343, 2344, 2345, 2346, 2347, 2348, 2349]),\n",
       " array([2840, 2841, 2842, 2843, 2844, 2845, 2846, 2847, 2848, 2849]),\n",
       " array([4780, 4781, 4782, 4783, 4784, 4785, 4786, 4787, 4788, 4789]),\n",
       " array([1630, 1631, 1632, 1633, 1634, 1635, 1636, 1637, 1638, 1639]),\n",
       " array([3700, 3701, 3702, 3703, 3704, 3705, 3706, 3707, 3708, 3709]),\n",
       " array([6310, 6311, 6312, 6313, 6314, 6315, 6316, 6317, 6318, 6319]),\n",
       " array([350, 351, 352, 353, 354, 355, 356, 357, 358, 359]),\n",
       " array([4210, 4211, 4212, 4213, 4214, 4215, 4216, 4217, 4218, 4219]),\n",
       " array([8830, 8831, 8832, 8833, 8834, 8835, 8836, 8837, 8838, 8839]),\n",
       " array([7930, 7931, 7932, 7933, 7934, 7935, 7936, 7937, 7938, 7939]),\n",
       " array([2810, 2811, 2812, 2813, 2814, 2815, 2816, 2817, 2818, 2819]),\n",
       " array([6810, 6811, 6812, 6813, 6814, 6815, 6816, 6817, 6818, 6819]),\n",
       " array([2750, 2751, 2752, 2753, 2754, 2755, 2756, 2757, 2758, 2759]),\n",
       " array([3330, 3331, 3332, 3333, 3334, 3335, 3336, 3337, 3338, 3339]),\n",
       " array([9300, 9301, 9302, 9303, 9304, 9305, 9306, 9307, 9308, 9309]),\n",
       " array([3500, 3501, 3502, 3503, 3504, 3505, 3506, 3507, 3508, 3509]),\n",
       " array([9480, 9481, 9482, 9483, 9484, 9485, 9486, 9487, 9488, 9489]),\n",
       " array([8670, 8671, 8672, 8673, 8674, 8675, 8676, 8677, 8678, 8679]),\n",
       " array([770, 771, 772, 773, 774, 775, 776, 777, 778, 779]),\n",
       " array([1090, 1091, 1092, 1093, 1094, 1095, 1096, 1097, 1098, 1099]),\n",
       " array([6080, 6081, 6082, 6083, 6084, 6085, 6086, 6087, 6088, 6089]),\n",
       " array([5780, 5781, 5782, 5783, 5784, 5785, 5786, 5787, 5788, 5789]),\n",
       " array([5570, 5571, 5572, 5573, 5574, 5575, 5576, 5577, 5578, 5579]),\n",
       " array([2930, 2931, 2932, 2933, 2934, 2935, 2936, 2937, 2938, 2939]),\n",
       " array([5070, 5071, 5072, 5073, 5074, 5075, 5076, 5077, 5078, 5079]),\n",
       " array([1480, 1481, 1482, 1483, 1484, 1485, 1486, 1487, 1488, 1489]),\n",
       " array([9060, 9061, 9062, 9063, 9064, 9065, 9066, 9067, 9068, 9069]),\n",
       " array([360, 361, 362, 363, 364, 365, 366, 367, 368, 369]),\n",
       " array([4360, 4361, 4362, 4363, 4364, 4365, 4366, 4367, 4368, 4369]),\n",
       " array([5840, 5841, 5842, 5843, 5844, 5845, 5846, 5847, 5848, 5849]),\n",
       " array([370, 371, 372, 373, 374, 375, 376, 377, 378, 379]),\n",
       " array([3780, 3781, 3782, 3783, 3784, 3785, 3786, 3787, 3788, 3789]),\n",
       " array([3100, 3101, 3102, 3103, 3104, 3105, 3106, 3107, 3108, 3109]),\n",
       " array([520, 521, 522, 523, 524, 525, 526, 527, 528, 529]),\n",
       " array([8440, 8441, 8442, 8443, 8444, 8445, 8446, 8447, 8448, 8449]),\n",
       " array([2090, 2091, 2092, 2093, 2094, 2095, 2096, 2097, 2098, 2099]),\n",
       " array([7640, 7641, 7642, 7643, 7644, 7645, 7646, 7647, 7648, 7649]),\n",
       " array([2100, 2101, 2102, 2103, 2104, 2105, 2106, 2107, 2108, 2109]),\n",
       " array([9930, 9931, 9932, 9933, 9934, 9935, 9936, 9937, 9938, 9939]),\n",
       " array([190, 191, 192, 193, 194, 195, 196, 197, 198, 199]),\n",
       " array([5280, 5281, 5282, 5283, 5284, 5285, 5286, 5287, 5288, 5289]),\n",
       " array([9760, 9761, 9762, 9763, 9764, 9765, 9766, 9767, 9768, 9769]),\n",
       " array([470, 471, 472, 473, 474, 475, 476, 477, 478, 479]),\n",
       " array([9130, 9131, 9132, 9133, 9134, 9135, 9136, 9137, 9138, 9139]),\n",
       " array([4310, 4311, 4312, 4313, 4314, 4315, 4316, 4317, 4318, 4319]),\n",
       " array([7740, 7741, 7742, 7743, 7744, 7745, 7746, 7747, 7748, 7749]),\n",
       " array([3940, 3941, 3942, 3943, 3944, 3945, 3946, 3947, 3948, 3949]),\n",
       " array([2140, 2141, 2142, 2143, 2144, 2145, 2146, 2147, 2148, 2149]),\n",
       " array([5510, 5511, 5512, 5513, 5514, 5515, 5516, 5517, 5518, 5519]),\n",
       " array([7440, 7441, 7442, 7443, 7444, 7445, 7446, 7447, 7448, 7449]),\n",
       " array([1640, 1641, 1642, 1643, 1644, 1645, 1646, 1647, 1648, 1649]),\n",
       " array([1390, 1391, 1392, 1393, 1394, 1395, 1396, 1397, 1398, 1399]),\n",
       " array([9270, 9271, 9272, 9273, 9274, 9275, 9276, 9277, 9278, 9279]),\n",
       " array([5900, 5901, 5902, 5903, 5904, 5905, 5906, 5907, 5908, 5909]),\n",
       " array([4390, 4391, 4392, 4393, 4394, 4395, 4396, 4397, 4398, 4399]),\n",
       " array([8240, 8241, 8242, 8243, 8244, 8245, 8246, 8247, 8248, 8249]),\n",
       " array([8910, 8911, 8912, 8913, 8914, 8915, 8916, 8917, 8918, 8919]),\n",
       " array([8540, 8541, 8542, 8543, 8544, 8545, 8546, 8547, 8548, 8549]),\n",
       " array([5750, 5751, 5752, 5753, 5754, 5755, 5756, 5757, 5758, 5759]),\n",
       " array([2370, 2371, 2372, 2373, 2374, 2375, 2376, 2377, 2378, 2379]),\n",
       " array([7430, 7431, 7432, 7433, 7434, 7435, 7436, 7437, 7438, 7439]),\n",
       " array([1740, 1741, 1742, 1743, 1744, 1745, 1746, 1747, 1748, 1749]),\n",
       " array([1210, 1211, 1212, 1213, 1214, 1215, 1216, 1217, 1218, 1219]),\n",
       " array([1370, 1371, 1372, 1373, 1374, 1375, 1376, 1377, 1378, 1379]),\n",
       " array([7260, 7261, 7262, 7263, 7264, 7265, 7266, 7267, 7268, 7269]),\n",
       " array([8870, 8871, 8872, 8873, 8874, 8875, 8876, 8877, 8878, 8879]),\n",
       " array([510, 511, 512, 513, 514, 515, 516, 517, 518, 519]),\n",
       " array([8210, 8211, 8212, 8213, 8214, 8215, 8216, 8217, 8218, 8219]),\n",
       " array([6360, 6361, 6362, 6363, 6364, 6365, 6366, 6367, 6368, 6369]),\n",
       " array([6220, 6221, 6222, 6223, 6224, 6225, 6226, 6227, 6228, 6229]),\n",
       " array([9550, 9551, 9552, 9553, 9554, 9555, 9556, 9557, 9558, 9559]),\n",
       " array([4200, 4201, 4202, 4203, 4204, 4205, 4206, 4207, 4208, 4209]),\n",
       " array([900, 901, 902, 903, 904, 905, 906, 907, 908, 909]),\n",
       " array([5160, 5161, 5162, 5163, 5164, 5165, 5166, 5167, 5168, 5169]),\n",
       " array([4910, 4911, 4912, 4913, 4914, 4915, 4916, 4917, 4918, 4919]),\n",
       " array([3190, 3191, 3192, 3193, 3194, 3195, 3196, 3197, 3198, 3199]),\n",
       " array([5970, 5971, 5972, 5973, 5974, 5975, 5976, 5977, 5978, 5979]),\n",
       " array([8190, 8191, 8192, 8193, 8194, 8195, 8196, 8197, 8198, 8199]),\n",
       " array([7850, 7851, 7852, 7853, 7854, 7855, 7856, 7857, 7858, 7859]),\n",
       " array([1040, 1041, 1042, 1043, 1044, 1045, 1046, 1047, 1048, 1049]),\n",
       " array([2960, 2961, 2962, 2963, 2964, 2965, 2966, 2967, 2968, 2969]),\n",
       " array([1450, 1451, 1452, 1453, 1454, 1455, 1456, 1457, 1458, 1459]),\n",
       " array([9580, 9581, 9582, 9583, 9584, 9585, 9586, 9587, 9588, 9589]),\n",
       " array([4280, 4281, 4282, 4283, 4284, 4285, 4286, 4287, 4288, 4289]),\n",
       " array([1290, 1291, 1292, 1293, 1294, 1295, 1296, 1297, 1298, 1299]),\n",
       " array([9860, 9861, 9862, 9863, 9864, 9865, 9866, 9867, 9868, 9869]),\n",
       " array([4040, 4041, 4042, 4043, 4044, 4045, 4046, 4047, 4048, 4049]),\n",
       " array([5060, 5061, 5062, 5063, 5064, 5065, 5066, 5067, 5068, 5069]),\n",
       " array([7470, 7471, 7472, 7473, 7474, 7475, 7476, 7477, 7478, 7479]),\n",
       " array([210, 211, 212, 213, 214, 215, 216, 217, 218, 219]),\n",
       " array([3850, 3851, 3852, 3853, 3854, 3855, 3856, 3857, 3858, 3859]),\n",
       " array([240, 241, 242, 243, 244, 245, 246, 247, 248, 249]),\n",
       " array([9590, 9591, 9592, 9593, 9594, 9595, 9596, 9597, 9598, 9599]),\n",
       " array([2080, 2081, 2082, 2083, 2084, 2085, 2086, 2087, 2088, 2089]),\n",
       " array([3660, 3661, 3662, 3663, 3664, 3665, 3666, 3667, 3668, 3669]),\n",
       " array([1800, 1801, 1802, 1803, 1804, 1805, 1806, 1807, 1808, 1809]),\n",
       " array([4350, 4351, 4352, 4353, 4354, 4355, 4356, 4357, 4358, 4359]),\n",
       " array([6590, 6591, 6592, 6593, 6594, 6595, 6596, 6597, 6598, 6599]),\n",
       " array([2420, 2421, 2422, 2423, 2424, 2425, 2426, 2427, 2428, 2429]),\n",
       " array([6860, 6861, 6862, 6863, 6864, 6865, 6866, 6867, 6868, 6869]),\n",
       " array([5650, 5651, 5652, 5653, 5654, 5655, 5656, 5657, 5658, 5659]),\n",
       " array([530, 531, 532, 533, 534, 535, 536, 537, 538, 539]),\n",
       " array([8570, 8571, 8572, 8573, 8574, 8575, 8576, 8577, 8578, 8579]),\n",
       " array([8200, 8201, 8202, 8203, 8204, 8205, 8206, 8207, 8208, 8209]),\n",
       " array([3800, 3801, 3802, 3803, 3804, 3805, 3806, 3807, 3808, 3809]),\n",
       " array([9210, 9211, 9212, 9213, 9214, 9215, 9216, 9217, 9218, 9219]),\n",
       " array([6100, 6101, 6102, 6103, 6104, 6105, 6106, 6107, 6108, 6109]),\n",
       " array([5710, 5711, 5712, 5713, 5714, 5715, 5716, 5717, 5718, 5719]),\n",
       " array([5920, 5921, 5922, 5923, 5924, 5925, 5926, 5927, 5928, 5929]),\n",
       " array([8340, 8341, 8342, 8343, 8344, 8345, 8346, 8347, 8348, 8349]),\n",
       " array([1140, 1141, 1142, 1143, 1144, 1145, 1146, 1147, 1148, 1149]),\n",
       " array([4850, 4851, 4852, 4853, 4854, 4855, 4856, 4857, 4858, 4859]),\n",
       " array([6760, 6761, 6762, 6763, 6764, 6765, 6766, 6767, 6768, 6769]),\n",
       " array([4110, 4111, 4112, 4113, 4114, 4115, 4116, 4117, 4118, 4119]),\n",
       " array([3610, 3611, 3612, 3613, 3614, 3615, 3616, 3617, 3618, 3619]),\n",
       " array([1430, 1431, 1432, 1433, 1434, 1435, 1436, 1437, 1438, 1439]),\n",
       " array([1590, 1591, 1592, 1593, 1594, 1595, 1596, 1597, 1598, 1599]),\n",
       " array([7080, 7081, 7082, 7083, 7084, 7085, 7086, 7087, 7088, 7089]),\n",
       " array([2760, 2761, 2762, 2763, 2764, 2765, 2766, 2767, 2768, 2769]),\n",
       " array([2310, 2311, 2312, 2313, 2314, 2315, 2316, 2317, 2318, 2319]),\n",
       " array([570, 571, 572, 573, 574, 575, 576, 577, 578, 579]),\n",
       " array([5200, 5201, 5202, 5203, 5204, 5205, 5206, 5207, 5208, 5209]),\n",
       " array([3410, 3411, 3412, 3413, 3414, 3415, 3416, 3417, 3418, 3419]),\n",
       " array([4430, 4431, 4432, 4433, 4434, 4435, 4436, 4437, 4438, 4439]),\n",
       " array([1890, 1891, 1892, 1893, 1894, 1895, 1896, 1897, 1898, 1899]),\n",
       " array([1880, 1881, 1882, 1883, 1884, 1885, 1886, 1887, 1888, 1889]),\n",
       " array([8490, 8491, 8492, 8493, 8494, 8495, 8496, 8497, 8498, 8499]),\n",
       " array([2730, 2731, 2732, 2733, 2734, 2735, 2736, 2737, 2738, 2739]),\n",
       " array([960, 961, 962, 963, 964, 965, 966, 967, 968, 969]),\n",
       " array([9000, 9001, 9002, 9003, 9004, 9005, 9006, 9007, 9008, 9009]),\n",
       " array([6470, 6471, 6472, 6473, 6474, 6475, 6476, 6477, 6478, 6479]),\n",
       " array([7160, 7161, 7162, 7163, 7164, 7165, 7166, 7167, 7168, 7169]),\n",
       " array([7380, 7381, 7382, 7383, 7384, 7385, 7386, 7387, 7388, 7389]),\n",
       " array([1660, 1661, 1662, 1663, 1664, 1665, 1666, 1667, 1668, 1669]),\n",
       " array([8040, 8041, 8042, 8043, 8044, 8045, 8046, 8047, 8048, 8049]),\n",
       " array([6700, 6701, 6702, 6703, 6704, 6705, 6706, 6707, 6708, 6709]),\n",
       " array([6600, 6601, 6602, 6603, 6604, 6605, 6606, 6607, 6608, 6609]),\n",
       " array([7190, 7191, 7192, 7193, 7194, 7195, 7196, 7197, 7198, 7199]),\n",
       " array([8740, 8741, 8742, 8743, 8744, 8745, 8746, 8747, 8748, 8749]),\n",
       " array([3310, 3311, 3312, 3313, 3314, 3315, 3316, 3317, 3318, 3319]),\n",
       " array([6670, 6671, 6672, 6673, 6674, 6675, 6676, 6677, 6678, 6679]),\n",
       " array([2740, 2741, 2742, 2743, 2744, 2745, 2746, 2747, 2748, 2749]),\n",
       " array([8550, 8551, 8552, 8553, 8554, 8555, 8556, 8557, 8558, 8559]),\n",
       " array([2260, 2261, 2262, 2263, 2264, 2265, 2266, 2267, 2268, 2269]),\n",
       " array([6330, 6331, 6332, 6333, 6334, 6335, 6336, 6337, 6338, 6339]),\n",
       " array([1170, 1171, 1172, 1173, 1174, 1175, 1176, 1177, 1178, 1179]),\n",
       " array([690, 691, 692, 693, 694, 695, 696, 697, 698, 699]),\n",
       " array([3740, 3741, 3742, 3743, 3744, 3745, 3746, 3747, 3748, 3749]),\n",
       " array([6580, 6581, 6582, 6583, 6584, 6585, 6586, 6587, 6588, 6589]),\n",
       " array([930, 931, 932, 933, 934, 935, 936, 937, 938, 939]),\n",
       " array([4450, 4451, 4452, 4453, 4454, 4455, 4456, 4457, 4458, 4459]),\n",
       " array([2440, 2441, 2442, 2443, 2444, 2445, 2446, 2447, 2448, 2449]),\n",
       " array([8420, 8421, 8422, 8423, 8424, 8425, 8426, 8427, 8428, 8429]),\n",
       " array([8710, 8711, 8712, 8713, 8714, 8715, 8716, 8717, 8718, 8719]),\n",
       " array([4840, 4841, 4842, 4843, 4844, 4845, 4846, 4847, 4848, 4849]),\n",
       " array([6130, 6131, 6132, 6133, 6134, 6135, 6136, 6137, 6138, 6139]),\n",
       " array([1440, 1441, 1442, 1443, 1444, 1445, 1446, 1447, 1448, 1449]),\n",
       " array([1000, 1001, 1002, 1003, 1004, 1005, 1006, 1007, 1008, 1009]),\n",
       " array([7780, 7781, 7782, 7783, 7784, 7785, 7786, 7787, 7788, 7789]),\n",
       " array([2640, 2641, 2642, 2643, 2644, 2645, 2646, 2647, 2648, 2649]),\n",
       " array([9080, 9081, 9082, 9083, 9084, 9085, 9086, 9087, 9088, 9089]),\n",
       " array([5380, 5381, 5382, 5383, 5384, 5385, 5386, 5387, 5388, 5389]),\n",
       " array([4600, 4601, 4602, 4603, 4604, 4605, 4606, 4607, 4608, 4609]),\n",
       " array([4460, 4461, 4462, 4463, 4464, 4465, 4466, 4467, 4468, 4469]),\n",
       " array([4670, 4671, 4672, 4673, 4674, 4675, 4676, 4677, 4678, 4679]),\n",
       " array([5980, 5981, 5982, 5983, 5984, 5985, 5986, 5987, 5988, 5989]),\n",
       " array([6040, 6041, 6042, 6043, 6044, 6045, 6046, 6047, 6048, 6049]),\n",
       " array([5490, 5491, 5492, 5493, 5494, 5495, 5496, 5497, 5498, 5499]),\n",
       " array([5190, 5191, 5192, 5193, 5194, 5195, 5196, 5197, 5198, 5199]),\n",
       " array([1500, 1501, 1502, 1503, 1504, 1505, 1506, 1507, 1508, 1509]),\n",
       " array([4180, 4181, 4182, 4183, 4184, 4185, 4186, 4187, 4188, 4189]),\n",
       " array([2610, 2611, 2612, 2613, 2614, 2615, 2616, 2617, 2618, 2619]),\n",
       " array([7420, 7421, 7422, 7423, 7424, 7425, 7426, 7427, 7428, 7429]),\n",
       " array([3300, 3301, 3302, 3303, 3304, 3305, 3306, 3307, 3308, 3309])]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[np.where(y_val == i)[0] for i in range(1000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9NSKu-Q6T-bt"
   },
   "outputs": [],
   "source": [
    "# make train pairs\n",
    "pairs_train = make_pairs(x_train, y_train)\n",
    "\n",
    "# make validation pairs\n",
    "pairs_val = make_pairs(x_val, y_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dgbxF_PT7ZU8",
    "outputId": "07f07905-148b-459c-a826-cdf75b2de535"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 3, 2048)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cCsanjqAb7A7",
    "outputId": "bd2e9216-2851-49b2-ff3a-e63aa0004754"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40000, 3, 2048)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 200
    },
    "id": "HqaA0oseVoOG",
    "outputId": "346b65b0-9625-48f8-afe9-0436e738040b"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-7f3e377c0245>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mx_train_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpairs_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# x_train_1.shape is (60000, 28, 28)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mx_train_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpairs_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mx_train_3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpairs_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pairs_train' is not defined"
     ]
    }
   ],
   "source": [
    "x_train_1 = pairs_train[:, 0]  # x_train_1.shape is (60000, 28, 28)\n",
    "x_train_2 = pairs_train[:, 1]\n",
    "x_train_3 = pairs_train[:, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hiVDPJ9cVs79",
    "outputId": "ae6c67db-75e6-434f-ae3a-dfeb806dfe08"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40000, 2048)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j_5sXmagVwge",
    "outputId": "5cca9426-af68-4b01-a872-c3313c06b902"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40000, 2048)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AZERuNkh7h7T",
    "outputId": "8465ae36-72b3-455e-a2a3-d1154ad9ee76"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40000, 2048)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cawWkZMHVzX7"
   },
   "outputs": [],
   "source": [
    "x_val_1 = pairs_val[:, 0]  # x_val_1.shape = (60000, 28, 28)\n",
    "x_val_2 = pairs_val[:, 1]\n",
    "x_val_3 = pairs_val[:, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "luSK0F4DcCqA",
    "outputId": "935a58b7-0c71-4c58-c2bc-589515bd8304"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 2048)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_val_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6kPZFbOcV1yp"
   },
   "outputs": [],
   "source": [
    "input = layers.Input((2048))\n",
    "x = tf.keras.layers.BatchNormalization()(input)\n",
    "x=layers.Dropout(0.2)(x)\n",
    "x = layers.Dense(1000)(x)\n",
    "x = tf.keras.layers.BatchNormalization()(x)\n",
    "x = layers.Dense(1000, activation=\"tanh\")(x)\n",
    "embedding_network = keras.Model(input, x)\n",
    "\n",
    "input_1 = layers.Input((2048))\n",
    "input_2 = layers.Input((2048))\n",
    "input_3 = layers.Input((2048))\n",
    "\n",
    "# As mentioned above, Siamese Network share weights between\n",
    "# tower networks (sister networks). To allow this, we will use\n",
    "# same embedding network for both tower networks.\n",
    "tower_1 = embedding_network(input_1)\n",
    "tower_2 = embedding_network(input_2)\n",
    "tower_3 = embedding_network(input_3)\n",
    "\n",
    "merge_layer = tf.keras.layers.concatenate([tower_1, tower_2,tower_3], axis=1)\n",
    "siamese = keras.Model(inputs=[input_1, input_2,input_3], outputs=merge_layer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ye2OJseXGQfX",
    "outputId": "5ad28cb3-dfb7-4000-fec6-1a113646d6ca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)           [(None, 2048)]       0           []                               \n",
      "                                                                                                  \n",
      " input_3 (InputLayer)           [(None, 2048)]       0           []                               \n",
      "                                                                                                  \n",
      " input_4 (InputLayer)           [(None, 2048)]       0           []                               \n",
      "                                                                                                  \n",
      " model (Functional)             (None, 1000)         3062192     ['input_2[0][0]',                \n",
      "                                                                  'input_3[0][0]',                \n",
      "                                                                  'input_4[0][0]']                \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 3000)         0           ['model[0][0]',                  \n",
      "                                                                  'model[1][0]',                  \n",
      "                                                                  'model[2][0]']                  \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3,062,192\n",
      "Trainable params: 3,056,096\n",
      "Non-trainable params: 6,096\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "siamese.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ccLjvZBy-khV"
   },
   "outputs": [],
   "source": [
    "alpha = 1\n",
    "emb_size=1000\n",
    "def triplet_loss(y_true, y_pred):\n",
    "    anchor, positive, negative = y_pred[:,:emb_size], y_pred[:,emb_size:2*emb_size], y_pred[:,2*emb_size:]\n",
    "    positive_dist = tf.reduce_sum(tf.square(anchor - positive), axis=1)\n",
    "    negative_dist = tf.reduce_sum(tf.square(anchor - negative), axis=1)\n",
    "    return tf.maximum(positive_dist - negative_dist + alpha, 0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tq6YKRvAXz3x"
   },
   "outputs": [],
   "source": [
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(\"./drive/MyDrive/SGD_Siamese_triplet_after_r50.h5\",\n",
    "                             monitor=\"val_loss\",\n",
    "                             mode=\"min\",\n",
    "                             save_best_only = True,\n",
    "                             verbose=1)\n",
    "\n",
    "\n",
    "earlystop = tf.keras.callbacks.EarlyStopping(monitor = 'val_loss', # value being monitored for improvement\n",
    "                          min_delta = 0, #Abs value and is the min change required before we stop\n",
    "                          patience = 50, #Number of epochs we wait before stopping \n",
    "                          verbose = 1,\n",
    "                          restore_best_weights = True) #keeps the best weigths once stopped\n",
    "def scheduler(epoch, lr):\n",
    "  return(0.1*(0.1 ** (epoch // 30)))\n",
    "\n",
    "lr_scheduler=tf.keras.callbacks.LearningRateScheduler(scheduler,verbose=1)\n",
    "# we put our call backs into a callback list\n",
    "callbacks = [earlystop,checkpoint,lr_scheduler]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wred_IIvbhnR",
    "outputId": "902ac1c2-83ba-40e2-f2fd-1b07c84943f2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow-addons\n",
      "  Downloading tensorflow_addons-0.16.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.1 MB 5.0 MB/s \n",
      "\u001b[?25hRequirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow-addons) (2.7.1)\n",
      "Installing collected packages: tensorflow-addons\n",
      "Successfully installed tensorflow-addons-0.16.1\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow-addons\n",
    "import tensorflow_addons as tfa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N2uB73SoXMuV",
    "outputId": "24eca828-5a86-4ee2-8b50-233b44054068"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)           [(None, 2048)]       0           []                               \n",
      "                                                                                                  \n",
      " input_3 (InputLayer)           [(None, 2048)]       0           []                               \n",
      "                                                                                                  \n",
      " input_4 (InputLayer)           [(None, 2048)]       0           []                               \n",
      "                                                                                                  \n",
      " model (Functional)             (None, 1000)         3062192     ['input_2[0][0]',                \n",
      "                                                                  'input_3[0][0]',                \n",
      "                                                                  'input_4[0][0]']                \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 3000)         0           ['model[0][0]',                  \n",
      "                                                                  'model[1][0]',                  \n",
      "                                                                  'model[2][0]']                  \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3,062,192\n",
      "Trainable params: 3,056,096\n",
      "Non-trainable params: 6,096\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "siamese.compile(loss=triplet_loss, optimizer=tfa.optimizers.SGDW(learning_rate=0.1, momentum=0.9, nesterov=False,weight_decay=1e-4))\n",
    "siamese.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8y5TaL_qXZJ5",
    "outputId": "5165728b-1e14-4eaa-fe44-04e6719a05d4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: LearningRateScheduler setting learning rate to 0.1.\n",
      "Epoch 1/200\n",
      "1246/1250 [============================>.] - ETA: 0s - loss: 3.9608\n",
      "Epoch 1: val_loss improved from inf to 1.00127, saving model to ./drive/MyDrive/SGD_Siamese_triplet_after_r50.h5\n",
      "1250/1250 [==============================] - 11s 6ms/step - loss: 3.9513 - val_loss: 1.0013 - lr: 0.1000\n",
      "\n",
      "Epoch 2: LearningRateScheduler setting learning rate to 0.1.\n",
      "Epoch 2/200\n",
      "1246/1250 [============================>.] - ETA: 0s - loss: 1.0000\n",
      "Epoch 2: val_loss improved from 1.00127 to 1.00000, saving model to ./drive/MyDrive/SGD_Siamese_triplet_after_r50.h5\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 1.0000 - val_loss: 1.0000 - lr: 0.1000\n",
      "\n",
      "Epoch 3: LearningRateScheduler setting learning rate to 0.1.\n",
      "Epoch 3/200\n",
      "1247/1250 [============================>.] - ETA: 0s - loss: 1.0000\n",
      "Epoch 3: val_loss did not improve from 1.00000\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 1.0000 - val_loss: 1.0000 - lr: 0.1000\n",
      "\n",
      "Epoch 4: LearningRateScheduler setting learning rate to 0.1.\n",
      "Epoch 4/200\n",
      "1238/1250 [============================>.] - ETA: 0s - loss: 1.0000\n",
      "Epoch 4: val_loss did not improve from 1.00000\n",
      "1250/1250 [==============================] - 6s 4ms/step - loss: 1.0000 - val_loss: 1.0000 - lr: 0.1000\n",
      "\n",
      "Epoch 5: LearningRateScheduler setting learning rate to 0.1.\n",
      "Epoch 5/200\n",
      "1245/1250 [============================>.] - ETA: 0s - loss: 1.0000\n",
      "Epoch 5: val_loss did not improve from 1.00000\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 1.0000 - val_loss: 1.0000 - lr: 0.1000\n",
      "\n",
      "Epoch 6: LearningRateScheduler setting learning rate to 0.1.\n",
      "Epoch 6/200\n",
      "1248/1250 [============================>.] - ETA: 0s - loss: 1.0000\n",
      "Epoch 6: val_loss did not improve from 1.00000\n",
      "1250/1250 [==============================] - 6s 4ms/step - loss: 1.0000 - val_loss: 1.0000 - lr: 0.1000\n",
      "\n",
      "Epoch 7: LearningRateScheduler setting learning rate to 0.1.\n",
      "Epoch 7/200\n",
      "1242/1250 [============================>.] - ETA: 0s - loss: 1.0000\n",
      "Epoch 7: val_loss did not improve from 1.00000\n",
      "1250/1250 [==============================] - 6s 4ms/step - loss: 1.0000 - val_loss: 1.0000 - lr: 0.1000\n",
      "\n",
      "Epoch 8: LearningRateScheduler setting learning rate to 0.1.\n",
      "Epoch 8/200\n",
      "1242/1250 [============================>.] - ETA: 0s - loss: 1.0000\n",
      "Epoch 8: val_loss did not improve from 1.00000\n",
      "1250/1250 [==============================] - 6s 4ms/step - loss: 1.0000 - val_loss: 1.0000 - lr: 0.1000\n",
      "\n",
      "Epoch 9: LearningRateScheduler setting learning rate to 0.1.\n",
      "Epoch 9/200\n",
      "1247/1250 [============================>.] - ETA: 0s - loss: 1.0000\n",
      "Epoch 9: val_loss improved from 1.00000 to 1.00000, saving model to ./drive/MyDrive/SGD_Siamese_triplet_after_r50.h5\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 1.0000 - val_loss: 1.0000 - lr: 0.1000\n",
      "\n",
      "Epoch 10: LearningRateScheduler setting learning rate to 0.1.\n",
      "Epoch 10/200\n",
      "1250/1250 [==============================] - ETA: 0s - loss: 1.0000\n",
      "Epoch 10: val_loss improved from 1.00000 to 1.00000, saving model to ./drive/MyDrive/SGD_Siamese_triplet_after_r50.h5\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 1.0000 - val_loss: 1.0000 - lr: 0.1000\n",
      "\n",
      "Epoch 11: LearningRateScheduler setting learning rate to 0.1.\n",
      "Epoch 11/200\n",
      "1247/1250 [============================>.] - ETA: 0s - loss: 1.0001\n",
      "Epoch 11: val_loss improved from 1.00000 to 1.00000, saving model to ./drive/MyDrive/SGD_Siamese_triplet_after_r50.h5\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 1.0001 - val_loss: 1.0000 - lr: 0.1000\n",
      "\n",
      "Epoch 12: LearningRateScheduler setting learning rate to 0.1.\n",
      "Epoch 12/200\n",
      "1250/1250 [==============================] - ETA: 0s - loss: 1.0000\n",
      "Epoch 12: val_loss did not improve from 1.00000\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 1.0000 - val_loss: 1.0000 - lr: 0.1000\n",
      "\n",
      "Epoch 13: LearningRateScheduler setting learning rate to 0.1.\n",
      "Epoch 13/200\n",
      "1246/1250 [============================>.] - ETA: 0s - loss: 0.9995\n",
      "Epoch 13: val_loss improved from 1.00000 to 0.99984, saving model to ./drive/MyDrive/SGD_Siamese_triplet_after_r50.h5\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 0.9995 - val_loss: 0.9998 - lr: 0.1000\n",
      "\n",
      "Epoch 14: LearningRateScheduler setting learning rate to 0.1.\n",
      "Epoch 14/200\n",
      "1245/1250 [============================>.] - ETA: 0s - loss: 0.9925\n",
      "Epoch 14: val_loss did not improve from 0.99984\n",
      "1250/1250 [==============================] - 6s 4ms/step - loss: 0.9926 - val_loss: 0.9999 - lr: 0.1000\n",
      "\n",
      "Epoch 15: LearningRateScheduler setting learning rate to 0.1.\n",
      "Epoch 15/200\n",
      "1250/1250 [==============================] - ETA: 0s - loss: 0.7728\n",
      "Epoch 15: val_loss improved from 0.99984 to 0.53021, saving model to ./drive/MyDrive/SGD_Siamese_triplet_after_r50.h5\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 0.7728 - val_loss: 0.5302 - lr: 0.1000\n",
      "\n",
      "Epoch 16: LearningRateScheduler setting learning rate to 0.1.\n",
      "Epoch 16/200\n",
      "1242/1250 [============================>.] - ETA: 0s - loss: 0.7322\n",
      "Epoch 16: val_loss did not improve from 0.53021\n",
      "1250/1250 [==============================] - 6s 4ms/step - loss: 0.7331 - val_loss: 0.8890 - lr: 0.1000\n",
      "\n",
      "Epoch 17: LearningRateScheduler setting learning rate to 0.1.\n",
      "Epoch 17/200\n",
      "1249/1250 [============================>.] - ETA: 0s - loss: 0.9074\n",
      "Epoch 17: val_loss did not improve from 0.53021\n",
      "1250/1250 [==============================] - 6s 4ms/step - loss: 0.9074 - val_loss: 0.8997 - lr: 0.1000\n",
      "\n",
      "Epoch 18: LearningRateScheduler setting learning rate to 0.1.\n",
      "Epoch 18/200\n",
      "1246/1250 [============================>.] - ETA: 0s - loss: 0.7243\n",
      "Epoch 18: val_loss improved from 0.53021 to 0.52280, saving model to ./drive/MyDrive/SGD_Siamese_triplet_after_r50.h5\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 0.7241 - val_loss: 0.5228 - lr: 0.1000\n",
      "\n",
      "Epoch 19: LearningRateScheduler setting learning rate to 0.1.\n",
      "Epoch 19/200\n",
      "1244/1250 [============================>.] - ETA: 0s - loss: 0.7324\n",
      "Epoch 19: val_loss did not improve from 0.52280\n",
      "1250/1250 [==============================] - 6s 4ms/step - loss: 0.7337 - val_loss: 1.0000 - lr: 0.1000\n",
      "\n",
      "Epoch 20: LearningRateScheduler setting learning rate to 0.1.\n",
      "Epoch 20/200\n",
      "1245/1250 [============================>.] - ETA: 0s - loss: 0.9814\n",
      "Epoch 20: val_loss did not improve from 0.52280\n",
      "1250/1250 [==============================] - 6s 4ms/step - loss: 0.9803 - val_loss: 0.6548 - lr: 0.1000\n",
      "\n",
      "Epoch 21: LearningRateScheduler setting learning rate to 0.1.\n",
      "Epoch 21/200\n",
      "1246/1250 [============================>.] - ETA: 0s - loss: 0.5856\n",
      "Epoch 21: val_loss did not improve from 0.52280\n",
      "1250/1250 [==============================] - 6s 4ms/step - loss: 0.5869 - val_loss: 1.0000 - lr: 0.1000\n",
      "\n",
      "Epoch 22: LearningRateScheduler setting learning rate to 0.1.\n",
      "Epoch 22/200\n",
      "1238/1250 [============================>.] - ETA: 0s - loss: 0.9993\n",
      "Epoch 22: val_loss did not improve from 0.52280\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 0.9994 - val_loss: 1.0000 - lr: 0.1000\n",
      "\n",
      "Epoch 23: LearningRateScheduler setting learning rate to 0.1.\n",
      "Epoch 23/200\n",
      "1245/1250 [============================>.] - ETA: 0s - loss: 0.9814\n",
      "Epoch 23: val_loss did not improve from 0.52280\n",
      "1250/1250 [==============================] - 6s 4ms/step - loss: 0.9815 - val_loss: 0.7807 - lr: 0.1000\n",
      "\n",
      "Epoch 24: LearningRateScheduler setting learning rate to 0.1.\n",
      "Epoch 24/200\n",
      "1247/1250 [============================>.] - ETA: 0s - loss: 0.6724\n",
      "Epoch 24: val_loss improved from 0.52280 to 0.35876, saving model to ./drive/MyDrive/SGD_Siamese_triplet_after_r50.h5\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 0.6717 - val_loss: 0.3588 - lr: 0.1000\n",
      "\n",
      "Epoch 25: LearningRateScheduler setting learning rate to 0.1.\n",
      "Epoch 25/200\n",
      "1246/1250 [============================>.] - ETA: 0s - loss: 0.4536\n",
      "Epoch 25: val_loss improved from 0.35876 to 0.35085, saving model to ./drive/MyDrive/SGD_Siamese_triplet_after_r50.h5\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 0.4534 - val_loss: 0.3508 - lr: 0.1000\n",
      "\n",
      "Epoch 26: LearningRateScheduler setting learning rate to 0.1.\n",
      "Epoch 26/200\n",
      "1240/1250 [============================>.] - ETA: 0s - loss: 0.4447\n",
      "Epoch 26: val_loss did not improve from 0.35085\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 0.4449 - val_loss: 0.3519 - lr: 0.1000\n",
      "\n",
      "Epoch 27: LearningRateScheduler setting learning rate to 0.1.\n",
      "Epoch 27/200\n",
      "1238/1250 [============================>.] - ETA: 0s - loss: 0.4789\n",
      "Epoch 27: val_loss did not improve from 0.35085\n",
      "1250/1250 [==============================] - 6s 4ms/step - loss: 0.4794 - val_loss: 0.4017 - lr: 0.1000\n",
      "\n",
      "Epoch 28: LearningRateScheduler setting learning rate to 0.1.\n",
      "Epoch 28/200\n",
      "1249/1250 [============================>.] - ETA: 0s - loss: 0.4965\n",
      "Epoch 28: val_loss did not improve from 0.35085\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 0.4966 - val_loss: 0.3884 - lr: 0.1000\n",
      "\n",
      "Epoch 29: LearningRateScheduler setting learning rate to 0.1.\n",
      "Epoch 29/200\n",
      "1238/1250 [============================>.] - ETA: 0s - loss: 0.5989\n",
      "Epoch 29: val_loss did not improve from 0.35085\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 0.5978 - val_loss: 0.3690 - lr: 0.1000\n",
      "\n",
      "Epoch 30: LearningRateScheduler setting learning rate to 0.1.\n",
      "Epoch 30/200\n",
      "1247/1250 [============================>.] - ETA: 0s - loss: 0.5630\n",
      "Epoch 30: val_loss did not improve from 0.35085\n",
      "1250/1250 [==============================] - 6s 4ms/step - loss: 0.5632 - val_loss: 0.6172 - lr: 0.1000\n",
      "\n",
      "Epoch 31: LearningRateScheduler setting learning rate to 0.010000000000000002.\n",
      "Epoch 31/200\n",
      "1247/1250 [============================>.] - ETA: 0s - loss: 0.3529\n",
      "Epoch 31: val_loss improved from 0.35085 to 0.23739, saving model to ./drive/MyDrive/SGD_Siamese_triplet_after_r50.h5\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 0.3525 - val_loss: 0.2374 - lr: 0.0100\n",
      "\n",
      "Epoch 32: LearningRateScheduler setting learning rate to 0.010000000000000002.\n",
      "Epoch 32/200\n",
      "1250/1250 [==============================] - ETA: 0s - loss: 0.2851\n",
      "Epoch 32: val_loss improved from 0.23739 to 0.22655, saving model to ./drive/MyDrive/SGD_Siamese_triplet_after_r50.h5\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 0.2851 - val_loss: 0.2265 - lr: 0.0100\n",
      "\n",
      "Epoch 33: LearningRateScheduler setting learning rate to 0.010000000000000002.\n",
      "Epoch 33/200\n",
      "1248/1250 [============================>.] - ETA: 0s - loss: 0.2608\n",
      "Epoch 33: val_loss improved from 0.22655 to 0.21363, saving model to ./drive/MyDrive/SGD_Siamese_triplet_after_r50.h5\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 0.2608 - val_loss: 0.2136 - lr: 0.0100\n",
      "\n",
      "Epoch 34: LearningRateScheduler setting learning rate to 0.010000000000000002.\n",
      "Epoch 34/200\n",
      "1247/1250 [============================>.] - ETA: 0s - loss: 0.2465\n",
      "Epoch 34: val_loss improved from 0.21363 to 0.20581, saving model to ./drive/MyDrive/SGD_Siamese_triplet_after_r50.h5\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 0.2465 - val_loss: 0.2058 - lr: 0.0100\n",
      "\n",
      "Epoch 35: LearningRateScheduler setting learning rate to 0.010000000000000002.\n",
      "Epoch 35/200\n",
      "1245/1250 [============================>.] - ETA: 0s - loss: 0.2374\n",
      "Epoch 35: val_loss did not improve from 0.20581\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 0.2375 - val_loss: 0.2058 - lr: 0.0100\n",
      "\n",
      "Epoch 36: LearningRateScheduler setting learning rate to 0.010000000000000002.\n",
      "Epoch 36/200\n",
      "1240/1250 [============================>.] - ETA: 0s - loss: 0.2286\n",
      "Epoch 36: val_loss did not improve from 0.20581\n",
      "1250/1250 [==============================] - 6s 4ms/step - loss: 0.2285 - val_loss: 0.2156 - lr: 0.0100\n",
      "\n",
      "Epoch 37: LearningRateScheduler setting learning rate to 0.010000000000000002.\n",
      "Epoch 37/200\n",
      "1247/1250 [============================>.] - ETA: 0s - loss: 0.2226\n",
      "Epoch 37: val_loss improved from 0.20581 to 0.20102, saving model to ./drive/MyDrive/SGD_Siamese_triplet_after_r50.h5\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 0.2225 - val_loss: 0.2010 - lr: 0.0100\n",
      "\n",
      "Epoch 38: LearningRateScheduler setting learning rate to 0.010000000000000002.\n",
      "Epoch 38/200\n",
      "1240/1250 [============================>.] - ETA: 0s - loss: 0.2216\n",
      "Epoch 38: val_loss improved from 0.20102 to 0.20101, saving model to ./drive/MyDrive/SGD_Siamese_triplet_after_r50.h5\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 0.2219 - val_loss: 0.2010 - lr: 0.0100\n",
      "\n",
      "Epoch 39: LearningRateScheduler setting learning rate to 0.010000000000000002.\n",
      "Epoch 39/200\n",
      "1248/1250 [============================>.] - ETA: 0s - loss: 0.2170\n",
      "Epoch 39: val_loss improved from 0.20101 to 0.20075, saving model to ./drive/MyDrive/SGD_Siamese_triplet_after_r50.h5\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 0.2169 - val_loss: 0.2007 - lr: 0.0100\n",
      "\n",
      "Epoch 40: LearningRateScheduler setting learning rate to 0.010000000000000002.\n",
      "Epoch 40/200\n",
      "1247/1250 [============================>.] - ETA: 0s - loss: 0.2152\n",
      "Epoch 40: val_loss did not improve from 0.20075\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 0.2152 - val_loss: 0.2037 - lr: 0.0100\n",
      "\n",
      "Epoch 41: LearningRateScheduler setting learning rate to 0.010000000000000002.\n",
      "Epoch 41/200\n",
      "1250/1250 [==============================] - ETA: 0s - loss: 0.2114\n",
      "Epoch 41: val_loss did not improve from 0.20075\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 0.2114 - val_loss: 0.2040 - lr: 0.0100\n",
      "\n",
      "Epoch 42: LearningRateScheduler setting learning rate to 0.010000000000000002.\n",
      "Epoch 42/200\n",
      "1246/1250 [============================>.] - ETA: 0s - loss: 0.2164\n",
      "Epoch 42: val_loss improved from 0.20075 to 0.20023, saving model to ./drive/MyDrive/SGD_Siamese_triplet_after_r50.h5\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 0.2166 - val_loss: 0.2002 - lr: 0.0100\n",
      "\n",
      "Epoch 43: LearningRateScheduler setting learning rate to 0.010000000000000002.\n",
      "Epoch 43/200\n",
      "1250/1250 [==============================] - ETA: 0s - loss: 0.2123\n",
      "Epoch 43: val_loss did not improve from 0.20023\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 0.2123 - val_loss: 0.2075 - lr: 0.0100\n",
      "\n",
      "Epoch 44: LearningRateScheduler setting learning rate to 0.010000000000000002.\n",
      "Epoch 44/200\n",
      "1240/1250 [============================>.] - ETA: 0s - loss: 0.2141\n",
      "Epoch 44: val_loss did not improve from 0.20023\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 0.2138 - val_loss: 0.2102 - lr: 0.0100\n",
      "\n",
      "Epoch 45: LearningRateScheduler setting learning rate to 0.010000000000000002.\n",
      "Epoch 45/200\n",
      "1243/1250 [============================>.] - ETA: 0s - loss: 0.2123\n",
      "Epoch 45: val_loss improved from 0.20023 to 0.19506, saving model to ./drive/MyDrive/SGD_Siamese_triplet_after_r50.h5\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 0.2122 - val_loss: 0.1951 - lr: 0.0100\n",
      "\n",
      "Epoch 46: LearningRateScheduler setting learning rate to 0.010000000000000002.\n",
      "Epoch 46/200\n",
      "1249/1250 [============================>.] - ETA: 0s - loss: 0.2073\n",
      "Epoch 46: val_loss improved from 0.19506 to 0.19428, saving model to ./drive/MyDrive/SGD_Siamese_triplet_after_r50.h5\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 0.2073 - val_loss: 0.1943 - lr: 0.0100\n",
      "\n",
      "Epoch 47: LearningRateScheduler setting learning rate to 0.010000000000000002.\n",
      "Epoch 47/200\n",
      "1248/1250 [============================>.] - ETA: 0s - loss: 0.2044\n",
      "Epoch 47: val_loss did not improve from 0.19428\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 0.2043 - val_loss: 0.2072 - lr: 0.0100\n",
      "\n",
      "Epoch 48: LearningRateScheduler setting learning rate to 0.010000000000000002.\n",
      "Epoch 48/200\n",
      "1244/1250 [============================>.] - ETA: 0s - loss: 0.2086\n",
      "Epoch 48: val_loss did not improve from 0.19428\n",
      "1250/1250 [==============================] - 6s 4ms/step - loss: 0.2085 - val_loss: 0.1990 - lr: 0.0100\n",
      "\n",
      "Epoch 49: LearningRateScheduler setting learning rate to 0.010000000000000002.\n",
      "Epoch 49/200\n",
      "1242/1250 [============================>.] - ETA: 0s - loss: 0.2039\n",
      "Epoch 49: val_loss did not improve from 0.19428\n",
      "1250/1250 [==============================] - 6s 4ms/step - loss: 0.2036 - val_loss: 0.1991 - lr: 0.0100\n",
      "\n",
      "Epoch 50: LearningRateScheduler setting learning rate to 0.010000000000000002.\n",
      "Epoch 50/200\n",
      "1244/1250 [============================>.] - ETA: 0s - loss: 0.2032\n",
      "Epoch 50: val_loss did not improve from 0.19428\n",
      "1250/1250 [==============================] - 6s 4ms/step - loss: 0.2029 - val_loss: 0.2042 - lr: 0.0100\n",
      "\n",
      "Epoch 51: LearningRateScheduler setting learning rate to 0.010000000000000002.\n",
      "Epoch 51/200\n",
      "1239/1250 [============================>.] - ETA: 0s - loss: 0.2066\n",
      "Epoch 51: val_loss did not improve from 0.19428\n",
      "1250/1250 [==============================] - 6s 4ms/step - loss: 0.2068 - val_loss: 0.2031 - lr: 0.0100\n",
      "\n",
      "Epoch 52: LearningRateScheduler setting learning rate to 0.010000000000000002.\n",
      "Epoch 52/200\n",
      "1238/1250 [============================>.] - ETA: 0s - loss: 0.2025\n",
      "Epoch 52: val_loss did not improve from 0.19428\n",
      "1250/1250 [==============================] - 6s 4ms/step - loss: 0.2026 - val_loss: 0.2084 - lr: 0.0100\n",
      "\n",
      "Epoch 53: LearningRateScheduler setting learning rate to 0.010000000000000002.\n",
      "Epoch 53/200\n",
      "1238/1250 [============================>.] - ETA: 0s - loss: 0.2034\n",
      "Epoch 53: val_loss did not improve from 0.19428\n",
      "1250/1250 [==============================] - 6s 4ms/step - loss: 0.2026 - val_loss: 0.2028 - lr: 0.0100\n",
      "\n",
      "Epoch 54: LearningRateScheduler setting learning rate to 0.010000000000000002.\n",
      "Epoch 54/200\n",
      "1247/1250 [============================>.] - ETA: 0s - loss: 0.1991\n",
      "Epoch 54: val_loss did not improve from 0.19428\n",
      "1250/1250 [==============================] - 6s 4ms/step - loss: 0.1994 - val_loss: 0.2035 - lr: 0.0100\n",
      "\n",
      "Epoch 55: LearningRateScheduler setting learning rate to 0.010000000000000002.\n",
      "Epoch 55/200\n",
      "1238/1250 [============================>.] - ETA: 0s - loss: 0.2004\n",
      "Epoch 55: val_loss did not improve from 0.19428\n",
      "1250/1250 [==============================] - 6s 4ms/step - loss: 0.2004 - val_loss: 0.2012 - lr: 0.0100\n",
      "\n",
      "Epoch 56: LearningRateScheduler setting learning rate to 0.010000000000000002.\n",
      "Epoch 56/200\n",
      "1246/1250 [============================>.] - ETA: 0s - loss: 0.1995\n",
      "Epoch 56: val_loss did not improve from 0.19428\n",
      "1250/1250 [==============================] - 6s 4ms/step - loss: 0.1995 - val_loss: 0.2064 - lr: 0.0100\n",
      "\n",
      "Epoch 57: LearningRateScheduler setting learning rate to 0.010000000000000002.\n",
      "Epoch 57/200\n",
      "1244/1250 [============================>.] - ETA: 0s - loss: 0.1984\n",
      "Epoch 57: val_loss did not improve from 0.19428\n",
      "1250/1250 [==============================] - 6s 4ms/step - loss: 0.1985 - val_loss: 0.2020 - lr: 0.0100\n",
      "\n",
      "Epoch 58: LearningRateScheduler setting learning rate to 0.010000000000000002.\n",
      "Epoch 58/200\n",
      "1242/1250 [============================>.] - ETA: 0s - loss: 0.2005\n",
      "Epoch 58: val_loss did not improve from 0.19428\n",
      "1250/1250 [==============================] - 6s 4ms/step - loss: 0.2010 - val_loss: 0.1949 - lr: 0.0100\n",
      "\n",
      "Epoch 59: LearningRateScheduler setting learning rate to 0.010000000000000002.\n",
      "Epoch 59/200\n",
      "1240/1250 [============================>.] - ETA: 0s - loss: 0.1983\n",
      "Epoch 59: val_loss did not improve from 0.19428\n",
      "1250/1250 [==============================] - 6s 4ms/step - loss: 0.1983 - val_loss: 0.2049 - lr: 0.0100\n",
      "\n",
      "Epoch 60: LearningRateScheduler setting learning rate to 0.010000000000000002.\n",
      "Epoch 60/200\n",
      "1248/1250 [============================>.] - ETA: 0s - loss: 0.1953\n",
      "Epoch 60: val_loss did not improve from 0.19428\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 0.1953 - val_loss: 0.2115 - lr: 0.0100\n",
      "\n",
      "Epoch 61: LearningRateScheduler setting learning rate to 0.0010000000000000002.\n",
      "Epoch 61/200\n",
      "1250/1250 [==============================] - ETA: 0s - loss: 0.1698\n",
      "Epoch 61: val_loss improved from 0.19428 to 0.18456, saving model to ./drive/MyDrive/SGD_Siamese_triplet_after_r50.h5\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 0.1698 - val_loss: 0.1846 - lr: 0.0010\n",
      "\n",
      "Epoch 62: LearningRateScheduler setting learning rate to 0.0010000000000000002.\n",
      "Epoch 62/200\n",
      "1243/1250 [============================>.] - ETA: 0s - loss: 0.1600\n",
      "Epoch 62: val_loss improved from 0.18456 to 0.18241, saving model to ./drive/MyDrive/SGD_Siamese_triplet_after_r50.h5\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 0.1600 - val_loss: 0.1824 - lr: 0.0010\n",
      "\n",
      "Epoch 63: LearningRateScheduler setting learning rate to 0.0010000000000000002.\n",
      "Epoch 63/200\n",
      "1245/1250 [============================>.] - ETA: 0s - loss: 0.1584\n",
      "Epoch 63: val_loss improved from 0.18241 to 0.18222, saving model to ./drive/MyDrive/SGD_Siamese_triplet_after_r50.h5\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 0.1586 - val_loss: 0.1822 - lr: 0.0010\n",
      "\n",
      "Epoch 64: LearningRateScheduler setting learning rate to 0.0010000000000000002.\n",
      "Epoch 64/200\n",
      "1247/1250 [============================>.] - ETA: 0s - loss: 0.1583\n",
      "Epoch 64: val_loss did not improve from 0.18222\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 0.1585 - val_loss: 0.1830 - lr: 0.0010\n",
      "\n",
      "Epoch 65: LearningRateScheduler setting learning rate to 0.0010000000000000002.\n",
      "Epoch 65/200\n",
      "1238/1250 [============================>.] - ETA: 0s - loss: 0.1602\n",
      "Epoch 65: val_loss improved from 0.18222 to 0.18102, saving model to ./drive/MyDrive/SGD_Siamese_triplet_after_r50.h5\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 0.1601 - val_loss: 0.1810 - lr: 0.0010\n",
      "\n",
      "Epoch 66: LearningRateScheduler setting learning rate to 0.0010000000000000002.\n",
      "Epoch 66/200\n",
      "1247/1250 [============================>.] - ETA: 0s - loss: 0.1626\n",
      "Epoch 66: val_loss did not improve from 0.18102\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 0.1625 - val_loss: 0.1835 - lr: 0.0010\n",
      "\n",
      "Epoch 67: LearningRateScheduler setting learning rate to 0.0010000000000000002.\n",
      "Epoch 67/200\n",
      "1250/1250 [==============================] - ETA: 0s - loss: 0.1651\n",
      "Epoch 67: val_loss did not improve from 0.18102\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 0.1651 - val_loss: 0.1858 - lr: 0.0010\n",
      "\n",
      "Epoch 68: LearningRateScheduler setting learning rate to 0.0010000000000000002.\n",
      "Epoch 68/200\n",
      "1244/1250 [============================>.] - ETA: 0s - loss: 0.1678\n",
      "Epoch 68: val_loss did not improve from 0.18102\n",
      "1250/1250 [==============================] - 6s 4ms/step - loss: 0.1678 - val_loss: 0.1846 - lr: 0.0010\n",
      "\n",
      "Epoch 69: LearningRateScheduler setting learning rate to 0.0010000000000000002.\n",
      "Epoch 69/200\n",
      "1248/1250 [============================>.] - ETA: 0s - loss: 0.1703\n",
      "Epoch 69: val_loss did not improve from 0.18102\n",
      "1250/1250 [==============================] - 6s 4ms/step - loss: 0.1707 - val_loss: 0.1848 - lr: 0.0010\n",
      "\n",
      "Epoch 70: LearningRateScheduler setting learning rate to 0.0010000000000000002.\n",
      "Epoch 70/200\n",
      "1240/1250 [============================>.] - ETA: 0s - loss: 0.1711\n",
      "Epoch 70: val_loss did not improve from 0.18102\n",
      "1250/1250 [==============================] - 6s 4ms/step - loss: 0.1712 - val_loss: 0.1877 - lr: 0.0010\n",
      "\n",
      "Epoch 71: LearningRateScheduler setting learning rate to 0.0010000000000000002.\n",
      "Epoch 71/200\n",
      "1250/1250 [==============================] - ETA: 0s - loss: 0.1727\n",
      "Epoch 71: val_loss did not improve from 0.18102\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 0.1727 - val_loss: 0.1871 - lr: 0.0010\n",
      "\n",
      "Epoch 72: LearningRateScheduler setting learning rate to 0.0010000000000000002.\n",
      "Epoch 72/200\n",
      "1247/1250 [============================>.] - ETA: 0s - loss: 0.1738\n",
      "Epoch 72: val_loss did not improve from 0.18102\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 0.1735 - val_loss: 0.1857 - lr: 0.0010\n",
      "\n",
      "Epoch 73: LearningRateScheduler setting learning rate to 0.0010000000000000002.\n",
      "Epoch 73/200\n",
      "1243/1250 [============================>.] - ETA: 0s - loss: 0.1746\n",
      "Epoch 73: val_loss did not improve from 0.18102\n",
      "1250/1250 [==============================] - 6s 4ms/step - loss: 0.1744 - val_loss: 0.1881 - lr: 0.0010\n",
      "\n",
      "Epoch 74: LearningRateScheduler setting learning rate to 0.0010000000000000002.\n",
      "Epoch 74/200\n",
      "1250/1250 [==============================] - ETA: 0s - loss: 0.1744\n",
      "Epoch 74: val_loss did not improve from 0.18102\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 0.1744 - val_loss: 0.1873 - lr: 0.0010\n",
      "\n",
      "Epoch 75: LearningRateScheduler setting learning rate to 0.0010000000000000002.\n",
      "Epoch 75/200\n",
      "1243/1250 [============================>.] - ETA: 0s - loss: 0.1757\n",
      "Epoch 75: val_loss did not improve from 0.18102\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 0.1759 - val_loss: 0.1903 - lr: 0.0010\n",
      "\n",
      "Epoch 76: LearningRateScheduler setting learning rate to 0.0010000000000000002.\n",
      "Epoch 76/200\n",
      "1240/1250 [============================>.] - ETA: 0s - loss: 0.1778\n",
      "Epoch 76: val_loss did not improve from 0.18102\n",
      "1250/1250 [==============================] - 6s 4ms/step - loss: 0.1779 - val_loss: 0.1919 - lr: 0.0010\n",
      "\n",
      "Epoch 77: LearningRateScheduler setting learning rate to 0.0010000000000000002.\n",
      "Epoch 77/200\n",
      "1250/1250 [==============================] - ETA: 0s - loss: 0.1764\n",
      "Epoch 77: val_loss did not improve from 0.18102\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 0.1764 - val_loss: 0.1866 - lr: 0.0010\n",
      "\n",
      "Epoch 78: LearningRateScheduler setting learning rate to 0.0010000000000000002.\n",
      "Epoch 78/200\n",
      "1246/1250 [============================>.] - ETA: 0s - loss: 0.1784\n",
      "Epoch 78: val_loss did not improve from 0.18102\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 0.1785 - val_loss: 0.1892 - lr: 0.0010\n",
      "\n",
      "Epoch 79: LearningRateScheduler setting learning rate to 0.0010000000000000002.\n",
      "Epoch 79/200\n",
      "1244/1250 [============================>.] - ETA: 0s - loss: 0.1800\n",
      "Epoch 79: val_loss did not improve from 0.18102\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 0.1800 - val_loss: 0.1897 - lr: 0.0010\n",
      "\n",
      "Epoch 80: LearningRateScheduler setting learning rate to 0.0010000000000000002.\n",
      "Epoch 80/200\n",
      "1245/1250 [============================>.] - ETA: 0s - loss: 0.1806\n",
      "Epoch 80: val_loss did not improve from 0.18102\n",
      "1250/1250 [==============================] - 6s 4ms/step - loss: 0.1806 - val_loss: 0.1867 - lr: 0.0010\n",
      "\n",
      "Epoch 81: LearningRateScheduler setting learning rate to 0.0010000000000000002.\n",
      "Epoch 81/200\n",
      "1239/1250 [============================>.] - ETA: 0s - loss: 0.1801\n",
      "Epoch 81: val_loss did not improve from 0.18102\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 0.1800 - val_loss: 0.1857 - lr: 0.0010\n",
      "\n",
      "Epoch 82: LearningRateScheduler setting learning rate to 0.0010000000000000002.\n",
      "Epoch 82/200\n",
      "1242/1250 [============================>.] - ETA: 0s - loss: 0.1769\n",
      "Epoch 82: val_loss did not improve from 0.18102\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 0.1769 - val_loss: 0.1865 - lr: 0.0010\n",
      "\n",
      "Epoch 83: LearningRateScheduler setting learning rate to 0.0010000000000000002.\n",
      "Epoch 83/200\n",
      "1248/1250 [============================>.] - ETA: 0s - loss: 0.1785\n",
      "Epoch 83: val_loss did not improve from 0.18102\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 0.1784 - val_loss: 0.1868 - lr: 0.0010\n",
      "\n",
      "Epoch 84: LearningRateScheduler setting learning rate to 0.0010000000000000002.\n",
      "Epoch 84/200\n",
      "1241/1250 [============================>.] - ETA: 0s - loss: 0.1811\n",
      "Epoch 84: val_loss did not improve from 0.18102\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 0.1814 - val_loss: 0.1889 - lr: 0.0010\n",
      "\n",
      "Epoch 85: LearningRateScheduler setting learning rate to 0.0010000000000000002.\n",
      "Epoch 85/200\n",
      "1238/1250 [============================>.] - ETA: 0s - loss: 0.1782\n",
      "Epoch 85: val_loss did not improve from 0.18102\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 0.1789 - val_loss: 0.1861 - lr: 0.0010\n",
      "\n",
      "Epoch 86: LearningRateScheduler setting learning rate to 0.0010000000000000002.\n",
      "Epoch 86/200\n",
      "1249/1250 [============================>.] - ETA: 0s - loss: 0.1781\n",
      "Epoch 86: val_loss did not improve from 0.18102\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 0.1780 - val_loss: 0.1889 - lr: 0.0010\n",
      "\n",
      "Epoch 87: LearningRateScheduler setting learning rate to 0.0010000000000000002.\n",
      "Epoch 87/200\n",
      "1241/1250 [============================>.] - ETA: 0s - loss: 0.1794\n",
      "Epoch 87: val_loss did not improve from 0.18102\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 0.1794 - val_loss: 0.1876 - lr: 0.0010\n",
      "\n",
      "Epoch 88: LearningRateScheduler setting learning rate to 0.0010000000000000002.\n",
      "Epoch 88/200\n",
      "1239/1250 [============================>.] - ETA: 0s - loss: 0.1796\n",
      "Epoch 88: val_loss did not improve from 0.18102\n",
      "1250/1250 [==============================] - 6s 4ms/step - loss: 0.1796 - val_loss: 0.1897 - lr: 0.0010\n",
      "\n",
      "Epoch 89: LearningRateScheduler setting learning rate to 0.0010000000000000002.\n",
      "Epoch 89/200\n",
      "1241/1250 [============================>.] - ETA: 0s - loss: 0.1801\n",
      "Epoch 89: val_loss did not improve from 0.18102\n",
      "1250/1250 [==============================] - 6s 4ms/step - loss: 0.1803 - val_loss: 0.1847 - lr: 0.0010\n",
      "\n",
      "Epoch 90: LearningRateScheduler setting learning rate to 0.0010000000000000002.\n",
      "Epoch 90/200\n",
      "1249/1250 [============================>.] - ETA: 0s - loss: 0.1774\n",
      "Epoch 90: val_loss did not improve from 0.18102\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 0.1775 - val_loss: 0.1912 - lr: 0.0010\n",
      "\n",
      "Epoch 91: LearningRateScheduler setting learning rate to 0.00010000000000000003.\n",
      "Epoch 91/200\n",
      "1249/1250 [============================>.] - ETA: 0s - loss: 0.1865\n",
      "Epoch 91: val_loss did not improve from 0.18102\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 0.1865 - val_loss: 0.2124 - lr: 1.0000e-04\n",
      "\n",
      "Epoch 92: LearningRateScheduler setting learning rate to 0.00010000000000000003.\n",
      "Epoch 92/200\n",
      "1249/1250 [============================>.] - ETA: 0s - loss: 0.2376\n",
      "Epoch 92: val_loss did not improve from 0.18102\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 0.2376 - val_loss: 0.2434 - lr: 1.0000e-04\n",
      "\n",
      "Epoch 93: LearningRateScheduler setting learning rate to 0.00010000000000000003.\n",
      "Epoch 93/200\n",
      "1240/1250 [============================>.] - ETA: 0s - loss: 0.2773\n",
      "Epoch 93: val_loss did not improve from 0.18102\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 0.2777 - val_loss: 0.2646 - lr: 1.0000e-04\n",
      "\n",
      "Epoch 94: LearningRateScheduler setting learning rate to 0.00010000000000000003.\n",
      "Epoch 94/200\n",
      "1250/1250 [==============================] - ETA: 0s - loss: 0.3019\n",
      "Epoch 94: val_loss did not improve from 0.18102\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 0.3019 - val_loss: 0.2788 - lr: 1.0000e-04\n",
      "\n",
      "Epoch 95: LearningRateScheduler setting learning rate to 0.00010000000000000003.\n",
      "Epoch 95/200\n",
      "1247/1250 [============================>.] - ETA: 0s - loss: 0.3182\n",
      "Epoch 95: val_loss did not improve from 0.18102\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 0.3181 - val_loss: 0.2912 - lr: 1.0000e-04\n",
      "\n",
      "Epoch 96: LearningRateScheduler setting learning rate to 0.00010000000000000003.\n",
      "Epoch 96/200\n",
      "1249/1250 [============================>.] - ETA: 0s - loss: 0.3313\n",
      "Epoch 96: val_loss did not improve from 0.18102\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 0.3313 - val_loss: 0.3005 - lr: 1.0000e-04\n",
      "\n",
      "Epoch 97: LearningRateScheduler setting learning rate to 0.00010000000000000003.\n",
      "Epoch 97/200\n",
      "1238/1250 [============================>.] - ETA: 0s - loss: 0.3416\n",
      "Epoch 97: val_loss did not improve from 0.18102\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 0.3412 - val_loss: 0.3077 - lr: 1.0000e-04\n",
      "\n",
      "Epoch 98: LearningRateScheduler setting learning rate to 0.00010000000000000003.\n",
      "Epoch 98/200\n",
      "1249/1250 [============================>.] - ETA: 0s - loss: 0.3476\n",
      "Epoch 98: val_loss did not improve from 0.18102\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 0.3478 - val_loss: 0.3116 - lr: 1.0000e-04\n",
      "\n",
      "Epoch 99: LearningRateScheduler setting learning rate to 0.00010000000000000003.\n",
      "Epoch 99/200\n",
      "1248/1250 [============================>.] - ETA: 0s - loss: 0.3536\n",
      "Epoch 99: val_loss did not improve from 0.18102\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 0.3535 - val_loss: 0.3171 - lr: 1.0000e-04\n",
      "\n",
      "Epoch 100: LearningRateScheduler setting learning rate to 0.00010000000000000003.\n",
      "Epoch 100/200\n",
      "1243/1250 [============================>.] - ETA: 0s - loss: 0.3554\n",
      "Epoch 100: val_loss did not improve from 0.18102\n",
      "1250/1250 [==============================] - 6s 4ms/step - loss: 0.3557 - val_loss: 0.3208 - lr: 1.0000e-04\n",
      "\n",
      "Epoch 101: LearningRateScheduler setting learning rate to 0.00010000000000000003.\n",
      "Epoch 101/200\n",
      "1246/1250 [============================>.] - ETA: 0s - loss: 0.3604\n",
      "Epoch 101: val_loss did not improve from 0.18102\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 0.3605 - val_loss: 0.3228 - lr: 1.0000e-04\n",
      "\n",
      "Epoch 102: LearningRateScheduler setting learning rate to 0.00010000000000000003.\n",
      "Epoch 102/200\n",
      "1250/1250 [==============================] - ETA: 0s - loss: 0.3647\n",
      "Epoch 102: val_loss did not improve from 0.18102\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 0.3647 - val_loss: 0.3254 - lr: 1.0000e-04\n",
      "\n",
      "Epoch 103: LearningRateScheduler setting learning rate to 0.00010000000000000003.\n",
      "Epoch 103/200\n",
      "1244/1250 [============================>.] - ETA: 0s - loss: 0.3652\n",
      "Epoch 103: val_loss did not improve from 0.18102\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 0.3650 - val_loss: 0.3271 - lr: 1.0000e-04\n",
      "\n",
      "Epoch 104: LearningRateScheduler setting learning rate to 0.00010000000000000003.\n",
      "Epoch 104/200\n",
      "1242/1250 [============================>.] - ETA: 0s - loss: 0.3674\n",
      "Epoch 104: val_loss did not improve from 0.18102\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 0.3673 - val_loss: 0.3286 - lr: 1.0000e-04\n",
      "\n",
      "Epoch 105: LearningRateScheduler setting learning rate to 0.00010000000000000003.\n",
      "Epoch 105/200\n",
      "1246/1250 [============================>.] - ETA: 0s - loss: 0.3693\n",
      "Epoch 105: val_loss did not improve from 0.18102\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 0.3693 - val_loss: 0.3260 - lr: 1.0000e-04\n",
      "\n",
      "Epoch 106: LearningRateScheduler setting learning rate to 0.00010000000000000003.\n",
      "Epoch 106/200\n",
      "1244/1250 [============================>.] - ETA: 0s - loss: 0.3695\n",
      "Epoch 106: val_loss did not improve from 0.18102\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 0.3697 - val_loss: 0.3279 - lr: 1.0000e-04\n",
      "\n",
      "Epoch 107: LearningRateScheduler setting learning rate to 0.00010000000000000003.\n",
      "Epoch 107/200\n",
      "1244/1250 [============================>.] - ETA: 0s - loss: 0.3699\n",
      "Epoch 107: val_loss did not improve from 0.18102\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 0.3697 - val_loss: 0.3292 - lr: 1.0000e-04\n",
      "\n",
      "Epoch 108: LearningRateScheduler setting learning rate to 0.00010000000000000003.\n",
      "Epoch 108/200\n",
      "1247/1250 [============================>.] - ETA: 0s - loss: 0.3702\n",
      "Epoch 108: val_loss did not improve from 0.18102\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 0.3702 - val_loss: 0.3283 - lr: 1.0000e-04\n",
      "\n",
      "Epoch 109: LearningRateScheduler setting learning rate to 0.00010000000000000003.\n",
      "Epoch 109/200\n",
      "1244/1250 [============================>.] - ETA: 0s - loss: 0.3701\n",
      "Epoch 109: val_loss did not improve from 0.18102\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 0.3699 - val_loss: 0.3268 - lr: 1.0000e-04\n",
      "\n",
      "Epoch 110: LearningRateScheduler setting learning rate to 0.00010000000000000003.\n",
      "Epoch 110/200\n",
      "1248/1250 [============================>.] - ETA: 0s - loss: 0.3699\n",
      "Epoch 110: val_loss did not improve from 0.18102\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 0.3699 - val_loss: 0.3301 - lr: 1.0000e-04\n",
      "\n",
      "Epoch 111: LearningRateScheduler setting learning rate to 0.00010000000000000003.\n",
      "Epoch 111/200\n",
      "1238/1250 [============================>.] - ETA: 0s - loss: 0.3691\n",
      "Epoch 111: val_loss did not improve from 0.18102\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 0.3690 - val_loss: 0.3271 - lr: 1.0000e-04\n",
      "\n",
      "Epoch 112: LearningRateScheduler setting learning rate to 0.00010000000000000003.\n",
      "Epoch 112/200\n",
      "1250/1250 [==============================] - ETA: 0s - loss: 0.3709\n",
      "Epoch 112: val_loss did not improve from 0.18102\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 0.3709 - val_loss: 0.3278 - lr: 1.0000e-04\n",
      "\n",
      "Epoch 113: LearningRateScheduler setting learning rate to 0.00010000000000000003.\n",
      "Epoch 113/200\n",
      "1241/1250 [============================>.] - ETA: 0s - loss: 0.3707\n",
      "Epoch 113: val_loss did not improve from 0.18102\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 0.3711 - val_loss: 0.3263 - lr: 1.0000e-04\n",
      "\n",
      "Epoch 114: LearningRateScheduler setting learning rate to 0.00010000000000000003.\n",
      "Epoch 114/200\n",
      "1250/1250 [==============================] - ETA: 0s - loss: 0.3699\n",
      "Epoch 114: val_loss did not improve from 0.18102\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 0.3699 - val_loss: 0.3254 - lr: 1.0000e-04\n",
      "\n",
      "Epoch 115: LearningRateScheduler setting learning rate to 0.00010000000000000003.\n",
      "Epoch 115/200\n",
      "1242/1250 [============================>.] - ETA: 0s - loss: 0.3691Restoring model weights from the end of the best epoch: 65.\n",
      "\n",
      "Epoch 115: val_loss did not improve from 0.18102\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 0.3691 - val_loss: 0.3278 - lr: 1.0000e-04\n",
      "Epoch 115: early stopping\n"
     ]
    }
   ],
   "source": [
    "history = siamese.fit(\n",
    "    [x_train_1, x_train_2, x_train_3],\n",
    "    np.zeros(x_train_1.shape[0]),\n",
    "    validation_data=([x_val_1, x_val_2,x_val_3], np.zeros(x_val_1.shape[0])),\n",
    "    batch_size=32,\n",
    "    epochs=200,verbose=1,callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KSjXYvL6O8Xb",
    "outputId": "f86882f1-c2b1-4e13-cc9c-f1f7016bf6b0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 2ms/step - loss: 0.1810\n",
      "val loss, val acc: 0.1810242384672165\n"
     ]
    }
   ],
   "source": [
    "results = siamese.evaluate([x_val_1, x_val_2,x_val_3], np.zeros((x_val_1.shape[0])))\n",
    "print(\"val loss, val acc:\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CiwnAJNoOSfN"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dZsziiWVQmYF"
   },
   "outputs": [],
   "source": [
    "x_train_embedding=embedding_network.predict(x_train)\n",
    "x_val_embedding=embedding_network.predict(x_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D_a0CJETRTQx",
    "outputId": "d3949c9a-fc75-4058-b915-1684e2defcc0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40000, 1000)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eNtylOyUgQj3",
    "outputId": "2654d61a-00fd-47f0-e0de-f0b672fc35e6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 1000)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_val_embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1AAviofSRV4w",
    "outputId": "fa29596a-0610-42a3-e0c6-3eee574b144b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_8\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_11 (InputLayer)       [(None, 1000)]            0         \n",
      "                                                                 \n",
      " dropout_7 (Dropout)         (None, 1000)              0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 1000)              1001000   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,001,000\n",
      "Trainable params: 1,001,000\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inputs_final = keras.Input(shape=(1000))\n",
    "x = keras.layers.Dropout(0.2)(inputs_final)  # Regularize with dropout\n",
    "# x=keras.layers.Dense(1000)(x)\n",
    "# x = keras.layers.Dropout(0.2)(x)  # Regularize with dropout\n",
    "outputs = keras.layers.Dense(1000,activation='softmax')(x)\n",
    "model = keras.Model(inputs_final, outputs)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CnedBKFBT1Cd"
   },
   "outputs": [],
   "source": [
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(\"./drive/MyDrive/SGD_siamese_triplet_embeddings_transferLearning2.h5\",\n",
    "                             monitor=\"val_accuracy\",\n",
    "                             mode=\"max\",\n",
    "                             save_best_only = True,\n",
    "                             verbose=1)\n",
    "\n",
    "\n",
    "earlystop = tf.keras.callbacks.EarlyStopping(monitor = 'val_accuracy', # value being monitored for improvement\n",
    "                          min_delta = 0, #Abs value and is the min change required before we stop\n",
    "                          patience = 50, #Number of epochs we wait before stopping \n",
    "                          verbose = 1,\n",
    "                          restore_best_weights = True) #keeps the best weigths once stopped\n",
    "def scheduler(epoch, lr):\n",
    "  return(0.1*(0.1 ** (epoch // 30)))\n",
    "\n",
    "lr_scheduler=tf.keras.callbacks.LearningRateScheduler(scheduler,verbose=1)\n",
    "# we put our call backs into a callback list\n",
    "callbacks = [earlystop,checkpoint,\n",
    "             lr_scheduler\n",
    "             ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YXmA7FtJT37B"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer= tfa.optimizers.SGDW(learning_rate=0.1, momentum=0.9, nesterov=False,weight_decay=1e-4), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mI4ganKDgjT8",
    "outputId": "291e4d74-7135-440f-b683-3d59abdcd624"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([338, 338, 338, ..., 591, 591, 591])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E9wbgQLST9kc"
   },
   "outputs": [],
   "source": [
    "#onehotencoding\n",
    "y_train1=np.eye(1000)[y_train]\n",
    "y_val1=np.eye(1000)[y_val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DscXEeniRtWf",
    "outputId": "39109a5e-f966-42a9-f44c-02da69e45320"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: LearningRateScheduler setting learning rate to 0.1.\n",
      "Epoch 1/100\n",
      "1233/1250 [============================>.] - ETA: 0s - loss: 6.3642 - accuracy: 0.0483\n",
      "Epoch 1: val_accuracy improved from -inf to 0.14750, saving model to ./drive/MyDrive/SGD_siamese_triplet_embeddings_transferLearning2.h5\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 6.3581 - accuracy: 0.0488 - val_loss: 5.8165 - val_accuracy: 0.1475 - lr: 0.1000\n",
      "\n",
      "Epoch 2: LearningRateScheduler setting learning rate to 0.1.\n",
      "Epoch 2/100\n",
      "1235/1250 [============================>.] - ETA: 0s - loss: 5.5532 - accuracy: 0.1223\n",
      "Epoch 2: val_accuracy improved from 0.14750 to 0.19300, saving model to ./drive/MyDrive/SGD_siamese_triplet_embeddings_transferLearning2.h5\n",
      "1250/1250 [==============================] - 3s 3ms/step - loss: 5.5499 - accuracy: 0.1226 - val_loss: 5.2143 - val_accuracy: 0.1930 - lr: 0.1000\n",
      "\n",
      "Epoch 3: LearningRateScheduler setting learning rate to 0.1.\n",
      "Epoch 3/100\n",
      "1227/1250 [============================>.] - ETA: 0s - loss: 5.1041 - accuracy: 0.1580\n",
      "Epoch 3: val_accuracy improved from 0.19300 to 0.21610, saving model to ./drive/MyDrive/SGD_siamese_triplet_embeddings_transferLearning2.h5\n",
      "1250/1250 [==============================] - 3s 3ms/step - loss: 5.1020 - accuracy: 0.1579 - val_loss: 4.8582 - val_accuracy: 0.2161 - lr: 0.1000\n",
      "\n",
      "Epoch 4: LearningRateScheduler setting learning rate to 0.1.\n",
      "Epoch 4/100\n",
      "1245/1250 [============================>.] - ETA: 0s - loss: 4.8296 - accuracy: 0.1884\n",
      "Epoch 4: val_accuracy improved from 0.21610 to 0.23970, saving model to ./drive/MyDrive/SGD_siamese_triplet_embeddings_transferLearning2.h5\n",
      "1250/1250 [==============================] - 3s 3ms/step - loss: 4.8295 - accuracy: 0.1883 - val_loss: 4.6293 - val_accuracy: 0.2397 - lr: 0.1000\n",
      "\n",
      "Epoch 5: LearningRateScheduler setting learning rate to 0.1.\n",
      "Epoch 5/100\n",
      "1235/1250 [============================>.] - ETA: 0s - loss: 4.6535 - accuracy: 0.2030\n",
      "Epoch 5: val_accuracy improved from 0.23970 to 0.25450, saving model to ./drive/MyDrive/SGD_siamese_triplet_embeddings_transferLearning2.h5\n",
      "1250/1250 [==============================] - 3s 3ms/step - loss: 4.6524 - accuracy: 0.2031 - val_loss: 4.4766 - val_accuracy: 0.2545 - lr: 0.1000\n",
      "\n",
      "Epoch 6: LearningRateScheduler setting learning rate to 0.1.\n",
      "Epoch 6/100\n",
      "1240/1250 [============================>.] - ETA: 0s - loss: 4.5265 - accuracy: 0.2171\n",
      "Epoch 6: val_accuracy improved from 0.25450 to 0.26550, saving model to ./drive/MyDrive/SGD_siamese_triplet_embeddings_transferLearning2.h5\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 4.5263 - accuracy: 0.2172 - val_loss: 4.3694 - val_accuracy: 0.2655 - lr: 0.1000\n",
      "\n",
      "Epoch 7: LearningRateScheduler setting learning rate to 0.1.\n",
      "Epoch 7/100\n",
      "1243/1250 [============================>.] - ETA: 0s - loss: 4.4431 - accuracy: 0.2252\n",
      "Epoch 7: val_accuracy improved from 0.26550 to 0.26690, saving model to ./drive/MyDrive/SGD_siamese_triplet_embeddings_transferLearning2.h5\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 4.4425 - accuracy: 0.2253 - val_loss: 4.2925 - val_accuracy: 0.2669 - lr: 0.1000\n",
      "\n",
      "Epoch 8: LearningRateScheduler setting learning rate to 0.1.\n",
      "Epoch 8/100\n",
      "1248/1250 [============================>.] - ETA: 0s - loss: 4.3762 - accuracy: 0.2321\n",
      "Epoch 8: val_accuracy improved from 0.26690 to 0.27850, saving model to ./drive/MyDrive/SGD_siamese_triplet_embeddings_transferLearning2.h5\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 4.3764 - accuracy: 0.2321 - val_loss: 4.2360 - val_accuracy: 0.2785 - lr: 0.1000\n",
      "\n",
      "Epoch 9: LearningRateScheduler setting learning rate to 0.1.\n",
      "Epoch 9/100\n",
      "1240/1250 [============================>.] - ETA: 0s - loss: 4.3253 - accuracy: 0.2360\n",
      "Epoch 9: val_accuracy improved from 0.27850 to 0.28110, saving model to ./drive/MyDrive/SGD_siamese_triplet_embeddings_transferLearning2.h5\n",
      "1250/1250 [==============================] - 3s 3ms/step - loss: 4.3256 - accuracy: 0.2359 - val_loss: 4.1942 - val_accuracy: 0.2811 - lr: 0.1000\n",
      "\n",
      "Epoch 10: LearningRateScheduler setting learning rate to 0.1.\n",
      "Epoch 10/100\n",
      "1239/1250 [============================>.] - ETA: 0s - loss: 4.2909 - accuracy: 0.2406\n",
      "Epoch 10: val_accuracy improved from 0.28110 to 0.28130, saving model to ./drive/MyDrive/SGD_siamese_triplet_embeddings_transferLearning2.h5\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 4.2912 - accuracy: 0.2406 - val_loss: 4.1613 - val_accuracy: 0.2813 - lr: 0.1000\n",
      "\n",
      "Epoch 11: LearningRateScheduler setting learning rate to 0.1.\n",
      "Epoch 11/100\n",
      "1237/1250 [============================>.] - ETA: 0s - loss: 4.2658 - accuracy: 0.2419\n",
      "Epoch 11: val_accuracy improved from 0.28130 to 0.28750, saving model to ./drive/MyDrive/SGD_siamese_triplet_embeddings_transferLearning2.h5\n",
      "1250/1250 [==============================] - 3s 3ms/step - loss: 4.2655 - accuracy: 0.2420 - val_loss: 4.1385 - val_accuracy: 0.2875 - lr: 0.1000\n",
      "\n",
      "Epoch 12: LearningRateScheduler setting learning rate to 0.1.\n",
      "Epoch 12/100\n",
      "1242/1250 [============================>.] - ETA: 0s - loss: 4.2447 - accuracy: 0.2474\n",
      "Epoch 12: val_accuracy improved from 0.28750 to 0.29150, saving model to ./drive/MyDrive/SGD_siamese_triplet_embeddings_transferLearning2.h5\n",
      "1250/1250 [==============================] - 3s 3ms/step - loss: 4.2442 - accuracy: 0.2475 - val_loss: 4.1187 - val_accuracy: 0.2915 - lr: 0.1000\n",
      "\n",
      "Epoch 13: LearningRateScheduler setting learning rate to 0.1.\n",
      "Epoch 13/100\n",
      "1237/1250 [============================>.] - ETA: 0s - loss: 4.2266 - accuracy: 0.2524\n",
      "Epoch 13: val_accuracy did not improve from 0.29150\n",
      "1250/1250 [==============================] - 3s 3ms/step - loss: 4.2275 - accuracy: 0.2524 - val_loss: 4.1047 - val_accuracy: 0.2907 - lr: 0.1000\n",
      "\n",
      "Epoch 14: LearningRateScheduler setting learning rate to 0.1.\n",
      "Epoch 14/100\n",
      "1247/1250 [============================>.] - ETA: 0s - loss: 4.2154 - accuracy: 0.2502\n",
      "Epoch 14: val_accuracy did not improve from 0.29150\n",
      "1250/1250 [==============================] - 3s 3ms/step - loss: 4.2155 - accuracy: 0.2501 - val_loss: 4.0932 - val_accuracy: 0.2905 - lr: 0.1000\n",
      "\n",
      "Epoch 15: LearningRateScheduler setting learning rate to 0.1.\n",
      "Epoch 15/100\n",
      "1229/1250 [============================>.] - ETA: 0s - loss: 4.2065 - accuracy: 0.2522\n",
      "Epoch 15: val_accuracy improved from 0.29150 to 0.29530, saving model to ./drive/MyDrive/SGD_siamese_triplet_embeddings_transferLearning2.h5\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 4.2068 - accuracy: 0.2521 - val_loss: 4.0834 - val_accuracy: 0.2953 - lr: 0.1000\n",
      "\n",
      "Epoch 16: LearningRateScheduler setting learning rate to 0.1.\n",
      "Epoch 16/100\n",
      "1230/1250 [============================>.] - ETA: 0s - loss: 4.1953 - accuracy: 0.2549\n",
      "Epoch 16: val_accuracy improved from 0.29530 to 0.29810, saving model to ./drive/MyDrive/SGD_siamese_triplet_embeddings_transferLearning2.h5\n",
      "1250/1250 [==============================] - 3s 3ms/step - loss: 4.1954 - accuracy: 0.2555 - val_loss: 4.0770 - val_accuracy: 0.2981 - lr: 0.1000\n",
      "\n",
      "Epoch 17: LearningRateScheduler setting learning rate to 0.1.\n",
      "Epoch 17/100\n",
      "1232/1250 [============================>.] - ETA: 0s - loss: 4.1908 - accuracy: 0.2550\n",
      "Epoch 17: val_accuracy did not improve from 0.29810\n",
      "1250/1250 [==============================] - 3s 3ms/step - loss: 4.1911 - accuracy: 0.2549 - val_loss: 4.0729 - val_accuracy: 0.2925 - lr: 0.1000\n",
      "\n",
      "Epoch 18: LearningRateScheduler setting learning rate to 0.1.\n",
      "Epoch 18/100\n",
      "1243/1250 [============================>.] - ETA: 0s - loss: 4.1842 - accuracy: 0.2550\n",
      "Epoch 18: val_accuracy improved from 0.29810 to 0.30190, saving model to ./drive/MyDrive/SGD_siamese_triplet_embeddings_transferLearning2.h5\n",
      "1250/1250 [==============================] - 3s 3ms/step - loss: 4.1836 - accuracy: 0.2553 - val_loss: 4.0671 - val_accuracy: 0.3019 - lr: 0.1000\n",
      "\n",
      "Epoch 19: LearningRateScheduler setting learning rate to 0.1.\n",
      "Epoch 19/100\n",
      "1250/1250 [==============================] - ETA: 0s - loss: 4.1769 - accuracy: 0.2589\n",
      "Epoch 19: val_accuracy did not improve from 0.30190\n",
      "1250/1250 [==============================] - 3s 3ms/step - loss: 4.1769 - accuracy: 0.2589 - val_loss: 4.0636 - val_accuracy: 0.2964 - lr: 0.1000\n",
      "\n",
      "Epoch 20: LearningRateScheduler setting learning rate to 0.1.\n",
      "Epoch 20/100\n",
      "1247/1250 [============================>.] - ETA: 0s - loss: 4.1782 - accuracy: 0.2556\n",
      "Epoch 20: val_accuracy did not improve from 0.30190\n",
      "1250/1250 [==============================] - 3s 3ms/step - loss: 4.1778 - accuracy: 0.2556 - val_loss: 4.0610 - val_accuracy: 0.2898 - lr: 0.1000\n",
      "\n",
      "Epoch 21: LearningRateScheduler setting learning rate to 0.1.\n",
      "Epoch 21/100\n",
      "1241/1250 [============================>.] - ETA: 0s - loss: 4.1772 - accuracy: 0.2578\n",
      "Epoch 21: val_accuracy did not improve from 0.30190\n",
      "1250/1250 [==============================] - 3s 3ms/step - loss: 4.1772 - accuracy: 0.2577 - val_loss: 4.0587 - val_accuracy: 0.2984 - lr: 0.1000\n",
      "\n",
      "Epoch 22: LearningRateScheduler setting learning rate to 0.1.\n",
      "Epoch 22/100\n",
      "1237/1250 [============================>.] - ETA: 0s - loss: 4.1751 - accuracy: 0.2578\n",
      "Epoch 22: val_accuracy did not improve from 0.30190\n",
      "1250/1250 [==============================] - 3s 3ms/step - loss: 4.1738 - accuracy: 0.2579 - val_loss: 4.0577 - val_accuracy: 0.2956 - lr: 0.1000\n",
      "\n",
      "Epoch 23: LearningRateScheduler setting learning rate to 0.1.\n",
      "Epoch 23/100\n",
      "1247/1250 [============================>.] - ETA: 0s - loss: 4.1718 - accuracy: 0.2568\n",
      "Epoch 23: val_accuracy did not improve from 0.30190\n",
      "1250/1250 [==============================] - 3s 3ms/step - loss: 4.1720 - accuracy: 0.2567 - val_loss: 4.0554 - val_accuracy: 0.2982 - lr: 0.1000\n",
      "\n",
      "Epoch 24: LearningRateScheduler setting learning rate to 0.1.\n",
      "Epoch 24/100\n",
      "1235/1250 [============================>.] - ETA: 0s - loss: 4.1686 - accuracy: 0.2582\n",
      "Epoch 24: val_accuracy did not improve from 0.30190\n",
      "1250/1250 [==============================] - 3s 3ms/step - loss: 4.1683 - accuracy: 0.2581 - val_loss: 4.0540 - val_accuracy: 0.2989 - lr: 0.1000\n",
      "\n",
      "Epoch 25: LearningRateScheduler setting learning rate to 0.1.\n",
      "Epoch 25/100\n",
      "1229/1250 [============================>.] - ETA: 0s - loss: 4.1695 - accuracy: 0.2592\n",
      "Epoch 25: val_accuracy did not improve from 0.30190\n",
      "1250/1250 [==============================] - 3s 3ms/step - loss: 4.1699 - accuracy: 0.2589 - val_loss: 4.0530 - val_accuracy: 0.2970 - lr: 0.1000\n",
      "\n",
      "Epoch 26: LearningRateScheduler setting learning rate to 0.1.\n",
      "Epoch 26/100\n",
      "1242/1250 [============================>.] - ETA: 0s - loss: 4.1650 - accuracy: 0.2616\n",
      "Epoch 26: val_accuracy did not improve from 0.30190\n",
      "1250/1250 [==============================] - 3s 3ms/step - loss: 4.1654 - accuracy: 0.2616 - val_loss: 4.0518 - val_accuracy: 0.3013 - lr: 0.1000\n",
      "\n",
      "Epoch 27: LearningRateScheduler setting learning rate to 0.1.\n",
      "Epoch 27/100\n",
      "1235/1250 [============================>.] - ETA: 0s - loss: 4.1659 - accuracy: 0.2581\n",
      "Epoch 27: val_accuracy did not improve from 0.30190\n",
      "1250/1250 [==============================] - 3s 3ms/step - loss: 4.1665 - accuracy: 0.2579 - val_loss: 4.0517 - val_accuracy: 0.3019 - lr: 0.1000\n",
      "\n",
      "Epoch 28: LearningRateScheduler setting learning rate to 0.1.\n",
      "Epoch 28/100\n",
      "1234/1250 [============================>.] - ETA: 0s - loss: 4.1711 - accuracy: 0.2564\n",
      "Epoch 28: val_accuracy did not improve from 0.30190\n",
      "1250/1250 [==============================] - 3s 3ms/step - loss: 4.1714 - accuracy: 0.2562 - val_loss: 4.0519 - val_accuracy: 0.3007 - lr: 0.1000\n",
      "\n",
      "Epoch 29: LearningRateScheduler setting learning rate to 0.1.\n",
      "Epoch 29/100\n",
      "1239/1250 [============================>.] - ETA: 0s - loss: 4.1671 - accuracy: 0.2598\n",
      "Epoch 29: val_accuracy did not improve from 0.30190\n",
      "1250/1250 [==============================] - 3s 3ms/step - loss: 4.1667 - accuracy: 0.2596 - val_loss: 4.0517 - val_accuracy: 0.3017 - lr: 0.1000\n",
      "\n",
      "Epoch 30: LearningRateScheduler setting learning rate to 0.1.\n",
      "Epoch 30/100\n",
      "1230/1250 [============================>.] - ETA: 0s - loss: 4.1631 - accuracy: 0.2577\n",
      "Epoch 30: val_accuracy did not improve from 0.30190\n",
      "1250/1250 [==============================] - 3s 3ms/step - loss: 4.1642 - accuracy: 0.2573 - val_loss: 4.0513 - val_accuracy: 0.2954 - lr: 0.1000\n",
      "\n",
      "Epoch 31: LearningRateScheduler setting learning rate to 0.010000000000000002.\n",
      "Epoch 31/100\n",
      "1232/1250 [============================>.] - ETA: 0s - loss: 4.2151 - accuracy: 0.2801\n",
      "Epoch 31: val_accuracy improved from 0.30190 to 0.30320, saving model to ./drive/MyDrive/SGD_siamese_triplet_embeddings_transferLearning2.h5\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 4.2171 - accuracy: 0.2795 - val_loss: 4.2345 - val_accuracy: 0.3032 - lr: 0.0100\n",
      "\n",
      "Epoch 32: LearningRateScheduler setting learning rate to 0.010000000000000002.\n",
      "Epoch 32/100\n",
      "1246/1250 [============================>.] - ETA: 0s - loss: 4.3862 - accuracy: 0.2811\n",
      "Epoch 32: val_accuracy improved from 0.30320 to 0.30590, saving model to ./drive/MyDrive/SGD_siamese_triplet_embeddings_transferLearning2.h5\n",
      "1250/1250 [==============================] - 3s 3ms/step - loss: 4.3865 - accuracy: 0.2810 - val_loss: 4.4156 - val_accuracy: 0.3059 - lr: 0.0100\n",
      "\n",
      "Epoch 33: LearningRateScheduler setting learning rate to 0.010000000000000002.\n",
      "Epoch 33/100\n",
      "1250/1250 [==============================] - ETA: 0s - loss: 4.5535 - accuracy: 0.2800\n",
      "Epoch 33: val_accuracy did not improve from 0.30590\n",
      "1250/1250 [==============================] - 3s 3ms/step - loss: 4.5535 - accuracy: 0.2800 - val_loss: 4.5884 - val_accuracy: 0.3057 - lr: 0.0100\n",
      "\n",
      "Epoch 34: LearningRateScheduler setting learning rate to 0.010000000000000002.\n",
      "Epoch 34/100\n",
      "1229/1250 [============================>.] - ETA: 0s - loss: 4.7109 - accuracy: 0.2785\n",
      "Epoch 34: val_accuracy did not improve from 0.30590\n",
      "1250/1250 [==============================] - 3s 3ms/step - loss: 4.7115 - accuracy: 0.2786 - val_loss: 4.7497 - val_accuracy: 0.3043 - lr: 0.0100\n",
      "\n",
      "Epoch 35: LearningRateScheduler setting learning rate to 0.010000000000000002.\n",
      "Epoch 35/100\n",
      "1239/1250 [============================>.] - ETA: 0s - loss: 4.8596 - accuracy: 0.2737\n",
      "Epoch 35: val_accuracy did not improve from 0.30590\n",
      "1250/1250 [==============================] - 3s 3ms/step - loss: 4.8609 - accuracy: 0.2734 - val_loss: 4.8978 - val_accuracy: 0.3010 - lr: 0.0100\n",
      "\n",
      "Epoch 36: LearningRateScheduler setting learning rate to 0.010000000000000002.\n",
      "Epoch 36/100\n",
      "1249/1250 [============================>.] - ETA: 0s - loss: 4.9958 - accuracy: 0.2689\n",
      "Epoch 36: val_accuracy did not improve from 0.30590\n",
      "1250/1250 [==============================] - 3s 3ms/step - loss: 4.9958 - accuracy: 0.2690 - val_loss: 5.0320 - val_accuracy: 0.2954 - lr: 0.0100\n",
      "\n",
      "Epoch 37: LearningRateScheduler setting learning rate to 0.010000000000000002.\n",
      "Epoch 37/100\n",
      "1242/1250 [============================>.] - ETA: 0s - loss: 5.1186 - accuracy: 0.2645\n",
      "Epoch 37: val_accuracy did not improve from 0.30590\n",
      "1250/1250 [==============================] - 3s 3ms/step - loss: 5.1196 - accuracy: 0.2641 - val_loss: 5.1525 - val_accuracy: 0.2883 - lr: 0.0100\n",
      "\n",
      "Epoch 38: LearningRateScheduler setting learning rate to 0.010000000000000002.\n",
      "Epoch 38/100\n",
      "1231/1250 [============================>.] - ETA: 0s - loss: 5.2307 - accuracy: 0.2577\n",
      "Epoch 38: val_accuracy did not improve from 0.30590\n",
      "1250/1250 [==============================] - 3s 3ms/step - loss: 5.2317 - accuracy: 0.2575 - val_loss: 5.2600 - val_accuracy: 0.2808 - lr: 0.0100\n",
      "\n",
      "Epoch 39: LearningRateScheduler setting learning rate to 0.010000000000000002.\n",
      "Epoch 39/100\n",
      "1240/1250 [============================>.] - ETA: 0s - loss: 5.3259 - accuracy: 0.2513\n",
      "Epoch 39: val_accuracy did not improve from 0.30590\n",
      "1250/1250 [==============================] - 3s 3ms/step - loss: 5.3261 - accuracy: 0.2510 - val_loss: 5.3550 - val_accuracy: 0.2745 - lr: 0.0100\n",
      "\n",
      "Epoch 40: LearningRateScheduler setting learning rate to 0.010000000000000002.\n",
      "Epoch 40/100\n",
      "1232/1250 [============================>.] - ETA: 0s - loss: 5.4126 - accuracy: 0.2476\n",
      "Epoch 40: val_accuracy did not improve from 0.30590\n",
      "1250/1250 [==============================] - 3s 3ms/step - loss: 5.4143 - accuracy: 0.2476 - val_loss: 5.4387 - val_accuracy: 0.2683 - lr: 0.0100\n",
      "\n",
      "Epoch 41: LearningRateScheduler setting learning rate to 0.010000000000000002.\n",
      "Epoch 41/100\n",
      "1249/1250 [============================>.] - ETA: 0s - loss: 5.4904 - accuracy: 0.2391\n",
      "Epoch 41: val_accuracy did not improve from 0.30590\n",
      "1250/1250 [==============================] - 3s 3ms/step - loss: 5.4905 - accuracy: 0.2391 - val_loss: 5.5123 - val_accuracy: 0.2633 - lr: 0.0100\n",
      "\n",
      "Epoch 42: LearningRateScheduler setting learning rate to 0.010000000000000002.\n",
      "Epoch 42/100\n",
      "1237/1250 [============================>.] - ETA: 0s - loss: 5.5575 - accuracy: 0.2348\n",
      "Epoch 42: val_accuracy did not improve from 0.30590\n",
      "1250/1250 [==============================] - 3s 3ms/step - loss: 5.5579 - accuracy: 0.2347 - val_loss: 5.5767 - val_accuracy: 0.2556 - lr: 0.0100\n",
      "\n",
      "Epoch 43: LearningRateScheduler setting learning rate to 0.010000000000000002.\n",
      "Epoch 43/100\n",
      "1233/1250 [============================>.] - ETA: 0s - loss: 5.6196 - accuracy: 0.2311\n",
      "Epoch 43: val_accuracy did not improve from 0.30590\n",
      "1250/1250 [==============================] - 3s 3ms/step - loss: 5.6202 - accuracy: 0.2309 - val_loss: 5.6331 - val_accuracy: 0.2483 - lr: 0.0100\n",
      "\n",
      "Epoch 44: LearningRateScheduler setting learning rate to 0.010000000000000002.\n",
      "Epoch 44/100\n",
      "1232/1250 [============================>.] - ETA: 0s - loss: 5.6701 - accuracy: 0.2241\n",
      "Epoch 44: val_accuracy did not improve from 0.30590\n",
      "1250/1250 [==============================] - 3s 3ms/step - loss: 5.6703 - accuracy: 0.2241 - val_loss: 5.6822 - val_accuracy: 0.2429 - lr: 0.0100\n",
      "\n",
      "Epoch 45: LearningRateScheduler setting learning rate to 0.010000000000000002.\n",
      "Epoch 45/100\n",
      "1239/1250 [============================>.] - ETA: 0s - loss: 5.7176 - accuracy: 0.2177\n",
      "Epoch 45: val_accuracy did not improve from 0.30590\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 5.7174 - accuracy: 0.2176 - val_loss: 5.7251 - val_accuracy: 0.2370 - lr: 0.0100\n",
      "\n",
      "Epoch 46: LearningRateScheduler setting learning rate to 0.010000000000000002.\n",
      "Epoch 46/100\n",
      "1244/1250 [============================>.] - ETA: 0s - loss: 5.7554 - accuracy: 0.2130\n",
      "Epoch 46: val_accuracy did not improve from 0.30590\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 5.7552 - accuracy: 0.2133 - val_loss: 5.7623 - val_accuracy: 0.2340 - lr: 0.0100\n",
      "\n",
      "Epoch 47: LearningRateScheduler setting learning rate to 0.010000000000000002.\n",
      "Epoch 47/100\n",
      "1249/1250 [============================>.] - ETA: 0s - loss: 5.7901 - accuracy: 0.2104\n",
      "Epoch 47: val_accuracy did not improve from 0.30590\n",
      "1250/1250 [==============================] - 3s 3ms/step - loss: 5.7901 - accuracy: 0.2104 - val_loss: 5.7945 - val_accuracy: 0.2306 - lr: 0.0100\n",
      "\n",
      "Epoch 48: LearningRateScheduler setting learning rate to 0.010000000000000002.\n",
      "Epoch 48/100\n",
      "1237/1250 [============================>.] - ETA: 0s - loss: 5.8207 - accuracy: 0.2065\n",
      "Epoch 48: val_accuracy did not improve from 0.30590\n",
      "1250/1250 [==============================] - 3s 3ms/step - loss: 5.8211 - accuracy: 0.2063 - val_loss: 5.8226 - val_accuracy: 0.2251 - lr: 0.0100\n",
      "\n",
      "Epoch 49: LearningRateScheduler setting learning rate to 0.010000000000000002.\n",
      "Epoch 49/100\n",
      "1239/1250 [============================>.] - ETA: 0s - loss: 5.8455 - accuracy: 0.2011\n",
      "Epoch 49: val_accuracy did not improve from 0.30590\n",
      "1250/1250 [==============================] - 3s 3ms/step - loss: 5.8455 - accuracy: 0.2008 - val_loss: 5.8470 - val_accuracy: 0.2210 - lr: 0.0100\n",
      "\n",
      "Epoch 50: LearningRateScheduler setting learning rate to 0.010000000000000002.\n",
      "Epoch 50/100\n",
      "1243/1250 [============================>.] - ETA: 0s - loss: 5.8679 - accuracy: 0.1942\n",
      "Epoch 50: val_accuracy did not improve from 0.30590\n",
      "1250/1250 [==============================] - 3s 3ms/step - loss: 5.8679 - accuracy: 0.1942 - val_loss: 5.8681 - val_accuracy: 0.2158 - lr: 0.0100\n",
      "\n",
      "Epoch 51: LearningRateScheduler setting learning rate to 0.010000000000000002.\n",
      "Epoch 51/100\n",
      "1229/1250 [============================>.] - ETA: 0s - loss: 5.8866 - accuracy: 0.1957\n",
      "Epoch 51: val_accuracy did not improve from 0.30590\n",
      "1250/1250 [==============================] - 3s 3ms/step - loss: 5.8875 - accuracy: 0.1953 - val_loss: 5.8864 - val_accuracy: 0.2116 - lr: 0.0100\n",
      "\n",
      "Epoch 52: LearningRateScheduler setting learning rate to 0.010000000000000002.\n",
      "Epoch 52/100\n",
      "1233/1250 [============================>.] - ETA: 0s - loss: 5.9055 - accuracy: 0.1889\n",
      "Epoch 52: val_accuracy did not improve from 0.30590\n",
      "1250/1250 [==============================] - 3s 3ms/step - loss: 5.9055 - accuracy: 0.1891 - val_loss: 5.9023 - val_accuracy: 0.2081 - lr: 0.0100\n",
      "\n",
      "Epoch 53: LearningRateScheduler setting learning rate to 0.010000000000000002.\n",
      "Epoch 53/100\n",
      "1234/1250 [============================>.] - ETA: 0s - loss: 5.9191 - accuracy: 0.1845\n",
      "Epoch 53: val_accuracy did not improve from 0.30590\n",
      "1250/1250 [==============================] - 3s 3ms/step - loss: 5.9196 - accuracy: 0.1845 - val_loss: 5.9161 - val_accuracy: 0.2054 - lr: 0.0100\n",
      "\n",
      "Epoch 54: LearningRateScheduler setting learning rate to 0.010000000000000002.\n",
      "Epoch 54/100\n",
      "1240/1250 [============================>.] - ETA: 0s - loss: 5.9337 - accuracy: 0.1850\n",
      "Epoch 54: val_accuracy did not improve from 0.30590\n",
      "1250/1250 [==============================] - 3s 3ms/step - loss: 5.9342 - accuracy: 0.1849 - val_loss: 5.9281 - val_accuracy: 0.2056 - lr: 0.0100\n",
      "\n",
      "Epoch 55: LearningRateScheduler setting learning rate to 0.010000000000000002.\n",
      "Epoch 55/100\n",
      "1239/1250 [============================>.] - ETA: 0s - loss: 5.9441 - accuracy: 0.1827\n",
      "Epoch 55: val_accuracy did not improve from 0.30590\n",
      "1250/1250 [==============================] - 3s 3ms/step - loss: 5.9444 - accuracy: 0.1829 - val_loss: 5.9385 - val_accuracy: 0.2045 - lr: 0.0100\n",
      "\n",
      "Epoch 56: LearningRateScheduler setting learning rate to 0.010000000000000002.\n",
      "Epoch 56/100\n",
      "1231/1250 [============================>.] - ETA: 0s - loss: 5.9538 - accuracy: 0.1789\n",
      "Epoch 56: val_accuracy did not improve from 0.30590\n",
      "1250/1250 [==============================] - 3s 3ms/step - loss: 5.9537 - accuracy: 0.1787 - val_loss: 5.9475 - val_accuracy: 0.2025 - lr: 0.0100\n",
      "\n",
      "Epoch 57: LearningRateScheduler setting learning rate to 0.010000000000000002.\n",
      "Epoch 57/100\n",
      "1229/1250 [============================>.] - ETA: 0s - loss: 5.9617 - accuracy: 0.1774\n",
      "Epoch 57: val_accuracy did not improve from 0.30590\n",
      "1250/1250 [==============================] - 3s 3ms/step - loss: 5.9618 - accuracy: 0.1774 - val_loss: 5.9553 - val_accuracy: 0.1994 - lr: 0.0100\n",
      "\n",
      "Epoch 58: LearningRateScheduler setting learning rate to 0.010000000000000002.\n",
      "Epoch 58/100\n",
      "1238/1250 [============================>.] - ETA: 0s - loss: 5.9696 - accuracy: 0.1770\n",
      "Epoch 58: val_accuracy did not improve from 0.30590\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 5.9697 - accuracy: 0.1768 - val_loss: 5.9621 - val_accuracy: 0.1982 - lr: 0.0100\n",
      "\n",
      "Epoch 59: LearningRateScheduler setting learning rate to 0.010000000000000002.\n",
      "Epoch 59/100\n",
      "1250/1250 [==============================] - ETA: 0s - loss: 5.9754 - accuracy: 0.1721\n",
      "Epoch 59: val_accuracy did not improve from 0.30590\n",
      "1250/1250 [==============================] - 3s 3ms/step - loss: 5.9754 - accuracy: 0.1721 - val_loss: 5.9679 - val_accuracy: 0.1948 - lr: 0.0100\n",
      "\n",
      "Epoch 60: LearningRateScheduler setting learning rate to 0.010000000000000002.\n",
      "Epoch 60/100\n",
      "1250/1250 [==============================] - ETA: 0s - loss: 5.9812 - accuracy: 0.1723\n",
      "Epoch 60: val_accuracy did not improve from 0.30590\n",
      "1250/1250 [==============================] - 3s 3ms/step - loss: 5.9812 - accuracy: 0.1723 - val_loss: 5.9730 - val_accuracy: 0.1928 - lr: 0.0100\n",
      "\n",
      "Epoch 61: LearningRateScheduler setting learning rate to 0.0010000000000000002.\n",
      "Epoch 61/100\n",
      "1248/1250 [============================>.] - ETA: 0s - loss: 6.0235 - accuracy: 0.1791\n",
      "Epoch 61: val_accuracy did not improve from 0.30590\n",
      "1250/1250 [==============================] - 3s 3ms/step - loss: 6.0234 - accuracy: 0.1792 - val_loss: 6.0639 - val_accuracy: 0.1937 - lr: 0.0010\n",
      "\n",
      "Epoch 62: LearningRateScheduler setting learning rate to 0.0010000000000000002.\n",
      "Epoch 62/100\n",
      "1237/1250 [============================>.] - ETA: 0s - loss: 6.1088 - accuracy: 0.1800\n",
      "Epoch 62: val_accuracy did not improve from 0.30590\n",
      "1250/1250 [==============================] - 3s 3ms/step - loss: 6.1092 - accuracy: 0.1798 - val_loss: 6.1460 - val_accuracy: 0.1936 - lr: 0.0010\n",
      "\n",
      "Epoch 63: LearningRateScheduler setting learning rate to 0.0010000000000000002.\n",
      "Epoch 63/100\n",
      "1230/1250 [============================>.] - ETA: 0s - loss: 6.1835 - accuracy: 0.1796\n",
      "Epoch 63: val_accuracy did not improve from 0.30590\n",
      "1250/1250 [==============================] - 3s 3ms/step - loss: 6.1846 - accuracy: 0.1791 - val_loss: 6.2195 - val_accuracy: 0.1923 - lr: 0.0010\n",
      "\n",
      "Epoch 64: LearningRateScheduler setting learning rate to 0.0010000000000000002.\n",
      "Epoch 64/100\n",
      "1250/1250 [==============================] - ETA: 0s - loss: 6.2533 - accuracy: 0.1791\n",
      "Epoch 64: val_accuracy did not improve from 0.30590\n",
      "1250/1250 [==============================] - 3s 3ms/step - loss: 6.2533 - accuracy: 0.1791 - val_loss: 6.2851 - val_accuracy: 0.1927 - lr: 0.0010\n",
      "\n",
      "Epoch 65: LearningRateScheduler setting learning rate to 0.0010000000000000002.\n",
      "Epoch 65/100\n",
      "1232/1250 [============================>.] - ETA: 0s - loss: 6.3141 - accuracy: 0.1786\n",
      "Epoch 65: val_accuracy did not improve from 0.30590\n",
      "1250/1250 [==============================] - 3s 3ms/step - loss: 6.3148 - accuracy: 0.1786 - val_loss: 6.3435 - val_accuracy: 0.1913 - lr: 0.0010\n",
      "\n",
      "Epoch 66: LearningRateScheduler setting learning rate to 0.0010000000000000002.\n",
      "Epoch 66/100\n",
      "1244/1250 [============================>.] - ETA: 0s - loss: 6.3695 - accuracy: 0.1777\n",
      "Epoch 66: val_accuracy did not improve from 0.30590\n",
      "1250/1250 [==============================] - 3s 3ms/step - loss: 6.3694 - accuracy: 0.1778 - val_loss: 6.3954 - val_accuracy: 0.1912 - lr: 0.0010\n",
      "\n",
      "Epoch 67: LearningRateScheduler setting learning rate to 0.0010000000000000002.\n",
      "Epoch 67/100\n",
      "1248/1250 [============================>.] - ETA: 0s - loss: 6.4190 - accuracy: 0.1776\n",
      "Epoch 67: val_accuracy did not improve from 0.30590\n",
      "1250/1250 [==============================] - 3s 3ms/step - loss: 6.4190 - accuracy: 0.1777 - val_loss: 6.4416 - val_accuracy: 0.1904 - lr: 0.0010\n",
      "\n",
      "Epoch 68: LearningRateScheduler setting learning rate to 0.0010000000000000002.\n",
      "Epoch 68/100\n",
      "1244/1250 [============================>.] - ETA: 0s - loss: 6.4620 - accuracy: 0.1765\n",
      "Epoch 68: val_accuracy did not improve from 0.30590\n",
      "1250/1250 [==============================] - 3s 3ms/step - loss: 6.4620 - accuracy: 0.1766 - val_loss: 6.4824 - val_accuracy: 0.1889 - lr: 0.0010\n",
      "\n",
      "Epoch 69: LearningRateScheduler setting learning rate to 0.0010000000000000002.\n",
      "Epoch 69/100\n",
      "1240/1250 [============================>.] - ETA: 0s - loss: 6.5003 - accuracy: 0.1735\n",
      "Epoch 69: val_accuracy did not improve from 0.30590\n",
      "1250/1250 [==============================] - 3s 3ms/step - loss: 6.5004 - accuracy: 0.1737 - val_loss: 6.5187 - val_accuracy: 0.1875 - lr: 0.0010\n",
      "\n",
      "Epoch 70: LearningRateScheduler setting learning rate to 0.0010000000000000002.\n",
      "Epoch 70/100\n",
      "1236/1250 [============================>.] - ETA: 0s - loss: 6.5348 - accuracy: 0.1717\n",
      "Epoch 70: val_accuracy did not improve from 0.30590\n",
      "1250/1250 [==============================] - 3s 3ms/step - loss: 6.5350 - accuracy: 0.1718 - val_loss: 6.5507 - val_accuracy: 0.1865 - lr: 0.0010\n",
      "\n",
      "Epoch 71: LearningRateScheduler setting learning rate to 0.0010000000000000002.\n",
      "Epoch 71/100\n",
      "1241/1250 [============================>.] - ETA: 0s - loss: 6.5644 - accuracy: 0.1717\n",
      "Epoch 71: val_accuracy did not improve from 0.30590\n",
      "1250/1250 [==============================] - 3s 3ms/step - loss: 6.5644 - accuracy: 0.1719 - val_loss: 6.5791 - val_accuracy: 0.1844 - lr: 0.0010\n",
      "\n",
      "Epoch 72: LearningRateScheduler setting learning rate to 0.0010000000000000002.\n",
      "Epoch 72/100\n",
      "1235/1250 [============================>.] - ETA: 0s - loss: 6.5909 - accuracy: 0.1713\n",
      "Epoch 72: val_accuracy did not improve from 0.30590\n",
      "1250/1250 [==============================] - 3s 3ms/step - loss: 6.5911 - accuracy: 0.1713 - val_loss: 6.6041 - val_accuracy: 0.1843 - lr: 0.0010\n",
      "\n",
      "Epoch 73: LearningRateScheduler setting learning rate to 0.0010000000000000002.\n",
      "Epoch 73/100\n",
      "1247/1250 [============================>.] - ETA: 0s - loss: 6.6151 - accuracy: 0.1688\n",
      "Epoch 73: val_accuracy did not improve from 0.30590\n",
      "1250/1250 [==============================] - 3s 3ms/step - loss: 6.6152 - accuracy: 0.1687 - val_loss: 6.6263 - val_accuracy: 0.1821 - lr: 0.0010\n",
      "\n",
      "Epoch 74: LearningRateScheduler setting learning rate to 0.0010000000000000002.\n",
      "Epoch 74/100\n",
      "1234/1250 [============================>.] - ETA: 0s - loss: 6.6354 - accuracy: 0.1679\n",
      "Epoch 74: val_accuracy did not improve from 0.30590\n",
      "1250/1250 [==============================] - 3s 3ms/step - loss: 6.6356 - accuracy: 0.1678 - val_loss: 6.6458 - val_accuracy: 0.1815 - lr: 0.0010\n",
      "\n",
      "Epoch 75: LearningRateScheduler setting learning rate to 0.0010000000000000002.\n",
      "Epoch 75/100\n",
      "1244/1250 [============================>.] - ETA: 0s - loss: 6.6544 - accuracy: 0.1653\n",
      "Epoch 75: val_accuracy did not improve from 0.30590\n",
      "1250/1250 [==============================] - 3s 3ms/step - loss: 6.6544 - accuracy: 0.1651 - val_loss: 6.6631 - val_accuracy: 0.1808 - lr: 0.0010\n",
      "\n",
      "Epoch 76: LearningRateScheduler setting learning rate to 0.0010000000000000002.\n",
      "Epoch 76/100\n",
      "1231/1250 [============================>.] - ETA: 0s - loss: 6.6704 - accuracy: 0.1658\n",
      "Epoch 76: val_accuracy did not improve from 0.30590\n",
      "1250/1250 [==============================] - 3s 3ms/step - loss: 6.6707 - accuracy: 0.1656 - val_loss: 6.6783 - val_accuracy: 0.1789 - lr: 0.0010\n",
      "\n",
      "Epoch 77: LearningRateScheduler setting learning rate to 0.0010000000000000002.\n",
      "Epoch 77/100\n",
      "1245/1250 [============================>.] - ETA: 0s - loss: 6.6843 - accuracy: 0.1630\n",
      "Epoch 77: val_accuracy did not improve from 0.30590\n",
      "1250/1250 [==============================] - 3s 3ms/step - loss: 6.6843 - accuracy: 0.1630 - val_loss: 6.6917 - val_accuracy: 0.1776 - lr: 0.0010\n",
      "\n",
      "Epoch 78: LearningRateScheduler setting learning rate to 0.0010000000000000002.\n",
      "Epoch 78/100\n",
      "1236/1250 [============================>.] - ETA: 0s - loss: 6.6968 - accuracy: 0.1637\n",
      "Epoch 78: val_accuracy did not improve from 0.30590\n",
      "1250/1250 [==============================] - 3s 3ms/step - loss: 6.6969 - accuracy: 0.1632 - val_loss: 6.7035 - val_accuracy: 0.1739 - lr: 0.0010\n",
      "\n",
      "Epoch 79: LearningRateScheduler setting learning rate to 0.0010000000000000002.\n",
      "Epoch 79/100\n",
      "1246/1250 [============================>.] - ETA: 0s - loss: 6.7087 - accuracy: 0.1602\n",
      "Epoch 79: val_accuracy did not improve from 0.30590\n",
      "1250/1250 [==============================] - 3s 3ms/step - loss: 6.7087 - accuracy: 0.1603 - val_loss: 6.7139 - val_accuracy: 0.1732 - lr: 0.0010\n",
      "\n",
      "Epoch 80: LearningRateScheduler setting learning rate to 0.0010000000000000002.\n",
      "Epoch 80/100\n",
      "1250/1250 [==============================] - ETA: 0s - loss: 6.7183 - accuracy: 0.1586\n",
      "Epoch 80: val_accuracy did not improve from 0.30590\n",
      "1250/1250 [==============================] - 3s 3ms/step - loss: 6.7183 - accuracy: 0.1586 - val_loss: 6.7231 - val_accuracy: 0.1716 - lr: 0.0010\n",
      "\n",
      "Epoch 81: LearningRateScheduler setting learning rate to 0.0010000000000000002.\n",
      "Epoch 81/100\n",
      "1239/1250 [============================>.] - ETA: 0s - loss: 6.7269 - accuracy: 0.1573\n",
      "Epoch 81: val_accuracy did not improve from 0.30590\n",
      "1250/1250 [==============================] - 3s 3ms/step - loss: 6.7270 - accuracy: 0.1570 - val_loss: 6.7312 - val_accuracy: 0.1701 - lr: 0.0010\n",
      "\n",
      "Epoch 82: LearningRateScheduler setting learning rate to 0.0010000000000000002.\n",
      "Epoch 82/100\n",
      "1244/1250 [============================>.] - ETA: 0s - loss: 6.7346 - accuracy: 0.1547Restoring model weights from the end of the best epoch: 32.\n",
      "\n",
      "Epoch 82: val_accuracy did not improve from 0.30590\n",
      "1250/1250 [==============================] - 3s 3ms/step - loss: 6.7346 - accuracy: 0.1546 - val_loss: 6.7384 - val_accuracy: 0.1676 - lr: 0.0010\n",
      "Epoch 82: early stopping\n"
     ]
    }
   ],
   "source": [
    "history1 = model.fit(x_train_embedding,\n",
    "    y_train1,\n",
    "    validation_data=(x_val_embedding, y_val1),\n",
    "    batch_size=32,\n",
    "    epochs=100,verbose=1,callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lJi9ciufazdQ",
    "outputId": "1722e1bb-a89a-4e8f-ddf6-c4d208ea3f19"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 2ms/step - loss: 4.4156 - accuracy: 0.3059\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[4.415596008300781, 0.3059000074863434]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_val_embedding,y_val1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0V6TFyPCUZuX"
   },
   "outputs": [],
   "source": [
    "\n",
    "def display_learning_curves(history):\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "    ax1.plot(history.history[\"loss\"])\n",
    "    ax1.plot(history.history[\"val_loss\"])\n",
    "    ax1.legend([\"train\", \"valid\"], loc=\"upper right\")\n",
    "    ax1.set_xlabel(\"Epochs\")\n",
    "    ax1.set_ylabel(\"Loss\")\n",
    "\n",
    "    ax2.plot(history.history[\"accuracy\"])\n",
    "    ax2.plot(history.history[\"val_accuracy\"])\n",
    "    ax2.legend([\"train\", \"valid\"], loc=\"upper right\")\n",
    "    ax2.set_xlabel(\"Epochs\")\n",
    "    ax2.set_ylabel(\"Accuracy\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 607
    },
    "id": "SA56eXKlUlkE",
    "outputId": "88eb7dc8-b98f-4da8-8959-7da515a48746"
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-77-3a82c900e8e8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdisplay_learning_curves\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-76-825201235042>\u001b[0m in \u001b[0;36mdisplay_learning_curves\u001b[0;34m(history)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0max1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_ylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loss\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0max2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"accuracy\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0max2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"val_accuracy\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0max2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"valid\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"upper right\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'accuracy'"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA4AAAAFBCAYAAAAmH8FvAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOzdeZhcZ3Xg/++pqt7Vi5bW2pIlY9mW90U4MATGwUBsCDZMIBgyM4EfwZMM/AgBZh7DzM9JmPxmkiETEoOB8QDDEhzimABOMDGbwYHYHku2vArbsixbu1pLb1Lv/c4ftyS15ZbcslRdrb7fz/PUo7r3vnX73OqWqo/Ofc8bKSUkSZIkSTNfodoBSJIkSZKmhgmgJEmSJOWECaAkSZIk5YQJoCRJkiTlhAmgJEmSJOWECaAkSZIk5YQJoCRJUywivhQRuyLi0aMcj4i4MSI2RMTDEXHJVMcoSZqZTAAlSZp6XwauPMbxq4CV5cd1wOemICZJUg6YAEqSNMVSSncDe48x5BrgqylzL9AWEYumJjpJ0kxmAihJ0vSzBNg8bntLeZ8kSSekVO0Ajte8efPS8uXLqx2GJGkKrF27dndKqb3acUxnEXEd2W2iNDU1XXr22WdXOSJJUqWdyOfjKZcALl++nDVr1lQ7DEnSFIiIZ6sdQ5VsBZaO2+4o73uBlNLNwM0Aq1evTn5GStLMdyKfj94CKknS9HM78G/L3UBfAXSnlLZXOyhJ0qnvlKsASpJ0qouIvwYuB+ZFxBbgD4AagJTS54E7gDcCG4ADwHuqE6kkaaYxAZQkaYqllN75IscT8P4pCkeSlCMVTwAjogisAbamlH7tiGN1wFeBS4E9wDtSSpsqHZMknQqGh4fZsmULAwMD1Q6l4urr6+no6KCmpqbaoUiSNKNNRQXw94D1QMsEx94L7EspnRER1wJ/CrxjCmKSpGlvy5YtNDc3s3z5ciKi2uFUTEqJPXv2sGXLFlasWFHtcCRJmtEq2gQmIjqANwFfOMqQa4CvlJ/fBlwRM/m3HEk6DgMDA8ydO3dGJ38AEcHcuXNzUemUJKnaKt0F9C+A/wiMHeX4oYVuU0ojQDcwt8IxSdIpY6Ynfwfl5TolSaq2iiWAEfFrwK6U0tqTcK7rImJNRKzp7Ow8CdFJkl5MV1cXn/3sZ4/7dW984xvp6uqqQESSJOlEVbIC+Crg6ojYBHwDeG1E/NURYw4tdBsRJaCVrBnM86SUbk4prU4prW5vf0kL3kuSjtPREsCRkZFjvu6OO+6gra2tUmFJkqQTULEEMKX0sZRSR0ppOXAt8OOU0r8+YtjtwG+Vn7+tPCZVKiaAHd0D3HLfc+zqca6JJB3L9ddfz9NPP81FF13Ey1/+cl796ldz9dVXc8455wDwlre8hUsvvZRzzz2Xm2+++dDrli9fzu7du9m0aROrVq3ife97H+eeey5veMMb6O/vr9blSJIkKj8H8AUi4hMRcXV584vA3IjYAHwYuL7SX39jZx8f/9YjPLN7f6W/lCSd0v7kT/6El73sZaxbt45PfvKTPPDAA/zlX/4lTz75JABf+tKXWLt2LWvWrOHGG29kz54X3MDBU089xfvf/34ee+wx2tra+OY3vznVlyFJksaZkoXgU0o/AX5Sfn7DuP0DwNunIoaDioWs0cDoWEULjZJ0Uv3R3z/G49t6Tuo5z1ncwh+8+dxJj7/sssuet0zDjTfeyLe+9S0ANm/ezFNPPcXcuc/v47VixQouuugiAC699FI2bdp04oFLkqSXbEoSwOmkVMwSwBETQEk6Lk1NTYee/+QnP+GHP/wh99xzD42NjVx++eUTLuNQV1d36HmxWPQWUEmSqix3CWCxkN31agVQ0qnkeCp1J0tzczO9vb0THuvu7mb27Nk0Njbyi1/8gnvvvXeKo5MkSS9F7hLAUsEKoCRNxty5c3nVq17FeeedR0NDAwsWLDh07Morr+Tzn/88q1at4qyzzuIVr3hFFSOVJEmTlbsE8PAcwKOtTS9JOuiWW26ZcH9dXR3f+973Jjx2cJ7fvHnzePTRRw/t/+hHP3rS45MkScdnyruAVpsVQEmSJEl5lbsE0C6gkiRJkvIqdwlgqdwEZnjUBFCSJElSvuQuASwWnQMoSZIkKZ9ylwA6B1CSJElSXuUuAXQOoCRJkqS8yl0CWFOeAzjiHEBJOqlmzZoFwLZt23jb29424ZjLL7+cNWvWTGVYkiRpnNwlgIfnAJoASlIlLF68mNtuu63aYUiSpAnkLgF0DqAkTc7111/PTTfddGj7D//wD/njP/5jrrjiCi655BLOP/98vvOd77zgdZs2beK8884DoL+/n2uvvZZVq1bx1re+lf7+/imLX5IkvVCp2gFMtcNzAO0CKknH8o53vIMPfehDvP/97wfg1ltv5c477+SDH/wgLS0t7N69m1e84hVcffXVRMSE5/jc5z5HY2Mj69ev5+GHH+aSSy6ZykuQJElHyF8CGFYAJZ2Cvnc97Hjk5J5z4flw1Z8c9fDFF1/Mrl272LZtG52dncyePZuFCxfy+7//+9x9990UCgW2bt3Kzp07Wbhw4YTnuPvuu/ngBz8IwAUXXMAFF1xwcq9BkiQdl9wlgIVCUAjnAErSZLz97W/ntttuY8eOHbzjHe/g61//Op2dnaxdu5aamhqWL1/OwMBAtcOUJEmTlLsEEKBUKFgBlHRqOUalrpLe8Y538L73vY/du3fz05/+lFtvvZX58+dTU1PDXXfdxbPPPnvM17/mNa/hlltu4bWvfS2PPvooDz/88BRFLkmSJpLLBLBYCCuAkjQJ5557Lr29vSxZsoRFixbxm7/5m7z5zW/m/PPPZ/Xq1Zx99tnHfP3v/u7v8p73vIdVq1axatUqLr300imKXJIkTSSXCWCpEK4DKEmT9Mgjh+cezps3j3vuuWfCcX19fQAsX76cRx99FICGhga+8Y1vVD5ISZI0KblbBgKytQDtAipJkiQpb3KZAJYK4RxASZIkSbmTywTQOYCSJEmS8iiXCaBdQCWdKlLKx79VeblOSZKqLZcJoBVASaeC+vp69uzZM+OTo5QSe/bsob6+vtqhSJI041WsC2hE1AN3A3Xlr3NbSukPjhjzbuCTwNbyrs+klL5QqZgOcg6gpFNBR0cHW7ZsobOzs9qhVFx9fT0dHR3VDkOSpBmvkstADAKvTSn1RUQN8LOI+F5K6d4jxv1NSukDFYzjBbIKoF1AJU1vNTU1rFixotphSJKkGaRiCWDK7lnqK2/WlB/TouxWdB1ASZIkSTlU0TmAEVGMiHXALuAHKaX7Jhj26xHxcETcFhFLKxnPQaWicwAlSZIk5U9FE8CU0mhK6SKgA7gsIs47YsjfA8tTShcAPwC+MtF5IuK6iFgTEWtOxlyYol1AJUmSJOXQlHQBTSl1AXcBVx6xf09KabC8+QXg0qO8/uaU0uqU0ur29vYTjqdkF1BJkiRJOVSxBDAi2iOirfy8AXg98Isjxiwat3k1sL5S8YxXLAQjNoGRJEmSlDOV7AK6CPhKRBTJEs1bU0r/EBGfANaklG4HPhgRVwMjwF7g3RWM55BSIRgaMQGUJEmSlC+V7AL6MHDxBPtvGPf8Y8DHKhXD0RRdB1CSJElSDk3JHMDpxjmAkiRJkvIonwlg0S6gkiRJkvInnwlgIRi1CYwkSZKknMllAugcQEmSJEl5lMsE0DmAkqRqi4grI+KJiNgQEddPcHxZRNwVEQ9GxMMR8cZqxClJmllymQAWCwVGRk0AJUnVUV4i6SbgKuAc4J0Rcc4Rw/4z2RJKFwPXAp+d2iglSTNRLhNAK4CSpCq7DNiQUtqYUhoCvgFcc8SYBLSUn7cC26YwPknSDJXLBLBYdA6gJKmqlgCbx21vKe8b7w+Bfx0RW4A7gP93ohNFxHURsSYi1nR2dlYiVknSDJLLBNAuoJKkU8A7gS+nlDqANwJfi4gXfG6nlG5OKa1OKa1ub2+f8iAlSaeWXCaAdgGVJFXZVmDpuO2O8r7x3gvcCpBSugeoB+ZNSXSSpBkrlwmgcwAlSVV2P7AyIlZERC1Zk5fbjxjzHHAFQESsIksAvcdTknRCcpkAFgsFK4CSpKpJKY0AHwDuBNaTdft8LCI+ERFXl4d9BHhfRDwE/DXw7pSSH16SpBNSqnYA1WAFUJJUbSmlO8iau4zfd8O4548Dr5rquCRJM1tOK4BZAuh/pEqSJEnKk1wmgKVCAFgFlCRJkpQruUwAi8UsAXQeoCRJkqQ8yWUCaAVQkiRJUh7lMgEsFrLLtgIoSZIkKU9ymQBaAZQkSZKUR7lMAIuFg3MAx6ociSRJkiRNnVwmgFYAJUmSJOVRLhPAQxXAURNASZIkSfmRywSw5DIQkiRJknIolwngwS6go84BlCRJkpQjFUsAI6I+Iv5PRDwUEY9FxB9NMKYuIv4mIjZExH0RsbxS8YxXKlgBlCRJkpQ/lawADgKvTSldCFwEXBkRrzhizHuBfSmlM4BPAX9awXgOKTkHUJIkSVIOVSwBTJm+8mZN+XFkxnUN8JXy89uAKyIiKhXTQQfnANoFVJIkSVKeVHQOYEQUI2IdsAv4QUrpviOGLAE2A6SURoBuYO4E57kuItZExJrOzs4TjuvgHEBvAZUkSZKUJxVNAFNKoymli4AO4LKIOO8lnufmlNLqlNLq9vb2E47LdQAlSZIk5dGUdAFNKXUBdwFXHnFoK7AUICJKQCuwp9LxHFoH0C6gkiRJknKkkl1A2yOirfy8AXg98Isjht0O/Fb5+duAH6eUKl6WswIoSZIkKY9KFTz3IuArEVEkSzRvTSn9Q0R8AliTUrod+CLwtYjYAOwFrq1gPIcUXQZCkiRJUg5VLAFMKT0MXDzB/hvGPR8A3l6pGI6mdHAheJeBkCRJkpQjUzIHcLqxAihJkiQpj3KZALoOoCRJkqQ8ymUCaBdQSZIkSXmUywTQLqCSJEmS8iiXCaBzACVJkiTlUS4TwENdQE0AJUmSJOVILhNAK4CSJEmS8iiXCeChOYCjNoGRJEmSlB+5TACLRSuAkiRJkvInlwmgXUAlSZIk5VEuE0DnAEqSJEnKo1wmgHYBlSRJkpRHuUwAywVAK4CSJEmSciWXCWBEUCoEI3YBlSRJkpQjuUwAIZsH6C2gkiRJkvIktwlgqRDeAipJkiQpV3KbAFoBlCRJkpQ3uU0Aa4oFRsacAyhJkiQpP3KbAFoBlCRJkpQ3uU0Asy6gJoCSJEmS8iO3CWCxaAVQkiRJUr7kNgEsFQp2AZUkSZKUK7lNAJ0DKEmSJClvKpYARsTSiLgrIh6PiMci4vcmGHN5RHRHxLry44ZKxXOkbB1Au4BKkiRJyo9SBc89AnwkpfRARDQDayPiBymlx48Y908ppV+rYBwTsgIoSZIkKW8qVgFMKW1PKT1Qft4LrAeWVOrrHa+sAmgCKEmaehFxZUQ8EREbIuL6o4z5jXF30dwy1TFKkmamSlYAD4mI5cDFwH0THH5lRDwEbAM+mlJ6bCpisgIoSaqGiCgCNwGvB7YA90fE7ePvkImIlcDHgFellPZFxPzqRCtJmmkq3gQmImYB3wQ+lFLqOeLwA8BpKaULgU8D3z7KOa6LiDURsaazs/OkxFUqFFwHUJJUDZcBG1JKG1NKQ8A3gGuOGPM+4KaU0j6AlNKuKY5RkjRDVTQBjIgasuTv6ymlvzvyeEqpJ6XUV35+B1ATEfMmGHdzSml1Sml1e3v7SYnNCqAkqUqWAJvHbW/hhVMkzgTOjIifR8S9EXHllEUnSZrRKnYLaEQE8EVgfUrpz48yZiGwM6WUIuIysoR0T6ViGq9UDAZHRqfiS0mSdLxKwErgcqADuDsizk8pdR05MCKuA64DWLZs2VTGKEk6BVVyDuCrgH8DPBIR68r7Pg4sA0gpfR54G/C7ETEC9APXppSmpCxnBVCSVCVbgaXjtjvK+8bbAtyXUhoGnomIJ8kSwvuPPFlK6WbgZoDVq1f7wSZJOqaKJYAppZ8B8SJjPgN8plIxHItdQCVJVXI/sDIiVpAlftcC7zpizLeBdwL/uzw14kxg45RGKUmakSreBGa6sgIoSaqGlNII8AHgTrIlkm5NKT0WEZ+IiKvLw+4E9kTE48BdwH9IKU3JFAlJ0sw2JctATEelQsEKoCSpKsqNz+44Yt8N454n4MPlhyRJJ40VQEmSJEnKidwmgNkcwLFqhyFJkiRJUya3CWCxEIy6ELwkSZKkHMltAlgq2gVUkiRJUr7kNgEsugyEJEmSpJzJbQJYKhQYGXUOoCRJkqT8yG0CaBdQSZIkSXmT2wTQOYCSJEmS8ia/CaAVQEmSJEk5k9sEsFgoMDKWSMkkUJIkSVI+5DYBLBUCAIuAkiRJkvIitwlgsZwAjozZCVSSJElSPuQ2ATxYAXQeoCRJkqS8mFQCGBFNEVEoPz8zIq6OiJrKhlZZhyuAJoCSJEmS8mGyFcC7gfqIWAJ8H/g3wJcrFdRUOFQBHDUBlCRJkpQPk00AI6V0APhXwGdTSm8Hzq1cWJVXLGaXbgVQkiRJUl5MOgGMiFcCvwl8t7yvWJmQpoZzACVJkiTlzWQTwA8BHwO+lVJ6LCJOB+6qXFiVZxdQSZIkSXlTmsyglNJPgZ8ClJvB7E4pfbCSgVWaFUBJkiRJeTPZLqC3RERLRDQBjwKPR8R/qGxolWUXUEmSJEl5M9lbQM9JKfUAbwG+B6wg6wR6yioVsku3AihJkiQpLyabANaU1/17C3B7SmkYOKUzp0MVQJeBkCRJkpQTk00A/yewCWgC7o6I04CeY70gIpZGxF0R8XhEPBYRvzfBmIiIGyNiQ0Q8HBGXHO8FvFTOAZQkSZKUN5NtAnMjcOO4Xc9GxK+8yMtGgI+klB6IiGZgbUT8IKX0+LgxVwEry49fAj5X/rPiikW7gEqSJEnKl8k2gWmNiD+PiDXlx/8gqwYeVUppe0rpgfLzXmA9sOSIYdcAX02Ze4G2iFh0/Jdx/KwASpIkScqbyd4C+iWgF/iN8qMH+N+T/SIRsRy4GLjviENLgM3jtrfwwiSRiLjuYPLZ2dk52S97THYBlSRJkpQ3k7oFFHhZSunXx23/UUSsm8wLI2IW8E3gQ+VOosctpXQzcDPA6tWrT0rGZhdQSZIkSXkz2Qpgf0T88sGNiHgV0P9iLyp3Dv0m8PWU0t9NMGQrsHTcdkd5X8VZAZQkSZKUN5OtAP4O8NWIaC1v7wN+61gviIgAvgisTyn9+VGG3Q58ICK+Qdb8pTultH2SMZ2Q0qFlIGwCI0mSJCkfJtsF9CHgwohoKW/3RMSHgIeP8bJXkS0W/8i420U/Diwrn+PzwB3AG4ENwAHgPS/lIl4KK4CSJEmS8mayFUAgS/zGbX4Y+ItjjP0ZEC9yvgS8/3hiOFlKRbuASpIkScqXyc4BnMgxk7vp7mATGCuAkiRJkvLiRBLAUzpzOrwOoHMAJUmSJOXDMW8BjYheJk70AmioSERT5NAcwNFTOo+VJEmSpEk7ZgKYUmqeqkCmmnMAJUmSJOXNidwCekqzC6gkSZKkvMltAniwCYwVQEmSJEl5kdsE0AqgJEmSpLzJbQJoF1BJkiRJeZPbBNAKoCRJkqS8yW0CeKgC6DIQkiRJknIitwmgFUBJkiRJeZPbBDAiKBbCLqCSJEmSciO3CSBkVUArgJIkSZLyItcJYKkQdgGVJEmSlBu5TgCtAEqSqiUiroyIJyJiQ0Rcf4xxvx4RKSJWT2V8kqSZKdcJYMk5gJKkKoiIInATcBVwDvDOiDhngnHNwO8B901thJKkmSrXCWCxULACKEmqhsuADSmljSmlIeAbwDUTjPsvwJ8CA1MZnCRp5sp1AlgqhOsASpKqYQmwedz2lvK+QyLiEmBpSum7UxmYJGlmy3UC6BxASdJ0FBEF4M+Bj0xi7HURsSYi1nR2dlY+OEnSKS3XCWCpaBdQSVJVbAWWjtvuKO87qBk4D/hJRGwCXgHcPlEjmJTSzSml1Sml1e3t7RUMWZI0E+Q6AbQCKEmqkvuBlRGxIiJqgWuB2w8eTCl1p5TmpZSWp5SWA/cCV6eU1lQnXEnSTJHrBLBUCEacAyhJmmIppRHgA8CdwHrg1pTSYxHxiYi4urrRSZJmslK1A6gmu4BKkqolpXQHcMcR+244ytjLpyImSdLMV7EKYER8KSJ2RcSjRzl+eUR0R8S68mPCD71KytYBdA6gJEmSpHyoZAXwy8BngK8eY8w/pZR+rYIxHJNzACVJkiTlScUqgCmlu4G9lTr/yVBTDEZNACVJkiTlRLWbwLwyIh6KiO9FxLlT/cWtAEqSJEnKk2omgA8Ap6WULgQ+DXz7aAMrtchtqVCwAihJkiQpN6qWAKaUelJKfeXndwA1ETHvKGMrssitFUBJkiRJeVK1BDAiFkZElJ9fVo5lz1TGYBdQSZIkSXlSsS6gEfHXwOXAvIjYAvwBUAOQUvo88DbgdyNiBOgHrk0pTWk5ruhC8JIkSZJypGIJYErpnS9y/DNky0RUTckuoJIkSZJypNpdQKuqaBMYSZIkSTmS6wSwZBMYSZIkSTmS6wSwWPAWUEmSJEn5kesEMKsA2gVUkiRJUj7kOgG0AihJkiQpT3KdADoHUJIkSVKe5DoBLBYKjLoOoCRJkqScyHUCWCpaAZQkSZKUH7lOAJ0DKEmSJClPcp0A2gVUkiRJUp7kOgEsFoKxBGNWASVJkiTlQK4TwFIhABhNJoCSJEmSZr5cJ4DFQnb5zgOUJEmSlAe5TgAPVgDtBCpJkiQpD3KdABYPJoCjNoKRJEmSNPPlOgEsFa0ASpIkScqPXCeAByuAzgGUJEmSlAe5TgBryk1grABKkiRJyoNcJ4CHKoCjJoCSJEmSZr5cJ4CH5wDaBEaSJEnSzJfrBNA5gJIkSZLyJNcJoOsASpIkScqTXCeAxXITGCuAkiRJkvIg1wmgFUBJkiRJeVKxBDAivhQRuyLi0aMcj4i4MSI2RMTDEXFJpWI5msNzAG0CI0mSJGnmq2QF8MvAlcc4fhWwsvy4DvhcBWOZ0KEKoMtASJIkScqBUqVOnFK6OyKWH2PINcBXU0oJuDci2iJiUUppe6ViAmCgB3Y/CcVamnr6WRY76d62gW3RWZEvl46RW0ZAoRAUCgVaGmqoL5Xz8ebFUHwJ35qxUUhjUKw5Yv8YdG8GsmBG6+dQbGg5/vNLkiRJOqVVLAGchCXA5nHbW8r7KpsAbnsQvno1ABcCd9cBP6zoVzxuW1e8jSW/9cXjf+H3/z/Yuhbee+fz9//kv8Ldnzy0uSfN5r+e/U3e/vLTeOXpcymUK6GSJEmSZrZqJoCTFhHXkd0myrJly07sZAvOg3f9LYwOMjY8yKObdzM4PHLMlxxZxYuTlC+llCCNMTqW6BkYobNviF/a8deMbn7spZ1w5yOw7QEYG+PbD23noS1d/MGbz4XtD0HbaXD59fzsJ//IL3d9h2eeWMdvPrSD1afN5m9/55XEybooSZIkSdNWNRPArcDScdsd5X0vkFK6GbgZYPXq1Sc2Ya9pLpz5BiCbAHnBBSd0tpPu0U8/ROvutQyOjFJXKh7fi3t3wugQ9G7j1jVbeOC5fdzwa+cQ+zbBogvgonfxzZ8V+GW+w9++uY4/2baCL/38GTp7B5nfUl+R65EkSZI0fVRzGYjbgX9b7gb6CqC74vP/TgENc5cwn338YlvP8b+4bwcAae8zPL69h4HhMfb2DcC+Z2H2CgDW9s1loNBI7Y4H+ZWz2wHYuHv/SYtfkiRJ0vRVyWUg/hq4BzgrIrZExHsj4nci4nfKQ+4ANgIbgP8F/PtKxXIqmbtwGXUxwvpnnju+Fw73w0A3AF3bnqTrwDAAu7Y9C6ODMHs5Y2OJ7b3D7Jy1CrasYcW8JgA2dpoASpIkSXlQyS6g73yR4wl4f6W+/qmqtT27K/bZZzeStal5obGxROLwOoYA9O089HTflqeAhQD0bHsy2zlnBbv7BhkeTfTOvRCe/SqLm4K6UoFndvdV4EokSZIkTTfVvAVUE4jmLHHbvf3ZFx4c7IWn7+IfP/thPv25Tz//WO/hBHC48+nDLzn4fPZytnUPADCy6BIYG6Gw81FWzGuyAihJkiTlxCnRBTRXygngaPd2uvuHaW0or+n3gxvgnz8NaYw3AmemJewf/Pc01ZW/heX5fzTOo7b3WU6f18T27oGsAUwUoXUp27fuBqB++WXwz8DWNZzefhm/2N47pZcoSZIkqTqsAE435QRwfnTxyJbuw/t/cQcsPJ8nXv9lbhn5FRazmzWb9h4+frACuOwVzB7cxjmLW1jcVk9977PQ2gHFmkMVwPlLVmSLzW9dy4p5TTy39wDDo2NTdIGSJEmSqsUEcLqpbSLVzmJB7OOhLV3ZvrEx6HoOVryG7x44lw100BiDrHvi8K2e9O2AKDIw/2La6OXi+QWWzG6kZWArzMk6gG7v6qeuVGB2Yw0suQS2ruX0ebMYGUts3nugChcrSZIkaSqZAE5D0byIFfV9PPhcOQHs25l18mw7jZ9v2E1xznIAntv4i8Mv6t0Js+bzbLn5y0WzuljSVs+Cke0wOxu/vWeAxW0N2aLvHath70bOaB4C7AQqSVMpIq6MiCciYkNEXD/B8Q9HxOMR8XBE/CgiTqtGnJKkmccEcDpqXsiymh7Wbe4ipQRdWUOYA00drNvcxZLlZwIw0LmJ3oFsuQf6dsKsBTw+OBeAM2p2s7xplDZ6GW5dDmQVwEWt5QXfl1wKwMuGngDgGdcClKQpERFF4CbgKuAc4J0Rcc4Rwx4EVqeULgBuA/771EYpSZqpTACno+aFtLOP3X2DbO8egH2bAHiwt5XRscQ5q84HYDG7uP/gPMC+HdC8kP/T1QJAa/8WXlaTNX3ZW7MIgO3dAyxqbcjGL74YCGbtfpg5TbVsdCkISZoqlwEbUkobU0pDwDeAa8YPSCndlVI6eG/+vUDHFMcoSZqhTACno1kLaBraDSQe2twF+7IK4F076mmoKXLhymWkuhaWFfdwz9N7stf0ZhXAB3eO0lNohX3PsCyyxjDbCosYGR1jZ88Ai9vKFcC6Zmg/G7aucSkISZpaS4DN47a3lPcdzRJ4odMAAB/VSURBVHuB7x3tYERcFxFrImJNZ2fnSQpRkjRTmQBOR80LKYwOMLc4wLrNXdktoM2L+MnGXi5bMYe6UpFoW8Y5DV3cs3EPjI7A/k5GmuazYVcffY0dsG8T80e2A7BptJ1dvYOMJQ5XAAE6Li03gmlio7eAStK0ExH/GlgNfPJoY1JKN6eUVqeUVre3t09dcJKkU5IJ4HTUnN2y+S8XjfL1+56ja9tTDDUvZcOuPn75jHnZmLZlnFbczWPbeujevQ1I7BxrY2QskdqWw95naD6wmb1pFs/sL7G9ux+ARQcrgJBVAA/s4azZY3T2Dh6eTyhJqqStwNJx2x3lfc8TEa8D/hNwdUppcIpikyTNcCaA09GsBQB8/F/OoWN2Awd2Ps3a7mxu3y+vPJwAzh7eSUqJx596EoBnBpoBaFp4BnRvobj3abYXFrKtq59tXdkagIvHVwBbFgNwdkM2/89GMJI0Je4HVkbEioioBa4Fbh8/ICIuBv4nWfK3qwoxSpJmKBPA6ahcAZw3tpfbrlvNwtjL/+luYd6sWs5akCV5tC6lONzHgpp+1j66HoC/fXKYptoiLYtXQhqFLfezt3YJ27r6D1UAF7aOqwC2ZFNOltdmy02YAEpS5aWURoAPAHcC64FbU0qPRcQnIuLq8rBPArOAv42IdRFx+1FOJ0nScSlVOwBNoDmrANK3g1n924HEqlXn8+EzzqJQiOxY2zIA3rBkiC2bN0ENbB1u4XcvfxmFObOyMaODHGjtYGu5AthUW6Slfty3vFwBXMAeItp52kYwkjQlUkp3AHccse+Gcc9fN+VBSZJywQRwOqprhpom6N1xaA3AN7zqMli+7PCYtmz6yO+/vJ7e+bPgYbjto2+BUi10Nx4aNtK6nO1PDrCtq59FBxeBP6h5ERDU9G2nY/YyK4CSJEnSDOctoNNV88IsASyvAUjbac8/Xt6eM7SD02p7oHFulvxBltgV6wAozTudodExHt3afXgR+IOKNdl8w56tnD5vFhs7XQtQkiRJmslMAKer5oXQtzNbA7BQc+h2zUMaZkPtLOjaXF4DcOHhY4UCzM4SxMYFZwCwrXvg+Q1gDmpZDD3bWDGviWd27yelVKkrkiRJklRlJoDTVfNC6N2e3QLathQKxecfj4DWpdD1HPTtODxv8KDZK6BYy9xFyw/tet4SEAeVE8CzFzZzYGiUTXsOnPxrkSRJkjQtmABOV7MWZpW9fc++8PbPg9qWZQngkRVAgHPfCpf8W5bMbjq0a8IKYGsH9GzlwqVtAKzbvO9kXYEkSZKkacYEcLpqXgDD+6HziUO3c77AwQSwb+cLK4AXvRPe9D9oaSgxqy7r9XPUCuBgD2e2JRprizy0ufvEY9+/+8TPIUmSJOmkMwGcrsprATK8/xgVwKUw2A1jwy+sAJZFBIvLid+iCecAZmsBFvt2cN6SVh7c3HVicT93L3zyDNj5+ImdR5IkSdJJZwI4Xc0aV9GbvXziMW3jloU4sgI4zuK2hvKfR6kAAnRv4eKlbazf1sPgyOhxhfrIlm5+9VN3892Ht8Nz9wAJdpkASpIkSdONCeB0dbACCEe/BbR1XAJ4lAogwBnts1jYUk9j7QTLPh5MAHu2cdHSNoZGx1i/vfe4Qv3LHz3JEzt7ef8tD/Dwmn/KdpbXL5QkSZI0fbgQ/HQ1vqLXtnziMZOsAH7o9Wfy7lcd5RzNhxPACy8qN4J5bh8XlZvCvJindvbyw/W7+MCvnMHQ6BiN966HAgzteZbaSZ1BkiRJ0lSpaAUwIq6MiCciYkNEXD/B8XdHRGdErCs/fruS8ZxS6lqg1JCt9dc4Z+IxTfOyMXDMCuCsuhIdsxsnPliqhab50LOVRa31zG+u46Etk28Ec/PdG6mvKfD//PIKPv66ZbyssB2Aru0bJ30OSZIkSVOjYhXAiCgCNwGvB7YA90fE7SmlIyeH/U1K6QOViuOUFZGtBVjblD0/2pi2pdC7A2qPkuBNRsti6NlKRHDR0jbWTbIRzI7uAb69bivvumwZc5pqYcvDBIkDqY7ofu6lxyNJkiSpIipZAbwM2JBS2phSGgK+AVxTwa8385xxBax8/bHHzF5xeB7fS9WyBHq2AXDh0jae2b2frgNDL/qy//3PzzA6lvjtV5+e7dj+EACP1K+mZWAHpHRicUmSJEk6qSqZAC4BNo/b3lLed6Rfj4iHI+K2iFhawXhOPW/6H/C6Pzz2mDf8Mbzlcyf2dVqXQM9WAC4uz/17sdtAd/UOcMu9z/GmCxazdE65+rjjEahvpXfBy6ljkMGeXScWlyRJkqSTqtpdQP8eWJ5SugD4AfCViQZFxHURsSYi1nR2dk5pgNNe+5mw5JITO0fLYhjohsE+zu9oJQLWPffC20BHxxLff2wH/+5ra3jVn/yYA8Oj/LvXnH54wI5HYOEFzOlYCcDGp9afWFySJEmSTqpKJoBbgfEVvY7yvkNSSntSSoPlzS8Al050opTSzSml1Sml1e3t7RUJNtfKi8HTs43m+hrOaJ/Fus37njdkV88A/+aL93Hd19ay9tkufuuVy7njg6/mvCWt2YCxUdj5GCw8n+Wnnw3A1k1PTOVVSJIkSXoRlVwG4n5gZUSsIEv8rgXeNX5ARCxKKW0vb14NWDKqhkNrAW6F9jO5aGkb3398J//46HZWLWph4+79fPTWhzgwNMp/+1fn8/ZLOygVj/i/gz1Pw0g/LDyfOUvOyE63w06gkiRJ0nRSsQQwpTQSER8A7gSKwJdSSo9FxCeANSml24EPRsTVwAiwF3h3peLRMYyrAAJcsWoBf/fgVn7nrx44NOSsBc185l0Xs3JB88Tn2PFw9ufC86Ghjf5CE6N7XQxekiRJmk4quhB8SukO4I4j9t0w7vnHgI9VMgZNQvOi7M9yAnjleQt59A9/lSd39rJ+ew8HhkZ51y8to76mePRz7HgECjUw7ywA+hsX09a9k+3d/Sxqbaj0FUiSJEmahIomgDpF1NRD4zzo2XJoV0NtkQuXtnFhuSsokC3rsL8TmtpfuDbhjkdg/tnZwvJAcc5pdPQ8wYPPdbHofBNASZIkaTqodhdQTRctiw9VACeUEtz5cfizlXDjRfDdj8JTP4TRkex4uQPoQbPmr6AjdvPgc/uOckJJkiRJU80KoDItS6B789GP//RP4d7PwjlvgZEBePCv4P7/BS0dcMFvwP5d2fy/suKc02iOfn6xaQtwTuXjl/QCw6NjDI2MMZoSaQwS6dCxsQRjKTGWEiSICAqR/XlQAMViUFMoEAH9Q6McGB5lcHiURPb/QhFQWyxQWypQUyxQLAQ1xaBYCGqLheedT5IkVZ8JoDKtS+DZf4bOJ2DO6VCsOXzs3s/BT/4bXPSbcPVnoFCA4QF46vtw/xfgZ3+ejRuXANKarQDSs/1phkbGqC1ZbJYma2ws0T88yv6hEfYPjrJ3/yA7ugfZ2TPArt5B9vQNsrtvkP7hUVLKErHBkVH6Bkc4MDTK/sER+odHGR5NL/7FKuifr38ti9u8BVySpOnEBFCZ9rNhsBtuugyKtVlFMI3B6DD0boNVb4Y335glf5DNGzzn6uzR+SRsXQPL/sXh87VlCeD8sV3c98weXr3S9Rul8UbHEht29bGxs49New7w7J79bNnXz5Z9B9ja1X/U5K2mGMxtqmPurFqaaktEZH8t2xpr6ZjdSGNtkaa6Eg21RRpritSWsqpcIeJ5U3cDKBbiUIUuq+glxsbSoX1jKTE6lhgeTSQSjTVFGmqL1JWKzzvX0MgYw6OJoZFRRsay14yMJVoaxv1HkiRJmhZMAJV5+W9Dx8th13roXA9dm7MqYKEmqw6++iNQPMqPS/uZ2WO8ttMAOK+pi//87Ue544OvpqnOHzfl19DIGGue3cs/PbWbB57dxyNbuzkwNHro+LxZtSyd08j5HW1ced4i5jTV0FhborG2yJymWha01LOwuZa20hAxtB8Ge2GoL7sle/hAVpUfKT+iCHXNUN8CUYDh/sNjRgdhZDD7D56Df8eLtVCqg1I9kKB/H/R3wdggNLRCfSvUNMLYCIyNZmNK9YdfU6iBQjG7H3RoPwz2ZbEVOvBjRpKk6cVPZmUiYPFF2eNkaJwLpQZ+4wz4i3UH+P/vWM9/fev5L/46aQbpHxrlh+t38t2Ht/OzDbvpGxyhVAjOXdLKb6xeykVLmljVOszSun4ah/ZAzybo3pLNx93XCQf2ZI+BnnICd6Dal3R8PvToobsBJEnS9GACqMqIgLZlLKKT9736dG6+eyOvX7WAXzl7frUjkyruqZ29fPYnT3PnYzs4MDTKguZa3r0q8fq2naziGWr3PgXPPgXrNmVVtSPNWpA9GudC2zKoa4HapqwKV9d8+FHblFXgahqyR6k+e6TRrEI40JM9r2kaN6YOinVZZXBsGEaHYGSoXBkcAAIa2qBhdlYZHOiBge4s+SyUskofZFXEkfJrxkazc6WUxVQ3C2qboXnhVL7tkiRpEkwAVTltS6HrOT78r87kp0908h+/+TBf/+1f4swFzdWOTKqI5/Yc4C9+9CTffnArK2v38sdLn+U1pceYu+te4hd7s0GFGph7BsxfBauuzpZgaZqXrcXZshhaO7IkbbqobYKWRdWOQpIknSQmgKqc1qWw9QHqa4p86h0X8Y6b7+FX/+Ju3nrxEn7/dWeydE5jtSOUTppvPbCZr//dt3ld4X7ua3uE9v6NsBVoXgRn/ios/aXsFuv550yvBE+SJOWKCaAqp20Z9O+FwT7OWdzCT//Dr/D5nz7NV/55E7ev28YrXzaX15+zgNetWmCreJ2y0ugw3731f7Fi/Re4rfQ0KYrEgn8BZ/02nPE6mHcmuBaeJEmaJkwAVTlty7I/n/o+nHUVc5oa+PgbV/HeX17Bl37+DN9/bCc3fOcxbvjOYyxqrefcxa2cu7iF85a0ct6SFha21LuItKa1kSd/QNfffpBfG95GZ30HI1f8GaUL3pbNn5MkSZqGTABVOQvOy5pG3PaebN7T4ovh9MtZcMYVfOwNq/nYVavYsKuPnzyxi0e2dvPYth5+9IudpPLyZ3Maa1jRPouO2Q10zG6gsbZEqRAUC8GcploWttSzsLWe5voaGmqL1JcKlIovXHC+d2CYYiForPXHXSfJQDfpzv9E6cGvsW9sCfee+2e86W3vIY62VIokSdI04W8rqpz5Z8NHnoTN98Hme2HTz+Gf/gzu/u9ZV8MF53HG/LM5Y96ZsLIRVsLQ0ADdmx4itj9IW++TbN5zGj/e83K+1X8hW0cPV1USUX5AI4O0xAFa2E9LzRgt9SVa64t0jdTy1P56tgw20Br7ObdmJ+fV76KdLurH9lM/tp9dzOPntf+CZ2rPprWxlgWt9SxsqWPerDraCz2s6Lmf4lAvW2qWsyGW0V9sZkFzHQtb62mrL9AwuJv6/h0w0EP3EOwbKjA2PMhCdjN3pJPGGGRk1iKGZ3WQmuYxq7ZAUw3UlYqkmkZGSk2MUaC4fzvF3m3EQFfWDGTWgqyDYutSBlMwMDxGU02BUvcm2L4O9jwNezdmSwSsfm82x8xqaeUN7Yd1t8DPPkXq2c7nR97M6Guu5wNvOK/akUmSJE1KpIPlllPE6tWr05o1a6odhl6qA3vhmZ/CM3fDzsezRecHup8/pq4lqxYuOBe2PQjP3QucvJ/ToahlsNDEYLGR2UM7KDLK3mI7T5fOYP9I0DcMp7GN8wubXvDavlRfTjuhniGKcey4hlORmhg95phjxpqKPJsWsocWzo7naIv9h47tLsyjEMGc0U542RVw5X+D9rNe8tcipWxJgKHy1yjVZcsAEEDKjg/vz75fA91Z6/9CMVt0fPxC4mPD2ULgg73ZEgQHlw4YHT782sHew4uWF+ugdQm0LMmWORjqg8GebPHyNFb+OqVsMfKGNqiddXgB80KxvDh5+ZHKcY6NZLEO7c++bl1z9nNVU1+OrbyuXqEmi7tQyraH9mfvQdP8rCNnw2zo2Zol21vuh7Vfhv597Gw5n3/X+XYueMUV/NHV53qrcgVFxNqU0upqx3Gq8DNSkvLhRD4frQBqajXOgXPfmj0g+2V9/+7sl27I1iabtQAK427l7NsFT/84W4/skJQlBylBbWOWHNS3Hl7fLCL7Rf/A7uz89S0w7yyYt5LaxjnUAs0A/fvgiX9kzvq/Z86+TTA2QhobYaxpMb1Lr2XPwlcz1tRO+4Gnaep6kqa+nQyMjLF/aIy91DJQv4D+hgWkulZa6oK2OigWS+wpzmNnmsu+oaB+cDf1B7ZR6N/L/uFE31BiYHiUegZoSP0U0xi9NfPoqp3P/phFzdA+6gd20zy0k4XDW5k/tJnTh/ewpeH1/LxhFU8VV/IMC9k9WGTjji6uGv4HPrLx72i66TIoNRxeL65QACJ7L8ZGYHQkS8hKdYfXhRsZzJKxod7sz4nWpBMAYwRr6l7Jp9OV/NOuM3jT+Yv4gzeb/EmSpFOLFUDpFDYwPMrX7nmWv/rxWq4a/iGnNQ7Q0ZRYWD9CbREKAcWAVCiRokSKIowMEMMHiJF+hqjlQDSynwb6Uj29qY6e0VoGhkcZGx5kbGSIoZERhkez2277qaOHRnpTIyMUKDJGgUQtI9TFEHUMM0yJvtTAfuoZoUiJEWoYZYgaelIjPTTSlxoYjFoKNQ00xhBzRzqZlzppYIjB4ixGappIpUZKpSLFUg0NxVGa036a2U9D6ifGRiikEcZGR+gbzqq2A6NQLBQpFQsUiiWirolC3SyKNbUUBnspDPWShvvpTQ10jzVwgFoai4mGwgh1hcRgoZ6BaGAsisxL+5jPHprHetk6Npunhufz1Oh82tsXct6SFi7saOOai5ZQW3rhnFOdXFYAj4+fkZKUD1YApZyqrynyvteczttXd3Drmku4+7ku1m3uYvuOgUmfIwKaakvMqivRXF9iVn2J5pYamuuyfa2NNbTUl2hpqKGuVCAiKERQWypQV37UlgqUCgWKhaCmGJQKBUrFoDCuOFYqFKirKVBfKtJQWzx0roPGxrL/jCoUrKhJkiRVigmgNAO0NdZy3Wtedmh73/4h9g+NMDgyxuDwGIl0qLtqXalAfU2RupoCTbUlGmqK0yLpmg4xSJIkzXQmgNIMNLupltlNtdUOQ5IkSdOME1gkSZIkKSdMACVJkiQpJ0wAJUmSJCknTAAlSZIkKScqmgBGxJUR8UREbIiI6yc4XhcRf1M+fl9ELK9kPJIkSZKUZxVLACOiCNwEXAWcA7wzIs45Yth7gX0ppTOATwF/Wql4JEmSJCnvKlkBvAzYkFLamFIaAr4BXHPEmGuAr5Sf3wZcEeNXhpYkaYbyLhlJUjVUMgFcAmwet72lvG/CMSmlEaAbmFvBmCRJqjrvkpEkVcsp0QQmIq6LiDURsaazs7Pa4UiSdKK8S0aSVBWVTAC3AkvHbXeU9004JiJKQCuw58gTpZRuTimtTimtbm9vr1C4kiRNGe+SkSRVRamC574fWBkRK8gSvWuBdx0x5nbgt4B7gLcBP04ppWOddO3atbsj4tkTjG0esPsEz3Gq8ZrzIW/XnLfrhfxd82nVDmC6i4jrgOvKm4MR8Wg14znF5O3v04ny/To+vl/Hx/fr+Jz1Ul9YsQQwpTQSER8A7gSKwJdSSo9FxCeANSml24EvAl+LiA3AXrIk8cXOe8IlwIhYk1JafaLnOZV4zfmQt2vO2/VCPq95hjqeu2S2vNhdMsDN4M/H8fL9Oj6+X8fH9+v4+H4dn4hY81JfW8kKICmlO4A7jth3w7jnA8DbKxmDJEnTUEXukpEk6cVUNAGUJEkvVKm7ZCRJejF5TQBvrnYAVeA150Perjlv1wv5vOYZqUJ3yfjzcXx8v46P79fx8f06Pr5fx+clv1/h3SSSJEmSlA+nxDqAkiRJkqQTl7sEMCKujIgnImJDRFxf7XgqISKWRsRdEfF4RDwWEb9X3j8nIn4QEU+V/5xd7VhPpogoRsSDEfEP5e0VEXFf+Xv9NxFRW+0YT6aIaIuI2yLiFxGxPiJemYPv8e+Xf6YfjYi/joj6mfZ9jogvRcSu8a38j/Z9jcyN5Wt/OCIuqV7kmiov9jkWEXXlvwsbyn83lk99lNPLJN6zD5c/Mx+OiB9FRK6XH5ns70oR8esRkSIi150bJ/N+RcRvjPu97JapjnE6mcTfx2Xl32MfLP+dfGM14pwOJvqd4IjjL+n3gFwlgBFRBG4CrgLOAd4ZEedUN6qKGAE+klI6B3gF8P7ydV4P/CiltBL4UXl7Jvk9YP247T8FPpVSOgPYB7y3KlFVzl8C/5hSOhu4kOzaZ+z3OCKWAB8EVqeUziNrnHEtM+/7/GXgyiP2He37ehWwsvy4DvjcFMWoKpnk59h7gX3lvxOfIvs7kluTfM8eJPu35QLgNuC/T22U08dkf1eKiGayz937pjbC6WUy71dErAQ+BrwqpXQu8KEpD3SamOTP138Gbk0pXUz2Of/ZqY1yWvkyL/ydYLyX9HtArhJA4DJgQ0ppY0ppCPgGcE2VYzrpUkrbU0oPlJ/3kiUGS8iu9SvlYV8B3lKdCE++iOgA3gR8obwdwGvJPshh5l1vK/Aasi6BpJSGUkpdzODvcVkJaIhsTbRGYDsz7PucUrqbrOPjeEf7vl4DfDVl7gXaImLR1ESqKpnM59j4n5fbgCvK/ybm1Yu+Zymlu1JKB8qb95Kty5hXk/1d6b+Q/efCwFQGNw1N5v16H3BTSmkfQEpp1xTHOJ1M5v1KQEv5eSuwbQrjm1aO8jvBeC/p94C8JYBLgM3jtreU981Y5Vt/Lib7H7oFKaXt5UM7gAVVCqsS/gL4j8BYeXsu0JVSGilvz7Tv9QqgE/i/7d1/6F11Hcfx58ttkbmY0SCKLfZHM6Kfin+M/CdM+qNi/ZGkQ0tl/yS0fiAyqz+K8I/IkFhJpZlFjcTM1hcsLdQiSkutpW1ByFxrojUDF6sYa7z743y+62J9t+vY7vl+z3k+4PI993Mv5/s+n88595z3/Xw+597Whkh8LclZDLiNq+op4PPAPrrE7yDwKMNu53kLtevoPtM0VZsfe087Ng7SfSaO1Qs9TjYDPzqtES1uJ6yvNsxsbVXdPcvAFqlp9q9zgHOS/CLJQ0mO16MzdNPU16eBy5Psp7tT8pbZhLYkndR1wNgSwFFJshL4HvDRqvr75Gvtx4QHcQvYJO8G/lpVj/YdywwtB84DvtyGSPyD5w33HFIbA7R5b++hS35fBZzF8YdFDNLQ2lVaTJJcDpwP3NB3LItVkjOAG4Fr+o5lCVlON0TvbcAm4JYkZ/ca0eK2CfhGVa0B3kn3e6jmLKfQ2CrzKWDtxPM1rWxwkqygS/62V9Vdrfgv893C7e9QhiBcAGxMspduKMGFdPPjzm5DBWF4bb0f2F9V83Mv7qRLCIfaxgAXAU9W1YGqOgLcRdf2Q27neQu162g+03TMNG1+7D3t2FgF/G0m0S1OUx0nSS4CPglsrKrDM4ptMTpRfb0UeAPw03be3QDMjfhGMNPsX/uBuao6UlVPAn+kSwjHaJr62gzcAVBVDwIvBlbPJLql56SuA8aWAD4MrE9318AX0U0snes5plOuzfW4FfhDVd048dIccEVbvgL4waxjOx2q6uNVtaaq1tG16f1VdRnwAHBxe9tgthegqp4B/pzkta3o7cBuBtrGzT5gQ5KXtH18fpsH284TFmrXOeAD7S5gG4CDE0NFNUzTnMcm95eL6T4Tx9xrfMI6S3Iu8FW65G9IX5ydjOPWV1UdrKrVVbWunXcfoqu3R/oJt3fTHJM76Hr/SLKabkjonlkGuYhMU1/76M7xJHkdXQJ4YKZRLh0ndR2w/ERvGJKq+neSDwH30t1B8OtVtavnsE6HC4D3A48n2dnKPgF8FrgjyWbgT8D7eopvVrYCtye5nu4Ob7f2HM+ptgXY3j5A9wBX0X2pM8g2rqpfJbkT+A3dnW5/C9wM3M2A2jnJd+guFFa3+Q+fYuFj94d0w2OeAP5Jtw9owBY6jyX5DPBIVc3RHQPfSvIE3c0DLu0v4v5NWWc3ACuB77b75eyrqo29Bd2jKetLzZT1dS/wjiS7gaPAtVU1yl75KevrGrphsh+jm/Jw5Vi/xFrgmmAFQFV9hZO8DshI61OSJEmSRmdsQ0AlSZIkabRMACVJkiRpJEwAJUmSJGkkTAAlSZIkaSRMACVJkiRpJEwApRlIcjTJzonHdadw3euS/P5UrU+SJEnDNarfAZR69K+qekvfQUiSJGnc7AGUepRkb5LPJXk8ya+TvKaVr0tyf5LHktyX5NWt/BVJvp/kd+3x1raqZUluSbIryY+TnNne/+Eku9t6bu9pMyVJkrRImABKs3Hm84aAXjLx2sGqeiPwJeALreyLwDer6k3AdmBbK98G/Kyq3gycB+xq5euBm6rq9cBzwHtb+XXAuW09HzxdGydJkqSlIVXVdwzS4CU5VFUr/0/5XuDCqtqTZAXwTFW9PMmzwCur6kgrf7qqVic5AKypqsMT61gH/KSq1rfnW4EVVXV9knuAQ8AOYEdVHTrNmypJkqRFzB5AqX+1wPILcXhi+Sj/nd/7LuAmut7Ch5M471eSJGnETACl/l0y8ffBtvxL4NK2fBnw87Z8H3A1QJJlSVYttNIkZwBrq+oBYCuwCvifXkhJkiSNh70B0mycmWTnxPN7qmr+pyBeluQxul68Ta1sC3BbkmuBA8BVrfwjwM1JNtP19F0NPL3A/1wGfLsliQG2VdVzp2yLJEmStOQ4B1DqUZsDeH5VPdt3LJIkSRo+h4BKkiRJ0kjYAyhJkiRJI2EPoCRJkiSNhAmgJEmSJI2ECaAkSZIkjYQJoCRJkiSNhAmgJEmSJI2ECaAkSZIkjcR/AAamUCd9p5tcAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 1080x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_learning_curves(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 334
    },
    "id": "Clvgo7YvU02S",
    "outputId": "359113a9-ec8e-4cd8-9979-acf42d8dd0f1"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_learning_curves(history1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KzfzNIMJfXdg",
    "outputId": "afda526e-6433-4c6e-920c-10971317b52e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "embedding_network.save('./drive/MyDrive/SGD_Siamese_triplet_embeddings.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WOlsibrAfoDu",
    "outputId": "ee35bc35-4632-4010-cb9d-8d2348668d7a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "temp1=tf.keras.models.load_model('./drive/MyDrive/SGD_Siamese_triplet_embeddings.h5')\n",
    "temp2=tf.keras.models.load_model('./drive/MyDrive/SGD_siamese_triplet_embeddings_transferLearning2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CxcfoXdKg_NK",
    "outputId": "466abda1-992e-48b8-b95d-1f5b209a11ef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 3ms/step - loss: 4.4156 - accuracy: 0.3059\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[4.415596008300781, 0.3059000074863434]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp2.evaluate(temp1.predict(x_val),y_val1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vz-75x55n9im"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kFB8eLt9kXei"
   },
   "outputs": [],
   "source": [
    "test_dataset=pd.read_csv('./drive/MyDrive/test_representation.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "id": "JTuh2fKakpZH",
    "outputId": "f129458f-ff8d-4814-df5f-e35b8d071519"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-248c7ee3-d0d7-41b3-8338-b1d18a31ed1a\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>2039</th>\n",
       "      <th>2040</th>\n",
       "      <th>2041</th>\n",
       "      <th>2042</th>\n",
       "      <th>2043</th>\n",
       "      <th>2044</th>\n",
       "      <th>2045</th>\n",
       "      <th>2046</th>\n",
       "      <th>2047</th>\n",
       "      <th>category_class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.029340</td>\n",
       "      <td>0.959396</td>\n",
       "      <td>0.032180</td>\n",
       "      <td>0.007271</td>\n",
       "      <td>0.505135</td>\n",
       "      <td>0.047305</td>\n",
       "      <td>0.046234</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.160508</td>\n",
       "      <td>0.108298</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.031925</td>\n",
       "      <td>0.008031</td>\n",
       "      <td>0.129452</td>\n",
       "      <td>0.450996</td>\n",
       "      <td>0.004531</td>\n",
       "      <td>0.008310</td>\n",
       "      <td>65.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.018695</td>\n",
       "      <td>0.131584</td>\n",
       "      <td>0.000580</td>\n",
       "      <td>0.265691</td>\n",
       "      <td>0.075151</td>\n",
       "      <td>0.005068</td>\n",
       "      <td>0.164080</td>\n",
       "      <td>0.603745</td>\n",
       "      <td>0.167325</td>\n",
       "      <td>0.076654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.029152</td>\n",
       "      <td>0.214970</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.043662</td>\n",
       "      <td>0.057358</td>\n",
       "      <td>0.093535</td>\n",
       "      <td>0.103441</td>\n",
       "      <td>0.011182</td>\n",
       "      <td>0.052681</td>\n",
       "      <td>970.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.008372</td>\n",
       "      <td>0.236403</td>\n",
       "      <td>0.014399</td>\n",
       "      <td>0.041888</td>\n",
       "      <td>0.024327</td>\n",
       "      <td>0.042268</td>\n",
       "      <td>0.086519</td>\n",
       "      <td>0.659389</td>\n",
       "      <td>0.053838</td>\n",
       "      <td>0.071150</td>\n",
       "      <td>...</td>\n",
       "      <td>0.135679</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.235405</td>\n",
       "      <td>0.324949</td>\n",
       "      <td>0.021558</td>\n",
       "      <td>0.093964</td>\n",
       "      <td>0.012577</td>\n",
       "      <td>0.001841</td>\n",
       "      <td>0.246740</td>\n",
       "      <td>230.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.169781</td>\n",
       "      <td>0.011046</td>\n",
       "      <td>0.054933</td>\n",
       "      <td>1.199504</td>\n",
       "      <td>0.027333</td>\n",
       "      <td>0.123167</td>\n",
       "      <td>0.185943</td>\n",
       "      <td>0.762185</td>\n",
       "      <td>0.053278</td>\n",
       "      <td>0.096448</td>\n",
       "      <td>...</td>\n",
       "      <td>0.148584</td>\n",
       "      <td>0.007761</td>\n",
       "      <td>0.816408</td>\n",
       "      <td>0.135283</td>\n",
       "      <td>0.010906</td>\n",
       "      <td>0.306520</td>\n",
       "      <td>1.403771</td>\n",
       "      <td>0.235088</td>\n",
       "      <td>0.000832</td>\n",
       "      <td>809.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.050206</td>\n",
       "      <td>0.857661</td>\n",
       "      <td>0.150167</td>\n",
       "      <td>0.052228</td>\n",
       "      <td>0.291346</td>\n",
       "      <td>0.206538</td>\n",
       "      <td>0.145282</td>\n",
       "      <td>0.104864</td>\n",
       "      <td>0.128827</td>\n",
       "      <td>0.380020</td>\n",
       "      <td>...</td>\n",
       "      <td>0.163841</td>\n",
       "      <td>0.060867</td>\n",
       "      <td>0.049904</td>\n",
       "      <td>0.576424</td>\n",
       "      <td>0.294275</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.080127</td>\n",
       "      <td>0.016386</td>\n",
       "      <td>0.423166</td>\n",
       "      <td>516.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49995</th>\n",
       "      <td>0.002047</td>\n",
       "      <td>0.012996</td>\n",
       "      <td>0.043287</td>\n",
       "      <td>0.194361</td>\n",
       "      <td>0.340890</td>\n",
       "      <td>0.095029</td>\n",
       "      <td>0.137551</td>\n",
       "      <td>0.295975</td>\n",
       "      <td>0.000093</td>\n",
       "      <td>0.009103</td>\n",
       "      <td>...</td>\n",
       "      <td>0.101937</td>\n",
       "      <td>0.010900</td>\n",
       "      <td>0.057891</td>\n",
       "      <td>0.205408</td>\n",
       "      <td>0.010592</td>\n",
       "      <td>1.026036</td>\n",
       "      <td>0.037057</td>\n",
       "      <td>0.005353</td>\n",
       "      <td>1.050666</td>\n",
       "      <td>283.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>0.028609</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>0.154415</td>\n",
       "      <td>1.012217</td>\n",
       "      <td>0.000856</td>\n",
       "      <td>0.078721</td>\n",
       "      <td>0.403259</td>\n",
       "      <td>0.151994</td>\n",
       "      <td>0.003251</td>\n",
       "      <td>0.013472</td>\n",
       "      <td>...</td>\n",
       "      <td>0.033227</td>\n",
       "      <td>0.268508</td>\n",
       "      <td>0.017099</td>\n",
       "      <td>0.000700</td>\n",
       "      <td>0.121087</td>\n",
       "      <td>0.025590</td>\n",
       "      <td>1.007298</td>\n",
       "      <td>0.013507</td>\n",
       "      <td>0.169299</td>\n",
       "      <td>26.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>0.031759</td>\n",
       "      <td>0.009449</td>\n",
       "      <td>0.047738</td>\n",
       "      <td>0.047382</td>\n",
       "      <td>0.949164</td>\n",
       "      <td>0.059264</td>\n",
       "      <td>0.016978</td>\n",
       "      <td>0.000756</td>\n",
       "      <td>0.020112</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.354629</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.229080</td>\n",
       "      <td>0.020603</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.809951</td>\n",
       "      <td>0.017986</td>\n",
       "      <td>0.058925</td>\n",
       "      <td>0.459367</td>\n",
       "      <td>232.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>0.008271</td>\n",
       "      <td>0.562913</td>\n",
       "      <td>0.022252</td>\n",
       "      <td>0.110028</td>\n",
       "      <td>0.601711</td>\n",
       "      <td>0.062303</td>\n",
       "      <td>0.476841</td>\n",
       "      <td>0.167790</td>\n",
       "      <td>0.086034</td>\n",
       "      <td>0.083120</td>\n",
       "      <td>...</td>\n",
       "      <td>0.269349</td>\n",
       "      <td>0.010813</td>\n",
       "      <td>0.312314</td>\n",
       "      <td>0.175110</td>\n",
       "      <td>0.289860</td>\n",
       "      <td>0.014819</td>\n",
       "      <td>0.304625</td>\n",
       "      <td>0.050381</td>\n",
       "      <td>0.012309</td>\n",
       "      <td>982.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>0.001101</td>\n",
       "      <td>0.023040</td>\n",
       "      <td>0.043784</td>\n",
       "      <td>0.014679</td>\n",
       "      <td>0.292485</td>\n",
       "      <td>0.303807</td>\n",
       "      <td>0.093233</td>\n",
       "      <td>0.168107</td>\n",
       "      <td>0.040244</td>\n",
       "      <td>0.089973</td>\n",
       "      <td>...</td>\n",
       "      <td>0.156524</td>\n",
       "      <td>0.038731</td>\n",
       "      <td>0.110496</td>\n",
       "      <td>0.076927</td>\n",
       "      <td>0.020674</td>\n",
       "      <td>0.203182</td>\n",
       "      <td>0.361507</td>\n",
       "      <td>0.031252</td>\n",
       "      <td>0.029597</td>\n",
       "      <td>355.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50000 rows × 2049 columns</p>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-248c7ee3-d0d7-41b3-8338-b1d18a31ed1a')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-248c7ee3-d0d7-41b3-8338-b1d18a31ed1a button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-248c7ee3-d0d7-41b3-8338-b1d18a31ed1a');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "              0         1         2         3         4         5         6  \\\n",
       "0      0.000000  0.000000  0.029340  0.959396  0.032180  0.007271  0.505135   \n",
       "1      0.018695  0.131584  0.000580  0.265691  0.075151  0.005068  0.164080   \n",
       "2      0.008372  0.236403  0.014399  0.041888  0.024327  0.042268  0.086519   \n",
       "3      0.169781  0.011046  0.054933  1.199504  0.027333  0.123167  0.185943   \n",
       "4      0.050206  0.857661  0.150167  0.052228  0.291346  0.206538  0.145282   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "49995  0.002047  0.012996  0.043287  0.194361  0.340890  0.095029  0.137551   \n",
       "49996  0.028609  0.000027  0.154415  1.012217  0.000856  0.078721  0.403259   \n",
       "49997  0.031759  0.009449  0.047738  0.047382  0.949164  0.059264  0.016978   \n",
       "49998  0.008271  0.562913  0.022252  0.110028  0.601711  0.062303  0.476841   \n",
       "49999  0.001101  0.023040  0.043784  0.014679  0.292485  0.303807  0.093233   \n",
       "\n",
       "              7         8         9  ...      2039      2040      2041  \\\n",
       "0      0.047305  0.046234  0.000000  ...  0.160508  0.108298  0.000000   \n",
       "1      0.603745  0.167325  0.076654  ...  0.029152  0.214970  0.000000   \n",
       "2      0.659389  0.053838  0.071150  ...  0.135679  0.000000  0.235405   \n",
       "3      0.762185  0.053278  0.096448  ...  0.148584  0.007761  0.816408   \n",
       "4      0.104864  0.128827  0.380020  ...  0.163841  0.060867  0.049904   \n",
       "...         ...       ...       ...  ...       ...       ...       ...   \n",
       "49995  0.295975  0.000093  0.009103  ...  0.101937  0.010900  0.057891   \n",
       "49996  0.151994  0.003251  0.013472  ...  0.033227  0.268508  0.017099   \n",
       "49997  0.000756  0.020112  0.000000  ...  0.354629  0.000000  0.229080   \n",
       "49998  0.167790  0.086034  0.083120  ...  0.269349  0.010813  0.312314   \n",
       "49999  0.168107  0.040244  0.089973  ...  0.156524  0.038731  0.110496   \n",
       "\n",
       "           2042      2043      2044      2045      2046      2047  \\\n",
       "0      0.031925  0.008031  0.129452  0.450996  0.004531  0.008310   \n",
       "1      0.043662  0.057358  0.093535  0.103441  0.011182  0.052681   \n",
       "2      0.324949  0.021558  0.093964  0.012577  0.001841  0.246740   \n",
       "3      0.135283  0.010906  0.306520  1.403771  0.235088  0.000832   \n",
       "4      0.576424  0.294275  0.000000  0.080127  0.016386  0.423166   \n",
       "...         ...       ...       ...       ...       ...       ...   \n",
       "49995  0.205408  0.010592  1.026036  0.037057  0.005353  1.050666   \n",
       "49996  0.000700  0.121087  0.025590  1.007298  0.013507  0.169299   \n",
       "49997  0.020603  0.000000  0.809951  0.017986  0.058925  0.459367   \n",
       "49998  0.175110  0.289860  0.014819  0.304625  0.050381  0.012309   \n",
       "49999  0.076927  0.020674  0.203182  0.361507  0.031252  0.029597   \n",
       "\n",
       "       category_class  \n",
       "0                65.0  \n",
       "1               970.0  \n",
       "2               230.0  \n",
       "3               809.0  \n",
       "4               516.0  \n",
       "...               ...  \n",
       "49995           283.0  \n",
       "49996            26.0  \n",
       "49997           232.0  \n",
       "49998           982.0  \n",
       "49999           355.0  \n",
       "\n",
       "[50000 rows x 2049 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PSLJVE7RkeBC"
   },
   "outputs": [],
   "source": [
    "x_test=test_dataset.iloc[:,:-1].values\n",
    "y_test=test_dataset.iloc[:,-1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mle7jMSADCV2"
   },
   "outputs": [],
   "source": [
    "y_test=y_test.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bnRRc6JzksPp"
   },
   "outputs": [],
   "source": [
    "y_test1=np.eye(1000)[y_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V1pdtrJ5kyG6",
    "outputId": "60951100-edc5-4653-cc0c-89e3869699ea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1563/1563 [==============================] - 5s 3ms/step - loss: 4.7299 - accuracy: 0.1984\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[4.729886054992676, 0.19835999608039856]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp2.evaluate(temp1.predict(x_test),y_test1)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Siamese_triplet_Loss.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
