{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vbHVwbv8KYdT"
   },
   "outputs": [],
   "source": [
    "##This notebook is based on https://keras.io/examples/vision/reptile/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ggHdLeuKuwNT"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2jh7bb5fWM5E",
    "outputId": "bf24593e-b487-468c-d7b7-5980dd272428"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "#mounting google drive on colab\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5I9Zn-mIWNlB"
   },
   "outputs": [],
   "source": [
    "train_dataset=pd.read_csv('./drive/MyDrive/best_model_28_representation_train_040422.csv').iloc[:,1:]\n",
    "val_dataset=pd.read_csv('./drive/MyDrive/best_model_28_representation_val_040422.csv').iloc[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z5RocEVkuwNU"
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.003\n",
    "meta_step_size = 0.25\n",
    "\n",
    "inner_batch_size = 25\n",
    "eval_batch_size = 25\n",
    "\n",
    "meta_iters = 2000\n",
    "eval_iters = 5\n",
    "inner_iters = 4\n",
    "\n",
    "eval_interval = 1\n",
    "train_shots = 20\n",
    "shots = 5\n",
    "classes = 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "id": "zhUzaBJuWp-d",
    "outputId": "f81db5e8-9fc9-4609-a912-0cc30469c2bb"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-35957882-f85a-4670-adb0-a684b46700eb\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>2039</th>\n",
       "      <th>2040</th>\n",
       "      <th>2041</th>\n",
       "      <th>2042</th>\n",
       "      <th>2043</th>\n",
       "      <th>2044</th>\n",
       "      <th>2045</th>\n",
       "      <th>2046</th>\n",
       "      <th>2047</th>\n",
       "      <th>category_class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.313336</td>\n",
       "      <td>0.025133</td>\n",
       "      <td>0.229388</td>\n",
       "      <td>0.136946</td>\n",
       "      <td>0.034979</td>\n",
       "      <td>0.120104</td>\n",
       "      <td>0.196935</td>\n",
       "      <td>0.023299</td>\n",
       "      <td>0.166517</td>\n",
       "      <td>...</td>\n",
       "      <td>0.561781</td>\n",
       "      <td>0.001802</td>\n",
       "      <td>0.210695</td>\n",
       "      <td>0.161759</td>\n",
       "      <td>0.020894</td>\n",
       "      <td>0.524201</td>\n",
       "      <td>0.353692</td>\n",
       "      <td>0.020171</td>\n",
       "      <td>0.403670</td>\n",
       "      <td>338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.093444</td>\n",
       "      <td>0.316758</td>\n",
       "      <td>0.170155</td>\n",
       "      <td>0.137241</td>\n",
       "      <td>0.259935</td>\n",
       "      <td>0.474877</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.194750</td>\n",
       "      <td>0.067905</td>\n",
       "      <td>0.051878</td>\n",
       "      <td>...</td>\n",
       "      <td>1.161999</td>\n",
       "      <td>0.064962</td>\n",
       "      <td>0.178196</td>\n",
       "      <td>0.366895</td>\n",
       "      <td>0.073961</td>\n",
       "      <td>0.799406</td>\n",
       "      <td>0.238650</td>\n",
       "      <td>0.013355</td>\n",
       "      <td>0.226344</td>\n",
       "      <td>338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.005281</td>\n",
       "      <td>0.349727</td>\n",
       "      <td>0.094547</td>\n",
       "      <td>0.376952</td>\n",
       "      <td>0.304110</td>\n",
       "      <td>0.114644</td>\n",
       "      <td>0.173896</td>\n",
       "      <td>0.426651</td>\n",
       "      <td>0.051706</td>\n",
       "      <td>0.172470</td>\n",
       "      <td>...</td>\n",
       "      <td>0.607481</td>\n",
       "      <td>0.092371</td>\n",
       "      <td>0.017004</td>\n",
       "      <td>0.155562</td>\n",
       "      <td>0.018352</td>\n",
       "      <td>0.468657</td>\n",
       "      <td>0.072951</td>\n",
       "      <td>0.036000</td>\n",
       "      <td>0.496107</td>\n",
       "      <td>338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.089383</td>\n",
       "      <td>0.107351</td>\n",
       "      <td>0.075641</td>\n",
       "      <td>0.013774</td>\n",
       "      <td>0.096981</td>\n",
       "      <td>0.001354</td>\n",
       "      <td>0.072457</td>\n",
       "      <td>0.125850</td>\n",
       "      <td>0.032342</td>\n",
       "      <td>0.126957</td>\n",
       "      <td>...</td>\n",
       "      <td>0.416517</td>\n",
       "      <td>0.064198</td>\n",
       "      <td>0.024564</td>\n",
       "      <td>0.143060</td>\n",
       "      <td>0.013859</td>\n",
       "      <td>0.805468</td>\n",
       "      <td>0.205996</td>\n",
       "      <td>0.024247</td>\n",
       "      <td>0.788354</td>\n",
       "      <td>338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.013804</td>\n",
       "      <td>0.034383</td>\n",
       "      <td>0.114037</td>\n",
       "      <td>0.031434</td>\n",
       "      <td>0.449520</td>\n",
       "      <td>0.003940</td>\n",
       "      <td>0.158426</td>\n",
       "      <td>0.118010</td>\n",
       "      <td>0.004489</td>\n",
       "      <td>0.047437</td>\n",
       "      <td>...</td>\n",
       "      <td>0.264046</td>\n",
       "      <td>0.103955</td>\n",
       "      <td>0.075154</td>\n",
       "      <td>0.008518</td>\n",
       "      <td>0.005747</td>\n",
       "      <td>0.603387</td>\n",
       "      <td>0.416539</td>\n",
       "      <td>0.005948</td>\n",
       "      <td>0.242247</td>\n",
       "      <td>338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39995</th>\n",
       "      <td>0.008469</td>\n",
       "      <td>0.153707</td>\n",
       "      <td>0.295961</td>\n",
       "      <td>0.640903</td>\n",
       "      <td>0.147320</td>\n",
       "      <td>0.107286</td>\n",
       "      <td>0.033159</td>\n",
       "      <td>0.055960</td>\n",
       "      <td>0.008775</td>\n",
       "      <td>0.000215</td>\n",
       "      <td>...</td>\n",
       "      <td>0.466908</td>\n",
       "      <td>0.157604</td>\n",
       "      <td>0.001789</td>\n",
       "      <td>0.313070</td>\n",
       "      <td>0.021882</td>\n",
       "      <td>0.128249</td>\n",
       "      <td>0.420876</td>\n",
       "      <td>0.125631</td>\n",
       "      <td>0.086492</td>\n",
       "      <td>591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39996</th>\n",
       "      <td>0.010601</td>\n",
       "      <td>0.295052</td>\n",
       "      <td>0.014810</td>\n",
       "      <td>0.608878</td>\n",
       "      <td>0.032543</td>\n",
       "      <td>0.013807</td>\n",
       "      <td>0.200572</td>\n",
       "      <td>0.015779</td>\n",
       "      <td>0.207144</td>\n",
       "      <td>0.473168</td>\n",
       "      <td>...</td>\n",
       "      <td>0.044090</td>\n",
       "      <td>0.042290</td>\n",
       "      <td>0.077591</td>\n",
       "      <td>0.028302</td>\n",
       "      <td>0.023190</td>\n",
       "      <td>0.081915</td>\n",
       "      <td>0.164003</td>\n",
       "      <td>0.035183</td>\n",
       "      <td>0.097715</td>\n",
       "      <td>591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39997</th>\n",
       "      <td>0.192546</td>\n",
       "      <td>0.433518</td>\n",
       "      <td>0.085116</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.819737</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.049733</td>\n",
       "      <td>0.018173</td>\n",
       "      <td>0.015861</td>\n",
       "      <td>0.194425</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004095</td>\n",
       "      <td>0.006790</td>\n",
       "      <td>0.154554</td>\n",
       "      <td>0.153198</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.522131</td>\n",
       "      <td>0.000262</td>\n",
       "      <td>0.001606</td>\n",
       "      <td>591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39998</th>\n",
       "      <td>0.052466</td>\n",
       "      <td>1.688614</td>\n",
       "      <td>0.070324</td>\n",
       "      <td>0.110022</td>\n",
       "      <td>0.126059</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.095915</td>\n",
       "      <td>0.001023</td>\n",
       "      <td>0.010808</td>\n",
       "      <td>0.105967</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.058327</td>\n",
       "      <td>0.070632</td>\n",
       "      <td>0.421258</td>\n",
       "      <td>0.018547</td>\n",
       "      <td>0.004673</td>\n",
       "      <td>0.220298</td>\n",
       "      <td>0.007178</td>\n",
       "      <td>0.071261</td>\n",
       "      <td>591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39999</th>\n",
       "      <td>0.025741</td>\n",
       "      <td>0.752354</td>\n",
       "      <td>0.080995</td>\n",
       "      <td>0.015271</td>\n",
       "      <td>0.802826</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000496</td>\n",
       "      <td>0.077876</td>\n",
       "      <td>0.074441</td>\n",
       "      <td>0.062494</td>\n",
       "      <td>...</td>\n",
       "      <td>0.019828</td>\n",
       "      <td>0.070384</td>\n",
       "      <td>0.113285</td>\n",
       "      <td>0.291323</td>\n",
       "      <td>0.541946</td>\n",
       "      <td>0.457819</td>\n",
       "      <td>0.277326</td>\n",
       "      <td>0.044750</td>\n",
       "      <td>0.076066</td>\n",
       "      <td>591</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>40000 rows × 2049 columns</p>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-35957882-f85a-4670-adb0-a684b46700eb')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-35957882-f85a-4670-adb0-a684b46700eb button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-35957882-f85a-4670-adb0-a684b46700eb');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "              0         1         2         3         4         5         6  \\\n",
       "0      0.033333  0.313336  0.025133  0.229388  0.136946  0.034979  0.120104   \n",
       "1      0.093444  0.316758  0.170155  0.137241  0.259935  0.474877  0.000000   \n",
       "2      0.005281  0.349727  0.094547  0.376952  0.304110  0.114644  0.173896   \n",
       "3      0.089383  0.107351  0.075641  0.013774  0.096981  0.001354  0.072457   \n",
       "4      0.013804  0.034383  0.114037  0.031434  0.449520  0.003940  0.158426   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "39995  0.008469  0.153707  0.295961  0.640903  0.147320  0.107286  0.033159   \n",
       "39996  0.010601  0.295052  0.014810  0.608878  0.032543  0.013807  0.200572   \n",
       "39997  0.192546  0.433518  0.085116  0.000000  0.819737  0.000000  0.049733   \n",
       "39998  0.052466  1.688614  0.070324  0.110022  0.126059  0.000000  0.095915   \n",
       "39999  0.025741  0.752354  0.080995  0.015271  0.802826  0.000000  0.000496   \n",
       "\n",
       "              7         8         9  ...      2039      2040      2041  \\\n",
       "0      0.196935  0.023299  0.166517  ...  0.561781  0.001802  0.210695   \n",
       "1      0.194750  0.067905  0.051878  ...  1.161999  0.064962  0.178196   \n",
       "2      0.426651  0.051706  0.172470  ...  0.607481  0.092371  0.017004   \n",
       "3      0.125850  0.032342  0.126957  ...  0.416517  0.064198  0.024564   \n",
       "4      0.118010  0.004489  0.047437  ...  0.264046  0.103955  0.075154   \n",
       "...         ...       ...       ...  ...       ...       ...       ...   \n",
       "39995  0.055960  0.008775  0.000215  ...  0.466908  0.157604  0.001789   \n",
       "39996  0.015779  0.207144  0.473168  ...  0.044090  0.042290  0.077591   \n",
       "39997  0.018173  0.015861  0.194425  ...  0.000000  0.004095  0.006790   \n",
       "39998  0.001023  0.010808  0.105967  ...  0.000000  0.058327  0.070632   \n",
       "39999  0.077876  0.074441  0.062494  ...  0.019828  0.070384  0.113285   \n",
       "\n",
       "           2042      2043      2044      2045      2046      2047  \\\n",
       "0      0.161759  0.020894  0.524201  0.353692  0.020171  0.403670   \n",
       "1      0.366895  0.073961  0.799406  0.238650  0.013355  0.226344   \n",
       "2      0.155562  0.018352  0.468657  0.072951  0.036000  0.496107   \n",
       "3      0.143060  0.013859  0.805468  0.205996  0.024247  0.788354   \n",
       "4      0.008518  0.005747  0.603387  0.416539  0.005948  0.242247   \n",
       "...         ...       ...       ...       ...       ...       ...   \n",
       "39995  0.313070  0.021882  0.128249  0.420876  0.125631  0.086492   \n",
       "39996  0.028302  0.023190  0.081915  0.164003  0.035183  0.097715   \n",
       "39997  0.154554  0.153198  0.000000  0.522131  0.000262  0.001606   \n",
       "39998  0.421258  0.018547  0.004673  0.220298  0.007178  0.071261   \n",
       "39999  0.291323  0.541946  0.457819  0.277326  0.044750  0.076066   \n",
       "\n",
       "       category_class  \n",
       "0                 338  \n",
       "1                 338  \n",
       "2                 338  \n",
       "3                 338  \n",
       "4                 338  \n",
       "...               ...  \n",
       "39995             591  \n",
       "39996             591  \n",
       "39997             591  \n",
       "39998             591  \n",
       "39999             591  \n",
       "\n",
       "[40000 rows x 2049 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "id": "-tjQ1hrHWuKx",
    "outputId": "3cb80120-bf4f-41ed-ffef-90e293fb2244"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-476a71ac-c80c-4817-a9fa-18277c3ddee4\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>2039</th>\n",
       "      <th>2040</th>\n",
       "      <th>2041</th>\n",
       "      <th>2042</th>\n",
       "      <th>2043</th>\n",
       "      <th>2044</th>\n",
       "      <th>2045</th>\n",
       "      <th>2046</th>\n",
       "      <th>2047</th>\n",
       "      <th>category_class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.037247</td>\n",
       "      <td>0.084723</td>\n",
       "      <td>0.067953</td>\n",
       "      <td>0.148163</td>\n",
       "      <td>0.190729</td>\n",
       "      <td>0.034427</td>\n",
       "      <td>0.010865</td>\n",
       "      <td>0.190755</td>\n",
       "      <td>0.040758</td>\n",
       "      <td>0.071579</td>\n",
       "      <td>...</td>\n",
       "      <td>0.413018</td>\n",
       "      <td>0.040352</td>\n",
       "      <td>0.029557</td>\n",
       "      <td>0.017327</td>\n",
       "      <td>0.082976</td>\n",
       "      <td>0.208654</td>\n",
       "      <td>0.213978</td>\n",
       "      <td>0.007254</td>\n",
       "      <td>0.369544</td>\n",
       "      <td>338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.006566</td>\n",
       "      <td>0.147232</td>\n",
       "      <td>0.117783</td>\n",
       "      <td>0.476772</td>\n",
       "      <td>0.533292</td>\n",
       "      <td>0.588703</td>\n",
       "      <td>0.131454</td>\n",
       "      <td>0.441281</td>\n",
       "      <td>0.010310</td>\n",
       "      <td>0.016984</td>\n",
       "      <td>...</td>\n",
       "      <td>0.224854</td>\n",
       "      <td>0.045422</td>\n",
       "      <td>0.151592</td>\n",
       "      <td>0.346858</td>\n",
       "      <td>0.000291</td>\n",
       "      <td>0.781311</td>\n",
       "      <td>0.047355</td>\n",
       "      <td>0.008029</td>\n",
       "      <td>0.237799</td>\n",
       "      <td>338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.210000</td>\n",
       "      <td>0.083216</td>\n",
       "      <td>0.107389</td>\n",
       "      <td>0.373491</td>\n",
       "      <td>0.139399</td>\n",
       "      <td>0.228461</td>\n",
       "      <td>0.224466</td>\n",
       "      <td>0.400738</td>\n",
       "      <td>0.035322</td>\n",
       "      <td>0.065952</td>\n",
       "      <td>...</td>\n",
       "      <td>0.340572</td>\n",
       "      <td>0.006806</td>\n",
       "      <td>0.127561</td>\n",
       "      <td>0.045913</td>\n",
       "      <td>0.742372</td>\n",
       "      <td>0.063517</td>\n",
       "      <td>0.105330</td>\n",
       "      <td>0.058667</td>\n",
       "      <td>0.092687</td>\n",
       "      <td>338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.089227</td>\n",
       "      <td>0.028722</td>\n",
       "      <td>0.023529</td>\n",
       "      <td>0.516543</td>\n",
       "      <td>0.357591</td>\n",
       "      <td>1.165964</td>\n",
       "      <td>0.149411</td>\n",
       "      <td>0.075871</td>\n",
       "      <td>0.022748</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.272945</td>\n",
       "      <td>0.010554</td>\n",
       "      <td>1.335347</td>\n",
       "      <td>0.072965</td>\n",
       "      <td>0.002853</td>\n",
       "      <td>0.189264</td>\n",
       "      <td>0.011270</td>\n",
       "      <td>0.085637</td>\n",
       "      <td>0.058687</td>\n",
       "      <td>338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.005663</td>\n",
       "      <td>0.135440</td>\n",
       "      <td>0.112919</td>\n",
       "      <td>0.115065</td>\n",
       "      <td>0.094258</td>\n",
       "      <td>0.071284</td>\n",
       "      <td>0.024563</td>\n",
       "      <td>0.129603</td>\n",
       "      <td>0.003601</td>\n",
       "      <td>0.212501</td>\n",
       "      <td>...</td>\n",
       "      <td>0.402290</td>\n",
       "      <td>0.103481</td>\n",
       "      <td>0.176257</td>\n",
       "      <td>0.158090</td>\n",
       "      <td>0.010789</td>\n",
       "      <td>0.546413</td>\n",
       "      <td>0.319652</td>\n",
       "      <td>0.083233</td>\n",
       "      <td>0.016363</td>\n",
       "      <td>338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>0.000638</td>\n",
       "      <td>0.113700</td>\n",
       "      <td>0.108743</td>\n",
       "      <td>0.090572</td>\n",
       "      <td>0.055815</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.082937</td>\n",
       "      <td>0.025062</td>\n",
       "      <td>0.003649</td>\n",
       "      <td>0.045059</td>\n",
       "      <td>...</td>\n",
       "      <td>0.713404</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009588</td>\n",
       "      <td>0.051383</td>\n",
       "      <td>0.411699</td>\n",
       "      <td>0.089952</td>\n",
       "      <td>0.282519</td>\n",
       "      <td>0.000132</td>\n",
       "      <td>0.007038</td>\n",
       "      <td>591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>0.059231</td>\n",
       "      <td>0.211131</td>\n",
       "      <td>0.201713</td>\n",
       "      <td>0.032353</td>\n",
       "      <td>0.048716</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.077469</td>\n",
       "      <td>0.013222</td>\n",
       "      <td>0.060250</td>\n",
       "      <td>...</td>\n",
       "      <td>0.411932</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.028635</td>\n",
       "      <td>0.034902</td>\n",
       "      <td>0.292674</td>\n",
       "      <td>0.112189</td>\n",
       "      <td>0.008453</td>\n",
       "      <td>0.048728</td>\n",
       "      <td>591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>0.001761</td>\n",
       "      <td>1.463821</td>\n",
       "      <td>0.011850</td>\n",
       "      <td>0.032598</td>\n",
       "      <td>0.937367</td>\n",
       "      <td>0.021707</td>\n",
       "      <td>0.103139</td>\n",
       "      <td>0.006572</td>\n",
       "      <td>0.019002</td>\n",
       "      <td>0.021300</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009892</td>\n",
       "      <td>0.071254</td>\n",
       "      <td>0.661938</td>\n",
       "      <td>0.006536</td>\n",
       "      <td>0.265005</td>\n",
       "      <td>0.174313</td>\n",
       "      <td>0.000492</td>\n",
       "      <td>0.299318</td>\n",
       "      <td>591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>0.026339</td>\n",
       "      <td>0.870153</td>\n",
       "      <td>0.044403</td>\n",
       "      <td>0.028958</td>\n",
       "      <td>0.081213</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.152911</td>\n",
       "      <td>0.002433</td>\n",
       "      <td>0.006991</td>\n",
       "      <td>0.132442</td>\n",
       "      <td>...</td>\n",
       "      <td>0.021862</td>\n",
       "      <td>0.001303</td>\n",
       "      <td>0.022319</td>\n",
       "      <td>0.043793</td>\n",
       "      <td>0.260402</td>\n",
       "      <td>0.007712</td>\n",
       "      <td>0.034186</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.067226</td>\n",
       "      <td>591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>0.028445</td>\n",
       "      <td>0.396974</td>\n",
       "      <td>0.129699</td>\n",
       "      <td>0.304854</td>\n",
       "      <td>0.032658</td>\n",
       "      <td>0.027572</td>\n",
       "      <td>0.009164</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.114894</td>\n",
       "      <td>0.589555</td>\n",
       "      <td>...</td>\n",
       "      <td>0.069370</td>\n",
       "      <td>0.034535</td>\n",
       "      <td>0.012728</td>\n",
       "      <td>0.155390</td>\n",
       "      <td>0.008744</td>\n",
       "      <td>0.136618</td>\n",
       "      <td>0.080729</td>\n",
       "      <td>0.018973</td>\n",
       "      <td>0.069819</td>\n",
       "      <td>591</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 2049 columns</p>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-476a71ac-c80c-4817-a9fa-18277c3ddee4')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-476a71ac-c80c-4817-a9fa-18277c3ddee4 button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-476a71ac-c80c-4817-a9fa-18277c3ddee4');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "             0         1         2         3         4         5         6  \\\n",
       "0     0.037247  0.084723  0.067953  0.148163  0.190729  0.034427  0.010865   \n",
       "1     0.006566  0.147232  0.117783  0.476772  0.533292  0.588703  0.131454   \n",
       "2     0.210000  0.083216  0.107389  0.373491  0.139399  0.228461  0.224466   \n",
       "3     0.089227  0.028722  0.023529  0.516543  0.357591  1.165964  0.149411   \n",
       "4     0.005663  0.135440  0.112919  0.115065  0.094258  0.071284  0.024563   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "9995  0.000638  0.113700  0.108743  0.090572  0.055815  0.000000  0.082937   \n",
       "9996  0.059231  0.211131  0.201713  0.032353  0.048716  0.000000  0.000000   \n",
       "9997  0.001761  1.463821  0.011850  0.032598  0.937367  0.021707  0.103139   \n",
       "9998  0.026339  0.870153  0.044403  0.028958  0.081213  0.000000  0.152911   \n",
       "9999  0.028445  0.396974  0.129699  0.304854  0.032658  0.027572  0.009164   \n",
       "\n",
       "             7         8         9  ...      2039      2040      2041  \\\n",
       "0     0.190755  0.040758  0.071579  ...  0.413018  0.040352  0.029557   \n",
       "1     0.441281  0.010310  0.016984  ...  0.224854  0.045422  0.151592   \n",
       "2     0.400738  0.035322  0.065952  ...  0.340572  0.006806  0.127561   \n",
       "3     0.075871  0.022748  0.000000  ...  0.272945  0.010554  1.335347   \n",
       "4     0.129603  0.003601  0.212501  ...  0.402290  0.103481  0.176257   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "9995  0.025062  0.003649  0.045059  ...  0.713404  0.000000  0.009588   \n",
       "9996  0.077469  0.013222  0.060250  ...  0.411932  0.000000  0.000043   \n",
       "9997  0.006572  0.019002  0.021300  ...  0.000000  0.009892  0.071254   \n",
       "9998  0.002433  0.006991  0.132442  ...  0.021862  0.001303  0.022319   \n",
       "9999  0.000000  0.114894  0.589555  ...  0.069370  0.034535  0.012728   \n",
       "\n",
       "          2042      2043      2044      2045      2046      2047  \\\n",
       "0     0.017327  0.082976  0.208654  0.213978  0.007254  0.369544   \n",
       "1     0.346858  0.000291  0.781311  0.047355  0.008029  0.237799   \n",
       "2     0.045913  0.742372  0.063517  0.105330  0.058667  0.092687   \n",
       "3     0.072965  0.002853  0.189264  0.011270  0.085637  0.058687   \n",
       "4     0.158090  0.010789  0.546413  0.319652  0.083233  0.016363   \n",
       "...        ...       ...       ...       ...       ...       ...   \n",
       "9995  0.051383  0.411699  0.089952  0.282519  0.000132  0.007038   \n",
       "9996  0.028635  0.034902  0.292674  0.112189  0.008453  0.048728   \n",
       "9997  0.661938  0.006536  0.265005  0.174313  0.000492  0.299318   \n",
       "9998  0.043793  0.260402  0.007712  0.034186  0.000000  0.067226   \n",
       "9999  0.155390  0.008744  0.136618  0.080729  0.018973  0.069819   \n",
       "\n",
       "      category_class  \n",
       "0                338  \n",
       "1                338  \n",
       "2                338  \n",
       "3                338  \n",
       "4                338  \n",
       "...              ...  \n",
       "9995             591  \n",
       "9996             591  \n",
       "9997             591  \n",
       "9998             591  \n",
       "9999             591  \n",
       "\n",
       "[10000 rows x 2049 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VSYcdpJUWwaD"
   },
   "outputs": [],
   "source": [
    "X_train=train_dataset.iloc[:,:-1].values\n",
    "y_train=train_dataset.iloc[:,-1].values\n",
    "train_dataset1=[]\n",
    "for i in range(train_dataset.shape[0]):\n",
    "  train_dataset1.append([X_train[i],y_train[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dj19tbhGYRXH"
   },
   "outputs": [],
   "source": [
    "X_val=val_dataset.iloc[:,:-1].values\n",
    "y_val=val_dataset.iloc[:,-1].values\n",
    "val_dataset1=[]\n",
    "for i in range(val_dataset.shape[0]):\n",
    "  val_dataset1.append([X_val[i],y_val[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "J3_IExOUYexg",
    "outputId": "ee5ccf7e-6c2f-4655-cece-342be519b08a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "40000\n"
     ]
    }
   ],
   "source": [
    "print(len(val_dataset1))\n",
    "print(len(train_dataset1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pp95Mdx4Q4t5"
   },
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    # This class will facilitate the creation of a few-shot dataset\n",
    "    # from the Omniglot dataset that can be sampled from quickly while also\n",
    "    # allowing to create new labels at the same time.\n",
    "    def __init__(self, training):\n",
    "        # Download the tfrecord files containing the omniglot data and convert to a\n",
    "        # dataset.\n",
    "        split = \"train\" if training else \"test\"\n",
    "        if(split==\"train\"):\n",
    "          ds=train_dataset1\n",
    "        else:\n",
    "          ds=val_dataset1\n",
    "        # Iterate over the dataset to get each individual image and its class,\n",
    "        # and put that data into a dictionary.\n",
    "        self.data = {}\n",
    "\n",
    "        for image, label in ds:\n",
    "            image = np.array(image)\n",
    "            label = str(np.array(label))\n",
    "            if label not in self.data:\n",
    "                self.data[label] = []\n",
    "            self.data[label].append(image)\n",
    "        self.labels = list(self.data.keys())\n",
    "\n",
    "    def get_mini_dataset(\n",
    "        self, batch_size, repetitions, shots, num_classes, split=False\n",
    "    ):\n",
    "        temp_labels = np.zeros(shape=(num_classes * shots))\n",
    "        temp_images = np.zeros(shape=(num_classes * shots, 2048))\n",
    "        if split:\n",
    "            test_labels = np.zeros(shape=(num_classes))\n",
    "            test_images = np.zeros(shape=(num_classes, 2048))\n",
    "\n",
    "        # Get a random subset of labels from the entire label set.\n",
    "        label_subset = random.choices(self.labels, k=num_classes)\n",
    "        for class_idx, class_obj in enumerate(label_subset):\n",
    "            # Use enumerated index value as a temporary label for mini-batch in\n",
    "            # few shot learning.\n",
    "            temp_labels[class_idx * shots : (class_idx + 1) * shots] = class_idx\n",
    "            # If creating a split dataset for testing, select an extra sample from each\n",
    "            # label to create the test dataset.\n",
    "            if split:\n",
    "                test_labels[class_idx] = class_idx\n",
    "                images_to_split = random.choices(\n",
    "                    self.data[label_subset[class_idx]], k=shots + 1\n",
    "                )\n",
    "                test_images[class_idx] = images_to_split[-1]\n",
    "                temp_images[\n",
    "                    class_idx * shots : (class_idx + 1) * shots\n",
    "                ] = images_to_split[:-1]\n",
    "            else:\n",
    "                # For each index in the randomly selected label_subset, sample the\n",
    "                # necessary number of images.\n",
    "                temp_images[\n",
    "                    class_idx * shots : (class_idx + 1) * shots\n",
    "                ] = random.choices(self.data[label_subset[class_idx]], k=shots)\n",
    "\n",
    "        dataset = tf.data.Dataset.from_tensor_slices(\n",
    "            (temp_images.astype(np.float32), temp_labels.astype(np.int32))\n",
    "        )\n",
    "        dataset = dataset.shuffle(100).batch(batch_size).repeat(repetitions)\n",
    "        if split:\n",
    "            return dataset, test_images, test_labels\n",
    "        return dataset\n",
    "\n",
    "\n",
    "import urllib3\n",
    "\n",
    "urllib3.disable_warnings()  # Disable SSL warnings that may happen during download.\n",
    "train_dataset2 = Dataset(training=True)\n",
    "val_dataset2 = Dataset(training=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LwOrJa1vY7lW",
    "outputId": "e40bbf6c-3da4-4c49-fe12-0b760ac5f479"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.Dataset at 0x7fc214b63e50>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9Ttdrr0PY-6g",
    "outputId": "00b558d9-0575-4bfe-8d69-05b10e007736"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.Dataset at 0x7fc21c91ac90>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_dataset2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kh2eA2_buwNZ"
   },
   "outputs": [],
   "source": [
    "input = layers.Input((2048))\n",
    "x = tf.keras.layers.BatchNormalization()(input)\n",
    "x=layers.Dropout(0.2)(x)\n",
    "x = layers.Dense(1000)(x)\n",
    "x = tf.keras.layers.BatchNormalization()(x)\n",
    "x = layers.Dense(classes, activation=\"softmax\")(x)\n",
    "model = keras.Model(input, x)\n",
    "\n",
    "model.compile()\n",
    "optimizer = keras.optimizers.SGD(learning_rate=learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JYurFvTZuwNZ",
    "outputId": "6aab1810-ea0d-4cee-b27a-caa5e41f2efa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0: train=1.000000 test=1.000000\n",
      "batch 100: train=1.000000 test=0.800000\n",
      "batch 200: train=0.600000 test=0.800000\n",
      "batch 300: train=1.000000 test=1.000000\n",
      "batch 400: train=0.800000 test=1.000000\n",
      "batch 500: train=1.000000 test=1.000000\n",
      "batch 600: train=1.000000 test=1.000000\n",
      "batch 700: train=0.600000 test=1.000000\n",
      "batch 800: train=1.000000 test=0.800000\n",
      "batch 900: train=0.800000 test=0.800000\n",
      "batch 1000: train=1.000000 test=0.800000\n",
      "batch 1100: train=0.800000 test=0.800000\n",
      "batch 1200: train=1.000000 test=0.800000\n",
      "batch 1300: train=1.000000 test=1.000000\n",
      "batch 1400: train=0.800000 test=1.000000\n",
      "batch 1500: train=1.000000 test=1.000000\n",
      "batch 1600: train=1.000000 test=1.000000\n",
      "batch 1700: train=0.600000 test=1.000000\n",
      "batch 1800: train=1.000000 test=0.800000\n",
      "batch 1900: train=0.600000 test=0.800000\n"
     ]
    }
   ],
   "source": [
    "training = []\n",
    "testing = []\n",
    "for meta_iter in range(meta_iters):\n",
    "    frac_done = meta_iter / meta_iters\n",
    "    cur_meta_step_size = (1 - frac_done) * meta_step_size\n",
    "    # Temporarily save the weights from the model.\n",
    "    old_vars = model.get_weights()\n",
    "    # Get a sample from the full dataset.\n",
    "    mini_dataset = train_dataset2.get_mini_dataset(\n",
    "        inner_batch_size, inner_iters, train_shots, classes\n",
    "    )\n",
    "    for images, labels in mini_dataset:\n",
    "        with tf.GradientTape() as tape:\n",
    "            preds = model(images)\n",
    "            loss = keras.losses.sparse_categorical_crossentropy(labels, preds)\n",
    "        grads = tape.gradient(loss, model.trainable_weights)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "    new_vars = model.get_weights()\n",
    "    # Perform SGD for the meta step.\n",
    "    for var in range(len(new_vars)):\n",
    "        new_vars[var] = old_vars[var] + (\n",
    "            (new_vars[var] - old_vars[var]) * cur_meta_step_size\n",
    "        )\n",
    "    # After the meta-learning step, reload the newly-trained weights into the model.\n",
    "    model.set_weights(new_vars)\n",
    "    # Evaluation loop\n",
    "    if meta_iter % eval_interval == 0:\n",
    "        accuracies = []\n",
    "        for dataset in (train_dataset2, val_dataset2):\n",
    "            # Sample a mini dataset from the full dataset.\n",
    "            train_set, test_images, test_labels = dataset.get_mini_dataset(\n",
    "                eval_batch_size, eval_iters, shots, classes, split=True\n",
    "            )\n",
    "            old_vars = model.get_weights()\n",
    "            # Train on the samples and get the resulting accuracies.\n",
    "            for images, labels in train_set:\n",
    "                with tf.GradientTape() as tape:\n",
    "                    preds = model(images)\n",
    "                    loss = keras.losses.sparse_categorical_crossentropy(labels, preds)\n",
    "                grads = tape.gradient(loss, model.trainable_weights)\n",
    "                optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "            test_preds = model.predict(test_images)\n",
    "            test_preds = tf.argmax(test_preds,axis=1).numpy()\n",
    "            num_correct = (test_preds == test_labels).sum()\n",
    "            # Reset the weights after getting the evaluation accuracies.\n",
    "            model.set_weights(old_vars)\n",
    "            accuracies.append(num_correct / classes)\n",
    "        training.append(accuracies[0])\n",
    "        testing.append(accuracies[1])\n",
    "        if meta_iter % 100 == 0:\n",
    "            print(\n",
    "                \"batch %d: train=%f test=%f\" % (meta_iter, accuracies[0], accuracies[1])\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xk_QwlC9zFeh",
    "outputId": "5143c86c-ce33-4ca5-ca36-c59aee3aebbf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 2048)]            0         \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 2048)             8192      \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 2048)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1000)              2049000   \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 1000)             4000      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 5)                 5005      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,066,197\n",
      "Trainable params: 2,060,101\n",
      "Non-trainable params: 6,096\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QRByrhEvv18b",
    "outputId": "b9065fc0-5398-498b-852b-3f9c54fda53a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_1,batch_normalization,dropout,dense,batch_normalization_1,dense_1,"
     ]
    }
   ],
   "source": [
    "for layer in model.layers:\n",
    "  print(layer.name,end=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s-KXX9EbuLGx"
   },
   "outputs": [],
   "source": [
    "last_layer = tf.keras.models.Model(inputs=model.input, outputs=model.get_layer('batch_normalization_1').output) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F2GwFxJVy3iD",
    "outputId": "ce2f142b-a33f-45fc-d3e8-7e145840f22c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 2048)]            0         \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 2048)             8192      \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 2048)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1000)              2049000   \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 1000)             4000      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,061,192\n",
      "Trainable params: 2,055,096\n",
      "Non-trainable params: 6,096\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "last_layer.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tLe9SKlUv-_7"
   },
   "outputs": [],
   "source": [
    "input_1=layers.Input((2048))\n",
    "model1=last_layer(input_1)\n",
    "output_layer = layers.Dense(1000, activation=\"softmax\")(model1)\n",
    "meta_learning_initialisation = keras.Model(inputs=input_1, outputs=output_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6mgv7E18y8Ue",
    "outputId": "6cf9b573-8e3a-40bd-c2fc-98ef9521fb1f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 2048)]            0         \n",
      "                                                                 \n",
      " model_1 (Functional)        (None, 1000)              2061192   \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1000)              1001000   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,062,192\n",
      "Trainable params: 3,056,096\n",
      "Non-trainable params: 6,096\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "meta_learning_initialisation.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y8YjyeX-wW22",
    "outputId": "6acac6f9-ab2a-4cf5-e3ae-86ea9183c8a1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow-addons\n",
      "  Downloading tensorflow_addons-0.16.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
      "\u001b[?25l\r",
      "\u001b[K     |▎                               | 10 kB 24.8 MB/s eta 0:00:01\r",
      "\u001b[K     |▋                               | 20 kB 27.5 MB/s eta 0:00:01\r",
      "\u001b[K     |▉                               | 30 kB 18.0 MB/s eta 0:00:01\r",
      "\u001b[K     |█▏                              | 40 kB 15.9 MB/s eta 0:00:01\r",
      "\u001b[K     |█▌                              | 51 kB 7.1 MB/s eta 0:00:01\r",
      "\u001b[K     |█▊                              | 61 kB 8.3 MB/s eta 0:00:01\r",
      "\u001b[K     |██                              | 71 kB 9.3 MB/s eta 0:00:01\r",
      "\u001b[K     |██▍                             | 81 kB 8.7 MB/s eta 0:00:01\r",
      "\u001b[K     |██▋                             | 92 kB 9.5 MB/s eta 0:00:01\r",
      "\u001b[K     |███                             | 102 kB 7.7 MB/s eta 0:00:01\r",
      "\u001b[K     |███▏                            | 112 kB 7.7 MB/s eta 0:00:01\r",
      "\u001b[K     |███▌                            | 122 kB 7.7 MB/s eta 0:00:01\r",
      "\u001b[K     |███▉                            | 133 kB 7.7 MB/s eta 0:00:01\r",
      "\u001b[K     |████                            | 143 kB 7.7 MB/s eta 0:00:01\r",
      "\u001b[K     |████▍                           | 153 kB 7.7 MB/s eta 0:00:01\r",
      "\u001b[K     |████▊                           | 163 kB 7.7 MB/s eta 0:00:01\r",
      "\u001b[K     |█████                           | 174 kB 7.7 MB/s eta 0:00:01\r",
      "\u001b[K     |█████▎                          | 184 kB 7.7 MB/s eta 0:00:01\r",
      "\u001b[K     |█████▌                          | 194 kB 7.7 MB/s eta 0:00:01\r",
      "\u001b[K     |█████▉                          | 204 kB 7.7 MB/s eta 0:00:01\r",
      "\u001b[K     |██████▏                         | 215 kB 7.7 MB/s eta 0:00:01\r",
      "\u001b[K     |██████▍                         | 225 kB 7.7 MB/s eta 0:00:01\r",
      "\u001b[K     |██████▊                         | 235 kB 7.7 MB/s eta 0:00:01\r",
      "\u001b[K     |███████                         | 245 kB 7.7 MB/s eta 0:00:01\r",
      "\u001b[K     |███████▎                        | 256 kB 7.7 MB/s eta 0:00:01\r",
      "\u001b[K     |███████▋                        | 266 kB 7.7 MB/s eta 0:00:01\r",
      "\u001b[K     |███████▉                        | 276 kB 7.7 MB/s eta 0:00:01\r",
      "\u001b[K     |████████▏                       | 286 kB 7.7 MB/s eta 0:00:01\r",
      "\u001b[K     |████████▌                       | 296 kB 7.7 MB/s eta 0:00:01\r",
      "\u001b[K     |████████▊                       | 307 kB 7.7 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████                       | 317 kB 7.7 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████▍                      | 327 kB 7.7 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████▋                      | 337 kB 7.7 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████                      | 348 kB 7.7 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████▏                     | 358 kB 7.7 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████▌                     | 368 kB 7.7 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████▉                     | 378 kB 7.7 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████                     | 389 kB 7.7 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████▍                    | 399 kB 7.7 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████▊                    | 409 kB 7.7 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████                    | 419 kB 7.7 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████▎                   | 430 kB 7.7 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████▌                   | 440 kB 7.7 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████▉                   | 450 kB 7.7 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████▏                  | 460 kB 7.7 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████▍                  | 471 kB 7.7 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████▊                  | 481 kB 7.7 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████                  | 491 kB 7.7 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████▎                 | 501 kB 7.7 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████▋                 | 512 kB 7.7 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████▉                 | 522 kB 7.7 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████▏                | 532 kB 7.7 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████▌                | 542 kB 7.7 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████▊                | 552 kB 7.7 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████                | 563 kB 7.7 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████▍               | 573 kB 7.7 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████▋               | 583 kB 7.7 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████               | 593 kB 7.7 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████▏              | 604 kB 7.7 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████▌              | 614 kB 7.7 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████▉              | 624 kB 7.7 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████              | 634 kB 7.7 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████▍             | 645 kB 7.7 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████▊             | 655 kB 7.7 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████             | 665 kB 7.7 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████▎            | 675 kB 7.7 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████▌            | 686 kB 7.7 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████▉            | 696 kB 7.7 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████▏           | 706 kB 7.7 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████▍           | 716 kB 7.7 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████▊           | 727 kB 7.7 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████           | 737 kB 7.7 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████▎          | 747 kB 7.7 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████▋          | 757 kB 7.7 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████▉          | 768 kB 7.7 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████▏         | 778 kB 7.7 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████▌         | 788 kB 7.7 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████▊         | 798 kB 7.7 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████         | 808 kB 7.7 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████▍        | 819 kB 7.7 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████▋        | 829 kB 7.7 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████        | 839 kB 7.7 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████▏       | 849 kB 7.7 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████▌       | 860 kB 7.7 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████▉       | 870 kB 7.7 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████       | 880 kB 7.7 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████▍      | 890 kB 7.7 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████▊      | 901 kB 7.7 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████      | 911 kB 7.7 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████▎     | 921 kB 7.7 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████▌     | 931 kB 7.7 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████▉     | 942 kB 7.7 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████▏    | 952 kB 7.7 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████▍    | 962 kB 7.7 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████▊    | 972 kB 7.7 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████    | 983 kB 7.7 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████▎   | 993 kB 7.7 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████▋   | 1.0 MB 7.7 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████▉   | 1.0 MB 7.7 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████▏  | 1.0 MB 7.7 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████▌  | 1.0 MB 7.7 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████▊  | 1.0 MB 7.7 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████  | 1.1 MB 7.7 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████▍ | 1.1 MB 7.7 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████▋ | 1.1 MB 7.7 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████ | 1.1 MB 7.7 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▏| 1.1 MB 7.7 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▌| 1.1 MB 7.7 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▉| 1.1 MB 7.7 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 1.1 MB 7.7 MB/s \n",
      "\u001b[?25hRequirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow-addons) (2.7.1)\n",
      "Installing collected packages: tensorflow-addons\n",
      "Successfully installed tensorflow-addons-0.16.1\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow-addons\n",
    "import tensorflow_addons as tfa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kazX9j2azYLr"
   },
   "outputs": [],
   "source": [
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(\"./drive/MyDrive/reptiles_embeddings1.h5\",\n",
    "                             monitor=\"val_accuracy\",\n",
    "                             mode=\"max\",\n",
    "                             save_best_only = True,\n",
    "                             verbose=1)\n",
    "\n",
    "\n",
    "earlystop = tf.keras.callbacks.EarlyStopping(monitor = 'val_accuracy', # value being monitored for improvement\n",
    "                          min_delta = 0, #Abs value and is the min change required before we stop\n",
    "                          patience = 50, #Number of epochs we wait before stopping \n",
    "                          verbose = 1,\n",
    "                          restore_best_weights = True) #keeps the best weigths once stopped\n",
    "def scheduler(epoch, lr):\n",
    "  return(0.1*(0.1 ** (epoch // 30)))\n",
    "\n",
    "lr_scheduler=tf.keras.callbacks.LearningRateScheduler(scheduler,verbose=1)\n",
    "# we put our call backs into a callback list\n",
    "callbacks = [earlystop,checkpoint,lr_scheduler]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qNLZy8TwwacF"
   },
   "outputs": [],
   "source": [
    "meta_learning_initialisation.compile(optimizer= tfa.optimizers.SGDW(learning_rate=0.1, momentum=0.9, nesterov=False,weight_decay=1e-4), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YByYvToOzoyx"
   },
   "outputs": [],
   "source": [
    "#onehotencoding\n",
    "y_train1=np.eye(1000)[y_train]\n",
    "y_val1=np.eye(1000)[y_val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6-W0uf0yzqbc",
    "outputId": "c0844de7-cf69-4d03-fbac-2ea1059974d1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: LearningRateScheduler setting learning rate to 0.1.\n",
      "Epoch 1/100\n",
      "1243/1250 [============================>.] - ETA: 0s - loss: 3.9303 - accuracy: 0.2499\n",
      "Epoch 1: val_accuracy improved from -inf to 0.39910, saving model to ./drive/MyDrive/reptiles_embeddings1.h5\n",
      "1250/1250 [==============================] - 7s 5ms/step - loss: 3.9258 - accuracy: 0.2507 - val_loss: 2.5650 - val_accuracy: 0.3991 - lr: 0.1000\n",
      "\n",
      "Epoch 2: LearningRateScheduler setting learning rate to 0.1.\n",
      "Epoch 2/100\n",
      "1248/1250 [============================>.] - ETA: 0s - loss: 2.6118 - accuracy: 0.4095\n",
      "Epoch 2: val_accuracy improved from 0.39910 to 0.43600, saving model to ./drive/MyDrive/reptiles_embeddings1.h5\n",
      "1250/1250 [==============================] - 8s 6ms/step - loss: 2.6123 - accuracy: 0.4093 - val_loss: 2.3838 - val_accuracy: 0.4360 - lr: 0.1000\n",
      "\n",
      "Epoch 3: LearningRateScheduler setting learning rate to 0.1.\n",
      "Epoch 3/100\n",
      "1244/1250 [============================>.] - ETA: 0s - loss: 2.3614 - accuracy: 0.4519\n",
      "Epoch 3: val_accuracy improved from 0.43600 to 0.45230, saving model to ./drive/MyDrive/reptiles_embeddings1.h5\n",
      "1250/1250 [==============================] - 8s 6ms/step - loss: 2.3621 - accuracy: 0.4515 - val_loss: 2.2992 - val_accuracy: 0.4523 - lr: 0.1000\n",
      "\n",
      "Epoch 4: LearningRateScheduler setting learning rate to 0.1.\n",
      "Epoch 4/100\n",
      "1244/1250 [============================>.] - ETA: 0s - loss: 2.2323 - accuracy: 0.4772\n",
      "Epoch 4: val_accuracy improved from 0.45230 to 0.45700, saving model to ./drive/MyDrive/reptiles_embeddings1.h5\n",
      "1250/1250 [==============================] - 8s 6ms/step - loss: 2.2319 - accuracy: 0.4773 - val_loss: 2.2840 - val_accuracy: 0.4570 - lr: 0.1000\n",
      "\n",
      "Epoch 5: LearningRateScheduler setting learning rate to 0.1.\n",
      "Epoch 5/100\n",
      "1242/1250 [============================>.] - ETA: 0s - loss: 2.1397 - accuracy: 0.4918\n",
      "Epoch 5: val_accuracy improved from 0.45700 to 0.45930, saving model to ./drive/MyDrive/reptiles_embeddings1.h5\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 2.1413 - accuracy: 0.4915 - val_loss: 2.2836 - val_accuracy: 0.4593 - lr: 0.1000\n",
      "\n",
      "Epoch 6: LearningRateScheduler setting learning rate to 0.1.\n",
      "Epoch 6/100\n",
      "1238/1250 [============================>.] - ETA: 0s - loss: 2.0907 - accuracy: 0.5029\n",
      "Epoch 6: val_accuracy improved from 0.45930 to 0.46220, saving model to ./drive/MyDrive/reptiles_embeddings1.h5\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 2.0930 - accuracy: 0.5021 - val_loss: 2.2845 - val_accuracy: 0.4622 - lr: 0.1000\n",
      "\n",
      "Epoch 7: LearningRateScheduler setting learning rate to 0.1.\n",
      "Epoch 7/100\n",
      "1240/1250 [============================>.] - ETA: 0s - loss: 2.0563 - accuracy: 0.5062\n",
      "Epoch 7: val_accuracy improved from 0.46220 to 0.46500, saving model to ./drive/MyDrive/reptiles_embeddings1.h5\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 2.0574 - accuracy: 0.5062 - val_loss: 2.2882 - val_accuracy: 0.4650 - lr: 0.1000\n",
      "\n",
      "Epoch 8: LearningRateScheduler setting learning rate to 0.1.\n",
      "Epoch 8/100\n",
      "1238/1250 [============================>.] - ETA: 0s - loss: 2.0241 - accuracy: 0.5119\n",
      "Epoch 8: val_accuracy did not improve from 0.46500\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 2.0261 - accuracy: 0.5117 - val_loss: 2.2745 - val_accuracy: 0.4636 - lr: 0.1000\n",
      "\n",
      "Epoch 9: LearningRateScheduler setting learning rate to 0.1.\n",
      "Epoch 9/100\n",
      "1248/1250 [============================>.] - ETA: 0s - loss: 2.0017 - accuracy: 0.5148\n",
      "Epoch 9: val_accuracy did not improve from 0.46500\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 2.0022 - accuracy: 0.5148 - val_loss: 2.2961 - val_accuracy: 0.4618 - lr: 0.1000\n",
      "\n",
      "Epoch 10: LearningRateScheduler setting learning rate to 0.1.\n",
      "Epoch 10/100\n",
      "1242/1250 [============================>.] - ETA: 0s - loss: 1.9875 - accuracy: 0.5141\n",
      "Epoch 10: val_accuracy did not improve from 0.46500\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.9885 - accuracy: 0.5141 - val_loss: 2.3273 - val_accuracy: 0.4576 - lr: 0.1000\n",
      "\n",
      "Epoch 11: LearningRateScheduler setting learning rate to 0.1.\n",
      "Epoch 11/100\n",
      "1250/1250 [==============================] - ETA: 0s - loss: 1.9715 - accuracy: 0.5225\n",
      "Epoch 11: val_accuracy did not improve from 0.46500\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.9715 - accuracy: 0.5225 - val_loss: 2.3087 - val_accuracy: 0.4607 - lr: 0.1000\n",
      "\n",
      "Epoch 12: LearningRateScheduler setting learning rate to 0.1.\n",
      "Epoch 12/100\n",
      "1235/1250 [============================>.] - ETA: 0s - loss: 1.9620 - accuracy: 0.5206\n",
      "Epoch 12: val_accuracy did not improve from 0.46500\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.9653 - accuracy: 0.5198 - val_loss: 2.2849 - val_accuracy: 0.4634 - lr: 0.1000\n",
      "\n",
      "Epoch 13: LearningRateScheduler setting learning rate to 0.1.\n",
      "Epoch 13/100\n",
      "1236/1250 [============================>.] - ETA: 0s - loss: 1.9494 - accuracy: 0.5234\n",
      "Epoch 13: val_accuracy did not improve from 0.46500\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.9519 - accuracy: 0.5229 - val_loss: 2.3174 - val_accuracy: 0.4628 - lr: 0.1000\n",
      "\n",
      "Epoch 14: LearningRateScheduler setting learning rate to 0.1.\n",
      "Epoch 14/100\n",
      "1240/1250 [============================>.] - ETA: 0s - loss: 1.9355 - accuracy: 0.5261\n",
      "Epoch 14: val_accuracy improved from 0.46500 to 0.46940, saving model to ./drive/MyDrive/reptiles_embeddings1.h5\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 1.9380 - accuracy: 0.5253 - val_loss: 2.3015 - val_accuracy: 0.4694 - lr: 0.1000\n",
      "\n",
      "Epoch 15: LearningRateScheduler setting learning rate to 0.1.\n",
      "Epoch 15/100\n",
      "1246/1250 [============================>.] - ETA: 0s - loss: 1.9387 - accuracy: 0.5257\n",
      "Epoch 15: val_accuracy did not improve from 0.46940\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.9399 - accuracy: 0.5256 - val_loss: 2.2862 - val_accuracy: 0.4670 - lr: 0.1000\n",
      "\n",
      "Epoch 16: LearningRateScheduler setting learning rate to 0.1.\n",
      "Epoch 16/100\n",
      "1250/1250 [==============================] - ETA: 0s - loss: 1.9278 - accuracy: 0.5293\n",
      "Epoch 16: val_accuracy did not improve from 0.46940\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.9278 - accuracy: 0.5293 - val_loss: 2.2997 - val_accuracy: 0.4669 - lr: 0.1000\n",
      "\n",
      "Epoch 17: LearningRateScheduler setting learning rate to 0.1.\n",
      "Epoch 17/100\n",
      "1243/1250 [============================>.] - ETA: 0s - loss: 1.9209 - accuracy: 0.5257\n",
      "Epoch 17: val_accuracy did not improve from 0.46940\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.9224 - accuracy: 0.5256 - val_loss: 2.3133 - val_accuracy: 0.4617 - lr: 0.1000\n",
      "\n",
      "Epoch 18: LearningRateScheduler setting learning rate to 0.1.\n",
      "Epoch 18/100\n",
      "1235/1250 [============================>.] - ETA: 0s - loss: 1.9233 - accuracy: 0.5277\n",
      "Epoch 18: val_accuracy did not improve from 0.46940\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.9273 - accuracy: 0.5265 - val_loss: 2.3565 - val_accuracy: 0.4540 - lr: 0.1000\n",
      "\n",
      "Epoch 19: LearningRateScheduler setting learning rate to 0.1.\n",
      "Epoch 19/100\n",
      "1234/1250 [============================>.] - ETA: 0s - loss: 1.9153 - accuracy: 0.5312\n",
      "Epoch 19: val_accuracy did not improve from 0.46940\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 1.9190 - accuracy: 0.5303 - val_loss: 2.3097 - val_accuracy: 0.4618 - lr: 0.1000\n",
      "\n",
      "Epoch 20: LearningRateScheduler setting learning rate to 0.1.\n",
      "Epoch 20/100\n",
      "1247/1250 [============================>.] - ETA: 0s - loss: 1.9207 - accuracy: 0.5293\n",
      "Epoch 20: val_accuracy did not improve from 0.46940\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.9211 - accuracy: 0.5294 - val_loss: 2.2769 - val_accuracy: 0.4627 - lr: 0.1000\n",
      "\n",
      "Epoch 21: LearningRateScheduler setting learning rate to 0.1.\n",
      "Epoch 21/100\n",
      "1244/1250 [============================>.] - ETA: 0s - loss: 1.9119 - accuracy: 0.5304\n",
      "Epoch 21: val_accuracy did not improve from 0.46940\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.9138 - accuracy: 0.5300 - val_loss: 2.3044 - val_accuracy: 0.4671 - lr: 0.1000\n",
      "\n",
      "Epoch 22: LearningRateScheduler setting learning rate to 0.1.\n",
      "Epoch 22/100\n",
      "1244/1250 [============================>.] - ETA: 0s - loss: 1.9046 - accuracy: 0.5322\n",
      "Epoch 22: val_accuracy did not improve from 0.46940\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.9052 - accuracy: 0.5321 - val_loss: 2.3348 - val_accuracy: 0.4535 - lr: 0.1000\n",
      "\n",
      "Epoch 23: LearningRateScheduler setting learning rate to 0.1.\n",
      "Epoch 23/100\n",
      "1243/1250 [============================>.] - ETA: 0s - loss: 1.9079 - accuracy: 0.5312\n",
      "Epoch 23: val_accuracy did not improve from 0.46940\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.9094 - accuracy: 0.5308 - val_loss: 2.3209 - val_accuracy: 0.4591 - lr: 0.1000\n",
      "\n",
      "Epoch 24: LearningRateScheduler setting learning rate to 0.1.\n",
      "Epoch 24/100\n",
      "1239/1250 [============================>.] - ETA: 0s - loss: 1.9110 - accuracy: 0.5314\n",
      "Epoch 24: val_accuracy did not improve from 0.46940\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.9117 - accuracy: 0.5313 - val_loss: 2.3327 - val_accuracy: 0.4593 - lr: 0.1000\n",
      "\n",
      "Epoch 25: LearningRateScheduler setting learning rate to 0.1.\n",
      "Epoch 25/100\n",
      "1247/1250 [============================>.] - ETA: 0s - loss: 1.9093 - accuracy: 0.5286\n",
      "Epoch 25: val_accuracy did not improve from 0.46940\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.9100 - accuracy: 0.5286 - val_loss: 2.3360 - val_accuracy: 0.4617 - lr: 0.1000\n",
      "\n",
      "Epoch 26: LearningRateScheduler setting learning rate to 0.1.\n",
      "Epoch 26/100\n",
      "1242/1250 [============================>.] - ETA: 0s - loss: 1.9038 - accuracy: 0.5310\n",
      "Epoch 26: val_accuracy did not improve from 0.46940\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.9050 - accuracy: 0.5308 - val_loss: 2.3161 - val_accuracy: 0.4636 - lr: 0.1000\n",
      "\n",
      "Epoch 27: LearningRateScheduler setting learning rate to 0.1.\n",
      "Epoch 27/100\n",
      "1238/1250 [============================>.] - ETA: 0s - loss: 1.8945 - accuracy: 0.5338\n",
      "Epoch 27: val_accuracy did not improve from 0.46940\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.8969 - accuracy: 0.5334 - val_loss: 2.3357 - val_accuracy: 0.4608 - lr: 0.1000\n",
      "\n",
      "Epoch 28: LearningRateScheduler setting learning rate to 0.1.\n",
      "Epoch 28/100\n",
      "1247/1250 [============================>.] - ETA: 0s - loss: 1.9012 - accuracy: 0.5332\n",
      "Epoch 28: val_accuracy did not improve from 0.46940\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.9014 - accuracy: 0.5329 - val_loss: 2.3146 - val_accuracy: 0.4623 - lr: 0.1000\n",
      "\n",
      "Epoch 29: LearningRateScheduler setting learning rate to 0.1.\n",
      "Epoch 29/100\n",
      "1241/1250 [============================>.] - ETA: 0s - loss: 1.8988 - accuracy: 0.5351\n",
      "Epoch 29: val_accuracy did not improve from 0.46940\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.9003 - accuracy: 0.5352 - val_loss: 2.2820 - val_accuracy: 0.4644 - lr: 0.1000\n",
      "\n",
      "Epoch 30: LearningRateScheduler setting learning rate to 0.1.\n",
      "Epoch 30/100\n",
      "1244/1250 [============================>.] - ETA: 0s - loss: 1.8972 - accuracy: 0.5304\n",
      "Epoch 30: val_accuracy did not improve from 0.46940\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.8991 - accuracy: 0.5302 - val_loss: 2.3386 - val_accuracy: 0.4602 - lr: 0.1000\n",
      "\n",
      "Epoch 31: LearningRateScheduler setting learning rate to 0.010000000000000002.\n",
      "Epoch 31/100\n",
      "1250/1250 [==============================] - ETA: 0s - loss: 1.2471 - accuracy: 0.6862\n",
      "Epoch 31: val_accuracy improved from 0.46940 to 0.53400, saving model to ./drive/MyDrive/reptiles_embeddings1.h5\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.2471 - accuracy: 0.6862 - val_loss: 1.9763 - val_accuracy: 0.5340 - lr: 0.0100\n",
      "\n",
      "Epoch 32: LearningRateScheduler setting learning rate to 0.010000000000000002.\n",
      "Epoch 32/100\n",
      "1243/1250 [============================>.] - ETA: 0s - loss: 1.0845 - accuracy: 0.7268\n",
      "Epoch 32: val_accuracy did not improve from 0.53400\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.0845 - accuracy: 0.7266 - val_loss: 1.9553 - val_accuracy: 0.5333 - lr: 0.0100\n",
      "\n",
      "Epoch 33: LearningRateScheduler setting learning rate to 0.010000000000000002.\n",
      "Epoch 33/100\n",
      "1249/1250 [============================>.] - ETA: 0s - loss: 1.0470 - accuracy: 0.7372\n",
      "Epoch 33: val_accuracy improved from 0.53400 to 0.53740, saving model to ./drive/MyDrive/reptiles_embeddings1.h5\n",
      "1250/1250 [==============================] - 6s 4ms/step - loss: 1.0470 - accuracy: 0.7371 - val_loss: 1.9466 - val_accuracy: 0.5374 - lr: 0.0100\n",
      "\n",
      "Epoch 34: LearningRateScheduler setting learning rate to 0.010000000000000002.\n",
      "Epoch 34/100\n",
      "1236/1250 [============================>.] - ETA: 0s - loss: 1.0285 - accuracy: 0.7424\n",
      "Epoch 34: val_accuracy did not improve from 0.53740\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.0291 - accuracy: 0.7424 - val_loss: 1.9509 - val_accuracy: 0.5355 - lr: 0.0100\n",
      "\n",
      "Epoch 35: LearningRateScheduler setting learning rate to 0.010000000000000002.\n",
      "Epoch 35/100\n",
      "1234/1250 [============================>.] - ETA: 0s - loss: 1.0477 - accuracy: 0.7391\n",
      "Epoch 35: val_accuracy did not improve from 0.53740\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.0482 - accuracy: 0.7392 - val_loss: 1.9547 - val_accuracy: 0.5360 - lr: 0.0100\n",
      "\n",
      "Epoch 36: LearningRateScheduler setting learning rate to 0.010000000000000002.\n",
      "Epoch 36/100\n",
      "1242/1250 [============================>.] - ETA: 0s - loss: 1.0727 - accuracy: 0.7360\n",
      "Epoch 36: val_accuracy did not improve from 0.53740\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.0740 - accuracy: 0.7355 - val_loss: 1.9815 - val_accuracy: 0.5279 - lr: 0.0100\n",
      "\n",
      "Epoch 37: LearningRateScheduler setting learning rate to 0.010000000000000002.\n",
      "Epoch 37/100\n",
      "1245/1250 [============================>.] - ETA: 0s - loss: 1.1288 - accuracy: 0.7221\n",
      "Epoch 37: val_accuracy did not improve from 0.53740\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.1297 - accuracy: 0.7220 - val_loss: 1.9843 - val_accuracy: 0.5292 - lr: 0.0100\n",
      "\n",
      "Epoch 38: LearningRateScheduler setting learning rate to 0.010000000000000002.\n",
      "Epoch 38/100\n",
      "1235/1250 [============================>.] - ETA: 0s - loss: 1.1792 - accuracy: 0.7091\n",
      "Epoch 38: val_accuracy did not improve from 0.53740\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.1811 - accuracy: 0.7085 - val_loss: 2.0136 - val_accuracy: 0.5202 - lr: 0.0100\n",
      "\n",
      "Epoch 39: LearningRateScheduler setting learning rate to 0.010000000000000002.\n",
      "Epoch 39/100\n",
      "1244/1250 [============================>.] - ETA: 0s - loss: 1.2426 - accuracy: 0.6923\n",
      "Epoch 39: val_accuracy did not improve from 0.53740\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.2430 - accuracy: 0.6920 - val_loss: 2.0246 - val_accuracy: 0.5202 - lr: 0.0100\n",
      "\n",
      "Epoch 40: LearningRateScheduler setting learning rate to 0.010000000000000002.\n",
      "Epoch 40/100\n",
      "1239/1250 [============================>.] - ETA: 0s - loss: 1.3082 - accuracy: 0.6757\n",
      "Epoch 40: val_accuracy did not improve from 0.53740\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.3098 - accuracy: 0.6752 - val_loss: 2.0501 - val_accuracy: 0.5124 - lr: 0.0100\n",
      "\n",
      "Epoch 41: LearningRateScheduler setting learning rate to 0.010000000000000002.\n",
      "Epoch 41/100\n",
      "1247/1250 [============================>.] - ETA: 0s - loss: 1.3710 - accuracy: 0.6594\n",
      "Epoch 41: val_accuracy did not improve from 0.53740\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.3711 - accuracy: 0.6594 - val_loss: 2.0524 - val_accuracy: 0.5104 - lr: 0.0100\n",
      "\n",
      "Epoch 42: LearningRateScheduler setting learning rate to 0.010000000000000002.\n",
      "Epoch 42/100\n",
      "1235/1250 [============================>.] - ETA: 0s - loss: 1.4115 - accuracy: 0.6488\n",
      "Epoch 42: val_accuracy did not improve from 0.53740\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.4143 - accuracy: 0.6483 - val_loss: 2.0651 - val_accuracy: 0.5073 - lr: 0.0100\n",
      "\n",
      "Epoch 43: LearningRateScheduler setting learning rate to 0.010000000000000002.\n",
      "Epoch 43/100\n",
      "1243/1250 [============================>.] - ETA: 0s - loss: 1.4560 - accuracy: 0.6417\n",
      "Epoch 43: val_accuracy did not improve from 0.53740\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.4562 - accuracy: 0.6417 - val_loss: 2.0659 - val_accuracy: 0.5090 - lr: 0.0100\n",
      "\n",
      "Epoch 44: LearningRateScheduler setting learning rate to 0.010000000000000002.\n",
      "Epoch 44/100\n",
      "1249/1250 [============================>.] - ETA: 0s - loss: 1.4805 - accuracy: 0.6365\n",
      "Epoch 44: val_accuracy did not improve from 0.53740\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.4808 - accuracy: 0.6364 - val_loss: 2.0605 - val_accuracy: 0.5075 - lr: 0.0100\n",
      "\n",
      "Epoch 45: LearningRateScheduler setting learning rate to 0.010000000000000002.\n",
      "Epoch 45/100\n",
      "1248/1250 [============================>.] - ETA: 0s - loss: 1.5129 - accuracy: 0.6279\n",
      "Epoch 45: val_accuracy did not improve from 0.53740\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.5133 - accuracy: 0.6279 - val_loss: 2.0861 - val_accuracy: 0.4982 - lr: 0.0100\n",
      "\n",
      "Epoch 46: LearningRateScheduler setting learning rate to 0.010000000000000002.\n",
      "Epoch 46/100\n",
      "1239/1250 [============================>.] - ETA: 0s - loss: 1.5309 - accuracy: 0.6237\n",
      "Epoch 46: val_accuracy did not improve from 0.53740\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.5332 - accuracy: 0.6232 - val_loss: 2.0716 - val_accuracy: 0.5023 - lr: 0.0100\n",
      "\n",
      "Epoch 47: LearningRateScheduler setting learning rate to 0.010000000000000002.\n",
      "Epoch 47/100\n",
      "1249/1250 [============================>.] - ETA: 0s - loss: 1.5517 - accuracy: 0.6199\n",
      "Epoch 47: val_accuracy did not improve from 0.53740\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.5520 - accuracy: 0.6199 - val_loss: 2.0758 - val_accuracy: 0.5012 - lr: 0.0100\n",
      "\n",
      "Epoch 48: LearningRateScheduler setting learning rate to 0.010000000000000002.\n",
      "Epoch 48/100\n",
      "1234/1250 [============================>.] - ETA: 0s - loss: 1.5636 - accuracy: 0.6211\n",
      "Epoch 48: val_accuracy did not improve from 0.53740\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.5666 - accuracy: 0.6202 - val_loss: 2.0718 - val_accuracy: 0.5036 - lr: 0.0100\n",
      "\n",
      "Epoch 49: LearningRateScheduler setting learning rate to 0.010000000000000002.\n",
      "Epoch 49/100\n",
      "1240/1250 [============================>.] - ETA: 0s - loss: 1.5612 - accuracy: 0.6176\n",
      "Epoch 49: val_accuracy did not improve from 0.53740\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.5635 - accuracy: 0.6173 - val_loss: 2.0681 - val_accuracy: 0.5045 - lr: 0.0100\n",
      "\n",
      "Epoch 50: LearningRateScheduler setting learning rate to 0.010000000000000002.\n",
      "Epoch 50/100\n",
      "1240/1250 [============================>.] - ETA: 0s - loss: 1.5797 - accuracy: 0.6137\n",
      "Epoch 50: val_accuracy did not improve from 0.53740\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.5816 - accuracy: 0.6133 - val_loss: 2.0582 - val_accuracy: 0.5092 - lr: 0.0100\n",
      "\n",
      "Epoch 51: LearningRateScheduler setting learning rate to 0.010000000000000002.\n",
      "Epoch 51/100\n",
      "1236/1250 [============================>.] - ETA: 0s - loss: 1.5739 - accuracy: 0.6166\n",
      "Epoch 51: val_accuracy did not improve from 0.53740\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.5766 - accuracy: 0.6158 - val_loss: 2.0562 - val_accuracy: 0.5082 - lr: 0.0100\n",
      "\n",
      "Epoch 52: LearningRateScheduler setting learning rate to 0.010000000000000002.\n",
      "Epoch 52/100\n",
      "1237/1250 [============================>.] - ETA: 0s - loss: 1.5913 - accuracy: 0.6129\n",
      "Epoch 52: val_accuracy did not improve from 0.53740\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.5939 - accuracy: 0.6122 - val_loss: 2.0859 - val_accuracy: 0.5034 - lr: 0.0100\n",
      "\n",
      "Epoch 53: LearningRateScheduler setting learning rate to 0.010000000000000002.\n",
      "Epoch 53/100\n",
      "1249/1250 [============================>.] - ETA: 0s - loss: 1.5908 - accuracy: 0.6116\n",
      "Epoch 53: val_accuracy did not improve from 0.53740\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.5907 - accuracy: 0.6116 - val_loss: 2.0614 - val_accuracy: 0.5106 - lr: 0.0100\n",
      "\n",
      "Epoch 54: LearningRateScheduler setting learning rate to 0.010000000000000002.\n",
      "Epoch 54/100\n",
      "1249/1250 [============================>.] - ETA: 0s - loss: 1.5909 - accuracy: 0.6124\n",
      "Epoch 54: val_accuracy did not improve from 0.53740\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.5909 - accuracy: 0.6124 - val_loss: 2.0581 - val_accuracy: 0.5078 - lr: 0.0100\n",
      "\n",
      "Epoch 55: LearningRateScheduler setting learning rate to 0.010000000000000002.\n",
      "Epoch 55/100\n",
      "1242/1250 [============================>.] - ETA: 0s - loss: 1.5906 - accuracy: 0.6147\n",
      "Epoch 55: val_accuracy did not improve from 0.53740\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.5926 - accuracy: 0.6144 - val_loss: 2.0793 - val_accuracy: 0.5012 - lr: 0.0100\n",
      "\n",
      "Epoch 56: LearningRateScheduler setting learning rate to 0.010000000000000002.\n",
      "Epoch 56/100\n",
      "1245/1250 [============================>.] - ETA: 0s - loss: 1.5932 - accuracy: 0.6121\n",
      "Epoch 56: val_accuracy did not improve from 0.53740\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.5937 - accuracy: 0.6121 - val_loss: 2.0653 - val_accuracy: 0.5023 - lr: 0.0100\n",
      "\n",
      "Epoch 57: LearningRateScheduler setting learning rate to 0.010000000000000002.\n",
      "Epoch 57/100\n",
      "1246/1250 [============================>.] - ETA: 0s - loss: 1.6059 - accuracy: 0.6112\n",
      "Epoch 57: val_accuracy did not improve from 0.53740\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.6057 - accuracy: 0.6112 - val_loss: 2.0731 - val_accuracy: 0.5039 - lr: 0.0100\n",
      "\n",
      "Epoch 58: LearningRateScheduler setting learning rate to 0.010000000000000002.\n",
      "Epoch 58/100\n",
      "1237/1250 [============================>.] - ETA: 0s - loss: 1.5979 - accuracy: 0.6109\n",
      "Epoch 58: val_accuracy did not improve from 0.53740\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.6019 - accuracy: 0.6102 - val_loss: 2.0819 - val_accuracy: 0.4993 - lr: 0.0100\n",
      "\n",
      "Epoch 59: LearningRateScheduler setting learning rate to 0.010000000000000002.\n",
      "Epoch 59/100\n",
      "1238/1250 [============================>.] - ETA: 0s - loss: 1.6023 - accuracy: 0.6099\n",
      "Epoch 59: val_accuracy did not improve from 0.53740\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.6051 - accuracy: 0.6088 - val_loss: 2.0762 - val_accuracy: 0.5032 - lr: 0.0100\n",
      "\n",
      "Epoch 60: LearningRateScheduler setting learning rate to 0.010000000000000002.\n",
      "Epoch 60/100\n",
      "1245/1250 [============================>.] - ETA: 0s - loss: 1.6008 - accuracy: 0.6117\n",
      "Epoch 60: val_accuracy did not improve from 0.53740\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.6009 - accuracy: 0.6117 - val_loss: 2.0724 - val_accuracy: 0.5056 - lr: 0.0100\n",
      "\n",
      "Epoch 61: LearningRateScheduler setting learning rate to 0.0010000000000000002.\n",
      "Epoch 61/100\n",
      "1247/1250 [============================>.] - ETA: 0s - loss: 1.3682 - accuracy: 0.7130\n",
      "Epoch 61: val_accuracy improved from 0.53740 to 0.55370, saving model to ./drive/MyDrive/reptiles_embeddings1.h5\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.3684 - accuracy: 0.7130 - val_loss: 2.0315 - val_accuracy: 0.5537 - lr: 0.0010\n",
      "\n",
      "Epoch 62: LearningRateScheduler setting learning rate to 0.0010000000000000002.\n",
      "Epoch 62/100\n",
      "1246/1250 [============================>.] - ETA: 0s - loss: 1.5741 - accuracy: 0.7354\n",
      "Epoch 62: val_accuracy improved from 0.55370 to 0.55390, saving model to ./drive/MyDrive/reptiles_embeddings1.h5\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.5748 - accuracy: 0.7354 - val_loss: 2.1994 - val_accuracy: 0.5539 - lr: 0.0010\n",
      "\n",
      "Epoch 63: LearningRateScheduler setting learning rate to 0.0010000000000000002.\n",
      "Epoch 63/100\n",
      "1235/1250 [============================>.] - ETA: 0s - loss: 1.8725 - accuracy: 0.7281\n",
      "Epoch 63: val_accuracy did not improve from 0.55390\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.8746 - accuracy: 0.7280 - val_loss: 2.3887 - val_accuracy: 0.5524 - lr: 0.0010\n",
      "\n",
      "Epoch 64: LearningRateScheduler setting learning rate to 0.0010000000000000002.\n",
      "Epoch 64/100\n",
      "1250/1250 [==============================] - ETA: 0s - loss: 2.1527 - accuracy: 0.7148\n",
      "Epoch 64: val_accuracy did not improve from 0.55390\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 2.1527 - accuracy: 0.7148 - val_loss: 2.5602 - val_accuracy: 0.5519 - lr: 0.0010\n",
      "\n",
      "Epoch 65: LearningRateScheduler setting learning rate to 0.0010000000000000002.\n",
      "Epoch 65/100\n",
      "1239/1250 [============================>.] - ETA: 0s - loss: 2.3934 - accuracy: 0.6962\n",
      "Epoch 65: val_accuracy did not improve from 0.55390\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 2.3946 - accuracy: 0.6962 - val_loss: 2.7074 - val_accuracy: 0.5468 - lr: 0.0010\n",
      "\n",
      "Epoch 66: LearningRateScheduler setting learning rate to 0.0010000000000000002.\n",
      "Epoch 66/100\n",
      "1244/1250 [============================>.] - ETA: 0s - loss: 2.5935 - accuracy: 0.6775\n",
      "Epoch 66: val_accuracy did not improve from 0.55390\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 2.5937 - accuracy: 0.6775 - val_loss: 2.8311 - val_accuracy: 0.5430 - lr: 0.0010\n",
      "\n",
      "Epoch 67: LearningRateScheduler setting learning rate to 0.0010000000000000002.\n",
      "Epoch 67/100\n",
      "1241/1250 [============================>.] - ETA: 0s - loss: 2.7545 - accuracy: 0.6560\n",
      "Epoch 67: val_accuracy did not improve from 0.55390\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 2.7557 - accuracy: 0.6555 - val_loss: 2.9346 - val_accuracy: 0.5363 - lr: 0.0010\n",
      "\n",
      "Epoch 68: LearningRateScheduler setting learning rate to 0.0010000000000000002.\n",
      "Epoch 68/100\n",
      "1238/1250 [============================>.] - ETA: 0s - loss: 2.8920 - accuracy: 0.6348\n",
      "Epoch 68: val_accuracy did not improve from 0.55390\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 2.8925 - accuracy: 0.6349 - val_loss: 3.0175 - val_accuracy: 0.5298 - lr: 0.0010\n",
      "\n",
      "Epoch 69: LearningRateScheduler setting learning rate to 0.0010000000000000002.\n",
      "Epoch 69/100\n",
      "1245/1250 [============================>.] - ETA: 0s - loss: 3.0043 - accuracy: 0.6100\n",
      "Epoch 69: val_accuracy did not improve from 0.55390\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 3.0047 - accuracy: 0.6101 - val_loss: 3.0972 - val_accuracy: 0.5236 - lr: 0.0010\n",
      "\n",
      "Epoch 70: LearningRateScheduler setting learning rate to 0.0010000000000000002.\n",
      "Epoch 70/100\n",
      "1237/1250 [============================>.] - ETA: 0s - loss: 3.0990 - accuracy: 0.5947\n",
      "Epoch 70: val_accuracy did not improve from 0.55390\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 3.0995 - accuracy: 0.5946 - val_loss: 3.1617 - val_accuracy: 0.5152 - lr: 0.0010\n",
      "\n",
      "Epoch 71: LearningRateScheduler setting learning rate to 0.0010000000000000002.\n",
      "Epoch 71/100\n",
      "1237/1250 [============================>.] - ETA: 0s - loss: 3.1761 - accuracy: 0.5778\n",
      "Epoch 71: val_accuracy did not improve from 0.55390\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 3.1770 - accuracy: 0.5777 - val_loss: 3.2022 - val_accuracy: 0.5064 - lr: 0.0010\n",
      "\n",
      "Epoch 72: LearningRateScheduler setting learning rate to 0.0010000000000000002.\n",
      "Epoch 72/100\n",
      "1237/1250 [============================>.] - ETA: 0s - loss: 3.2472 - accuracy: 0.5603\n",
      "Epoch 72: val_accuracy did not improve from 0.55390\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 3.2482 - accuracy: 0.5600 - val_loss: 3.2512 - val_accuracy: 0.4992 - lr: 0.0010\n",
      "\n",
      "Epoch 73: LearningRateScheduler setting learning rate to 0.0010000000000000002.\n",
      "Epoch 73/100\n",
      "1245/1250 [============================>.] - ETA: 0s - loss: 3.3079 - accuracy: 0.5502\n",
      "Epoch 73: val_accuracy did not improve from 0.55390\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 3.3085 - accuracy: 0.5500 - val_loss: 3.2976 - val_accuracy: 0.4920 - lr: 0.0010\n",
      "\n",
      "Epoch 74: LearningRateScheduler setting learning rate to 0.0010000000000000002.\n",
      "Epoch 74/100\n",
      "1241/1250 [============================>.] - ETA: 0s - loss: 3.3603 - accuracy: 0.5353\n",
      "Epoch 74: val_accuracy did not improve from 0.55390\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 3.3589 - accuracy: 0.5355 - val_loss: 3.3411 - val_accuracy: 0.4844 - lr: 0.0010\n",
      "\n",
      "Epoch 75: LearningRateScheduler setting learning rate to 0.0010000000000000002.\n",
      "Epoch 75/100\n",
      "1247/1250 [============================>.] - ETA: 0s - loss: 3.4014 - accuracy: 0.5265\n",
      "Epoch 75: val_accuracy did not improve from 0.55390\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 3.4014 - accuracy: 0.5265 - val_loss: 3.3647 - val_accuracy: 0.4799 - lr: 0.0010\n",
      "\n",
      "Epoch 76: LearningRateScheduler setting learning rate to 0.0010000000000000002.\n",
      "Epoch 76/100\n",
      "1244/1250 [============================>.] - ETA: 0s - loss: 3.4412 - accuracy: 0.5141\n",
      "Epoch 76: val_accuracy did not improve from 0.55390\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 3.4419 - accuracy: 0.5139 - val_loss: 3.3937 - val_accuracy: 0.4764 - lr: 0.0010\n",
      "\n",
      "Epoch 77: LearningRateScheduler setting learning rate to 0.0010000000000000002.\n",
      "Epoch 77/100\n",
      "1239/1250 [============================>.] - ETA: 0s - loss: 3.4789 - accuracy: 0.5065\n",
      "Epoch 77: val_accuracy did not improve from 0.55390\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 3.4789 - accuracy: 0.5064 - val_loss: 3.4236 - val_accuracy: 0.4687 - lr: 0.0010\n",
      "\n",
      "Epoch 78: LearningRateScheduler setting learning rate to 0.0010000000000000002.\n",
      "Epoch 78/100\n",
      "1243/1250 [============================>.] - ETA: 0s - loss: 3.5103 - accuracy: 0.4975\n",
      "Epoch 78: val_accuracy did not improve from 0.55390\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 3.5109 - accuracy: 0.4972 - val_loss: 3.4471 - val_accuracy: 0.4665 - lr: 0.0010\n",
      "\n",
      "Epoch 79: LearningRateScheduler setting learning rate to 0.0010000000000000002.\n",
      "Epoch 79/100\n",
      "1239/1250 [============================>.] - ETA: 0s - loss: 3.5396 - accuracy: 0.4917\n",
      "Epoch 79: val_accuracy did not improve from 0.55390\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 3.5392 - accuracy: 0.4918 - val_loss: 3.4760 - val_accuracy: 0.4604 - lr: 0.0010\n",
      "\n",
      "Epoch 80: LearningRateScheduler setting learning rate to 0.0010000000000000002.\n",
      "Epoch 80/100\n",
      "1243/1250 [============================>.] - ETA: 0s - loss: 3.5636 - accuracy: 0.4859\n",
      "Epoch 80: val_accuracy did not improve from 0.55390\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 3.5644 - accuracy: 0.4856 - val_loss: 3.4888 - val_accuracy: 0.4540 - lr: 0.0010\n",
      "\n",
      "Epoch 81: LearningRateScheduler setting learning rate to 0.0010000000000000002.\n",
      "Epoch 81/100\n",
      "1245/1250 [============================>.] - ETA: 0s - loss: 3.5861 - accuracy: 0.4794\n",
      "Epoch 81: val_accuracy did not improve from 0.55390\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 3.5862 - accuracy: 0.4796 - val_loss: 3.5021 - val_accuracy: 0.4527 - lr: 0.0010\n",
      "\n",
      "Epoch 82: LearningRateScheduler setting learning rate to 0.0010000000000000002.\n",
      "Epoch 82/100\n",
      "1248/1250 [============================>.] - ETA: 0s - loss: 3.6066 - accuracy: 0.4750\n",
      "Epoch 82: val_accuracy did not improve from 0.55390\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 3.6067 - accuracy: 0.4749 - val_loss: 3.5239 - val_accuracy: 0.4510 - lr: 0.0010\n",
      "\n",
      "Epoch 83: LearningRateScheduler setting learning rate to 0.0010000000000000002.\n",
      "Epoch 83/100\n",
      "1242/1250 [============================>.] - ETA: 0s - loss: 3.6277 - accuracy: 0.4682\n",
      "Epoch 83: val_accuracy did not improve from 0.55390\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 3.6281 - accuracy: 0.4680 - val_loss: 3.5350 - val_accuracy: 0.4485 - lr: 0.0010\n",
      "\n",
      "Epoch 84: LearningRateScheduler setting learning rate to 0.0010000000000000002.\n",
      "Epoch 84/100\n",
      "1242/1250 [============================>.] - ETA: 0s - loss: 3.6432 - accuracy: 0.4634\n",
      "Epoch 84: val_accuracy did not improve from 0.55390\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 3.6430 - accuracy: 0.4633 - val_loss: 3.5506 - val_accuracy: 0.4415 - lr: 0.0010\n",
      "\n",
      "Epoch 85: LearningRateScheduler setting learning rate to 0.0010000000000000002.\n",
      "Epoch 85/100\n",
      "1248/1250 [============================>.] - ETA: 0s - loss: 3.6579 - accuracy: 0.4602\n",
      "Epoch 85: val_accuracy did not improve from 0.55390\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 3.6578 - accuracy: 0.4604 - val_loss: 3.5621 - val_accuracy: 0.4418 - lr: 0.0010\n",
      "\n",
      "Epoch 86: LearningRateScheduler setting learning rate to 0.0010000000000000002.\n",
      "Epoch 86/100\n",
      "1237/1250 [============================>.] - ETA: 0s - loss: 3.6759 - accuracy: 0.4541\n",
      "Epoch 86: val_accuracy did not improve from 0.55390\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 3.6764 - accuracy: 0.4541 - val_loss: 3.5768 - val_accuracy: 0.4377 - lr: 0.0010\n",
      "\n",
      "Epoch 87: LearningRateScheduler setting learning rate to 0.0010000000000000002.\n",
      "Epoch 87/100\n",
      "1247/1250 [============================>.] - ETA: 0s - loss: 3.6890 - accuracy: 0.4536\n",
      "Epoch 87: val_accuracy did not improve from 0.55390\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 3.6888 - accuracy: 0.4536 - val_loss: 3.5890 - val_accuracy: 0.4336 - lr: 0.0010\n",
      "\n",
      "Epoch 88: LearningRateScheduler setting learning rate to 0.0010000000000000002.\n",
      "Epoch 88/100\n",
      "1243/1250 [============================>.] - ETA: 0s - loss: 3.7015 - accuracy: 0.4501\n",
      "Epoch 88: val_accuracy did not improve from 0.55390\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 3.7010 - accuracy: 0.4500 - val_loss: 3.5989 - val_accuracy: 0.4296 - lr: 0.0010\n",
      "\n",
      "Epoch 89: LearningRateScheduler setting learning rate to 0.0010000000000000002.\n",
      "Epoch 89/100\n",
      "1247/1250 [============================>.] - ETA: 0s - loss: 3.7104 - accuracy: 0.4459\n",
      "Epoch 89: val_accuracy did not improve from 0.55390\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 3.7103 - accuracy: 0.4457 - val_loss: 3.6102 - val_accuracy: 0.4289 - lr: 0.0010\n",
      "\n",
      "Epoch 90: LearningRateScheduler setting learning rate to 0.0010000000000000002.\n",
      "Epoch 90/100\n",
      "1238/1250 [============================>.] - ETA: 0s - loss: 3.7230 - accuracy: 0.4420\n",
      "Epoch 90: val_accuracy did not improve from 0.55390\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 3.7232 - accuracy: 0.4422 - val_loss: 3.6135 - val_accuracy: 0.4271 - lr: 0.0010\n",
      "\n",
      "Epoch 91: LearningRateScheduler setting learning rate to 0.00010000000000000003.\n",
      "Epoch 91/100\n",
      "1244/1250 [============================>.] - ETA: 0s - loss: 3.9668 - accuracy: 0.4579\n",
      "Epoch 91: val_accuracy did not improve from 0.55390\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 3.9679 - accuracy: 0.4580 - val_loss: 4.1789 - val_accuracy: 0.4266 - lr: 1.0000e-04\n",
      "\n",
      "Epoch 92: LearningRateScheduler setting learning rate to 0.00010000000000000003.\n",
      "Epoch 92/100\n",
      "1250/1250 [==============================] - ETA: 0s - loss: 4.4995 - accuracy: 0.4527\n",
      "Epoch 92: val_accuracy did not improve from 0.55390\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 4.4995 - accuracy: 0.4527 - val_loss: 4.6799 - val_accuracy: 0.4232 - lr: 1.0000e-04\n",
      "\n",
      "Epoch 93: LearningRateScheduler setting learning rate to 0.00010000000000000003.\n",
      "Epoch 93/100\n",
      "1249/1250 [============================>.] - ETA: 0s - loss: 4.9698 - accuracy: 0.4516\n",
      "Epoch 93: val_accuracy did not improve from 0.55390\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 4.9699 - accuracy: 0.4518 - val_loss: 5.1185 - val_accuracy: 0.4196 - lr: 1.0000e-04\n",
      "\n",
      "Epoch 94: LearningRateScheduler setting learning rate to 0.00010000000000000003.\n",
      "Epoch 94/100\n",
      "1240/1250 [============================>.] - ETA: 0s - loss: 5.3703 - accuracy: 0.4479\n",
      "Epoch 94: val_accuracy did not improve from 0.55390\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 5.3715 - accuracy: 0.4477 - val_loss: 5.4956 - val_accuracy: 0.4155 - lr: 1.0000e-04\n",
      "\n",
      "Epoch 95: LearningRateScheduler setting learning rate to 0.00010000000000000003.\n",
      "Epoch 95/100\n",
      "1246/1250 [============================>.] - ETA: 0s - loss: 5.7002 - accuracy: 0.4397\n",
      "Epoch 95: val_accuracy did not improve from 0.55390\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 5.7006 - accuracy: 0.4397 - val_loss: 5.8026 - val_accuracy: 0.4116 - lr: 1.0000e-04\n",
      "\n",
      "Epoch 96: LearningRateScheduler setting learning rate to 0.00010000000000000003.\n",
      "Epoch 96/100\n",
      "1235/1250 [============================>.] - ETA: 0s - loss: 5.9684 - accuracy: 0.4357\n",
      "Epoch 96: val_accuracy did not improve from 0.55390\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 5.9696 - accuracy: 0.4358 - val_loss: 6.0512 - val_accuracy: 0.4059 - lr: 1.0000e-04\n",
      "\n",
      "Epoch 97: LearningRateScheduler setting learning rate to 0.00010000000000000003.\n",
      "Epoch 97/100\n",
      "1240/1250 [============================>.] - ETA: 0s - loss: 6.1846 - accuracy: 0.4299\n",
      "Epoch 97: val_accuracy did not improve from 0.55390\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 6.1855 - accuracy: 0.4294 - val_loss: 6.2511 - val_accuracy: 0.3997 - lr: 1.0000e-04\n",
      "\n",
      "Epoch 98: LearningRateScheduler setting learning rate to 0.00010000000000000003.\n",
      "Epoch 98/100\n",
      "1248/1250 [============================>.] - ETA: 0s - loss: 6.3564 - accuracy: 0.4256\n",
      "Epoch 98: val_accuracy did not improve from 0.55390\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 6.3566 - accuracy: 0.4255 - val_loss: 6.4101 - val_accuracy: 0.3969 - lr: 1.0000e-04\n",
      "\n",
      "Epoch 99: LearningRateScheduler setting learning rate to 0.00010000000000000003.\n",
      "Epoch 99/100\n",
      "1236/1250 [============================>.] - ETA: 0s - loss: 6.4924 - accuracy: 0.4184\n",
      "Epoch 99: val_accuracy did not improve from 0.55390\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 6.4930 - accuracy: 0.4182 - val_loss: 6.5365 - val_accuracy: 0.3920 - lr: 1.0000e-04\n",
      "\n",
      "Epoch 100: LearningRateScheduler setting learning rate to 0.00010000000000000003.\n",
      "Epoch 100/100\n",
      "1246/1250 [============================>.] - ETA: 0s - loss: 6.5990 - accuracy: 0.4143\n",
      "Epoch 100: val_accuracy did not improve from 0.55390\n",
      "1250/1250 [==============================] - 6s 4ms/step - loss: 6.5992 - accuracy: 0.4145 - val_loss: 6.6344 - val_accuracy: 0.3908 - lr: 1.0000e-04\n"
     ]
    }
   ],
   "source": [
    "history1 = meta_learning_initialisation.fit(X_train,\n",
    "    y_train1,\n",
    "    validation_data=(X_val, y_val1),\n",
    "    batch_size=32,\n",
    "    epochs=100,verbose=1,callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rZEMwkUV3Kru"
   },
   "outputs": [],
   "source": [
    "temp=tf.keras.models.load_model('./drive/MyDrive/reptiles_embeddings1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cPQdrYkE3Sdj",
    "outputId": "15e1cedf-ded5-4ed5-87fa-5d5ba9ee485a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 5s 6ms/step - loss: 2.1994 - accuracy: 0.5539\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.1993916034698486, 0.5539000034332275]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp.evaluate(X_val,y_val1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9wanvo9n5R8g",
    "outputId": "e4830427-dae0-4427-fdf4-602d6ca2b92c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 30s 3ms/step - loss: 2.1994 - accuracy: 0.5539\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.199389696121216, 0.5539000034332275]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp.evaluate(X_val,y_val1,batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BeHyhzK_3XWj"
   },
   "outputs": [],
   "source": [
    "\n",
    "def display_learning_curves(history):\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "    ax1.plot(history.history[\"loss\"])\n",
    "    ax1.plot(history.history[\"val_loss\"])\n",
    "    ax1.legend([\"train\", \"valid\"], loc=\"upper right\")\n",
    "    ax1.set_xlabel(\"Epochs\")\n",
    "    ax1.set_ylabel(\"Loss\")\n",
    "\n",
    "    ax2.plot(history.history[\"accuracy\"])\n",
    "    ax2.plot(history.history[\"val_accuracy\"])\n",
    "    ax2.legend([\"train\", \"valid\"], loc=\"upper right\")\n",
    "    ax2.set_xlabel(\"Epochs\")\n",
    "    ax2.set_ylabel(\"Accuracy\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 334
    },
    "id": "vWByG-fl3fPw",
    "outputId": "2364af19-7e80-4e7b-8aee-07c0fb41a49c"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_learning_curves(history1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DiaQm0Ww3kzz"
   },
   "outputs": [],
   "source": [
    "test_dataset=pd.read_csv('./drive/MyDrive/test_representation.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "id": "Vwb8s1qG4WNW",
    "outputId": "671f25a5-834f-4488-aeaa-b4f184e0a219"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-674f3d9e-fb22-4a5c-8f56-3b633e73c685\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>2039</th>\n",
       "      <th>2040</th>\n",
       "      <th>2041</th>\n",
       "      <th>2042</th>\n",
       "      <th>2043</th>\n",
       "      <th>2044</th>\n",
       "      <th>2045</th>\n",
       "      <th>2046</th>\n",
       "      <th>2047</th>\n",
       "      <th>category_class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.029340</td>\n",
       "      <td>0.959396</td>\n",
       "      <td>0.032180</td>\n",
       "      <td>0.007271</td>\n",
       "      <td>0.505135</td>\n",
       "      <td>0.047305</td>\n",
       "      <td>0.046234</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.160508</td>\n",
       "      <td>0.108298</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.031925</td>\n",
       "      <td>0.008031</td>\n",
       "      <td>0.129452</td>\n",
       "      <td>0.450996</td>\n",
       "      <td>0.004531</td>\n",
       "      <td>0.008310</td>\n",
       "      <td>65.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.018695</td>\n",
       "      <td>0.131584</td>\n",
       "      <td>0.000580</td>\n",
       "      <td>0.265691</td>\n",
       "      <td>0.075151</td>\n",
       "      <td>0.005068</td>\n",
       "      <td>0.164080</td>\n",
       "      <td>0.603745</td>\n",
       "      <td>0.167325</td>\n",
       "      <td>0.076654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.029152</td>\n",
       "      <td>0.214970</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.043662</td>\n",
       "      <td>0.057358</td>\n",
       "      <td>0.093535</td>\n",
       "      <td>0.103441</td>\n",
       "      <td>0.011182</td>\n",
       "      <td>0.052681</td>\n",
       "      <td>970.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.008372</td>\n",
       "      <td>0.236403</td>\n",
       "      <td>0.014399</td>\n",
       "      <td>0.041888</td>\n",
       "      <td>0.024327</td>\n",
       "      <td>0.042268</td>\n",
       "      <td>0.086519</td>\n",
       "      <td>0.659389</td>\n",
       "      <td>0.053838</td>\n",
       "      <td>0.071150</td>\n",
       "      <td>...</td>\n",
       "      <td>0.135679</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.235405</td>\n",
       "      <td>0.324949</td>\n",
       "      <td>0.021558</td>\n",
       "      <td>0.093964</td>\n",
       "      <td>0.012577</td>\n",
       "      <td>0.001841</td>\n",
       "      <td>0.246740</td>\n",
       "      <td>230.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.169781</td>\n",
       "      <td>0.011046</td>\n",
       "      <td>0.054933</td>\n",
       "      <td>1.199504</td>\n",
       "      <td>0.027333</td>\n",
       "      <td>0.123167</td>\n",
       "      <td>0.185943</td>\n",
       "      <td>0.762185</td>\n",
       "      <td>0.053278</td>\n",
       "      <td>0.096448</td>\n",
       "      <td>...</td>\n",
       "      <td>0.148584</td>\n",
       "      <td>0.007761</td>\n",
       "      <td>0.816408</td>\n",
       "      <td>0.135283</td>\n",
       "      <td>0.010906</td>\n",
       "      <td>0.306520</td>\n",
       "      <td>1.403771</td>\n",
       "      <td>0.235088</td>\n",
       "      <td>0.000832</td>\n",
       "      <td>809.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.050206</td>\n",
       "      <td>0.857661</td>\n",
       "      <td>0.150167</td>\n",
       "      <td>0.052228</td>\n",
       "      <td>0.291346</td>\n",
       "      <td>0.206538</td>\n",
       "      <td>0.145282</td>\n",
       "      <td>0.104864</td>\n",
       "      <td>0.128827</td>\n",
       "      <td>0.380020</td>\n",
       "      <td>...</td>\n",
       "      <td>0.163841</td>\n",
       "      <td>0.060867</td>\n",
       "      <td>0.049904</td>\n",
       "      <td>0.576424</td>\n",
       "      <td>0.294275</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.080127</td>\n",
       "      <td>0.016386</td>\n",
       "      <td>0.423166</td>\n",
       "      <td>516.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49995</th>\n",
       "      <td>0.002047</td>\n",
       "      <td>0.012996</td>\n",
       "      <td>0.043287</td>\n",
       "      <td>0.194361</td>\n",
       "      <td>0.340890</td>\n",
       "      <td>0.095029</td>\n",
       "      <td>0.137551</td>\n",
       "      <td>0.295975</td>\n",
       "      <td>0.000093</td>\n",
       "      <td>0.009103</td>\n",
       "      <td>...</td>\n",
       "      <td>0.101937</td>\n",
       "      <td>0.010900</td>\n",
       "      <td>0.057891</td>\n",
       "      <td>0.205408</td>\n",
       "      <td>0.010592</td>\n",
       "      <td>1.026036</td>\n",
       "      <td>0.037057</td>\n",
       "      <td>0.005353</td>\n",
       "      <td>1.050666</td>\n",
       "      <td>283.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>0.028609</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>0.154415</td>\n",
       "      <td>1.012217</td>\n",
       "      <td>0.000856</td>\n",
       "      <td>0.078721</td>\n",
       "      <td>0.403259</td>\n",
       "      <td>0.151994</td>\n",
       "      <td>0.003251</td>\n",
       "      <td>0.013472</td>\n",
       "      <td>...</td>\n",
       "      <td>0.033227</td>\n",
       "      <td>0.268508</td>\n",
       "      <td>0.017099</td>\n",
       "      <td>0.000700</td>\n",
       "      <td>0.121087</td>\n",
       "      <td>0.025590</td>\n",
       "      <td>1.007298</td>\n",
       "      <td>0.013507</td>\n",
       "      <td>0.169299</td>\n",
       "      <td>26.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>0.031759</td>\n",
       "      <td>0.009449</td>\n",
       "      <td>0.047738</td>\n",
       "      <td>0.047382</td>\n",
       "      <td>0.949164</td>\n",
       "      <td>0.059264</td>\n",
       "      <td>0.016978</td>\n",
       "      <td>0.000756</td>\n",
       "      <td>0.020112</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.354629</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.229080</td>\n",
       "      <td>0.020603</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.809951</td>\n",
       "      <td>0.017986</td>\n",
       "      <td>0.058925</td>\n",
       "      <td>0.459367</td>\n",
       "      <td>232.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>0.008271</td>\n",
       "      <td>0.562913</td>\n",
       "      <td>0.022252</td>\n",
       "      <td>0.110028</td>\n",
       "      <td>0.601711</td>\n",
       "      <td>0.062303</td>\n",
       "      <td>0.476841</td>\n",
       "      <td>0.167790</td>\n",
       "      <td>0.086034</td>\n",
       "      <td>0.083120</td>\n",
       "      <td>...</td>\n",
       "      <td>0.269349</td>\n",
       "      <td>0.010813</td>\n",
       "      <td>0.312314</td>\n",
       "      <td>0.175110</td>\n",
       "      <td>0.289860</td>\n",
       "      <td>0.014819</td>\n",
       "      <td>0.304625</td>\n",
       "      <td>0.050381</td>\n",
       "      <td>0.012309</td>\n",
       "      <td>982.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>0.001101</td>\n",
       "      <td>0.023040</td>\n",
       "      <td>0.043784</td>\n",
       "      <td>0.014679</td>\n",
       "      <td>0.292485</td>\n",
       "      <td>0.303807</td>\n",
       "      <td>0.093233</td>\n",
       "      <td>0.168107</td>\n",
       "      <td>0.040244</td>\n",
       "      <td>0.089973</td>\n",
       "      <td>...</td>\n",
       "      <td>0.156524</td>\n",
       "      <td>0.038731</td>\n",
       "      <td>0.110496</td>\n",
       "      <td>0.076927</td>\n",
       "      <td>0.020674</td>\n",
       "      <td>0.203182</td>\n",
       "      <td>0.361507</td>\n",
       "      <td>0.031252</td>\n",
       "      <td>0.029597</td>\n",
       "      <td>355.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50000 rows × 2049 columns</p>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-674f3d9e-fb22-4a5c-8f56-3b633e73c685')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-674f3d9e-fb22-4a5c-8f56-3b633e73c685 button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-674f3d9e-fb22-4a5c-8f56-3b633e73c685');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "              0         1         2         3         4         5         6  \\\n",
       "0      0.000000  0.000000  0.029340  0.959396  0.032180  0.007271  0.505135   \n",
       "1      0.018695  0.131584  0.000580  0.265691  0.075151  0.005068  0.164080   \n",
       "2      0.008372  0.236403  0.014399  0.041888  0.024327  0.042268  0.086519   \n",
       "3      0.169781  0.011046  0.054933  1.199504  0.027333  0.123167  0.185943   \n",
       "4      0.050206  0.857661  0.150167  0.052228  0.291346  0.206538  0.145282   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "49995  0.002047  0.012996  0.043287  0.194361  0.340890  0.095029  0.137551   \n",
       "49996  0.028609  0.000027  0.154415  1.012217  0.000856  0.078721  0.403259   \n",
       "49997  0.031759  0.009449  0.047738  0.047382  0.949164  0.059264  0.016978   \n",
       "49998  0.008271  0.562913  0.022252  0.110028  0.601711  0.062303  0.476841   \n",
       "49999  0.001101  0.023040  0.043784  0.014679  0.292485  0.303807  0.093233   \n",
       "\n",
       "              7         8         9  ...      2039      2040      2041  \\\n",
       "0      0.047305  0.046234  0.000000  ...  0.160508  0.108298  0.000000   \n",
       "1      0.603745  0.167325  0.076654  ...  0.029152  0.214970  0.000000   \n",
       "2      0.659389  0.053838  0.071150  ...  0.135679  0.000000  0.235405   \n",
       "3      0.762185  0.053278  0.096448  ...  0.148584  0.007761  0.816408   \n",
       "4      0.104864  0.128827  0.380020  ...  0.163841  0.060867  0.049904   \n",
       "...         ...       ...       ...  ...       ...       ...       ...   \n",
       "49995  0.295975  0.000093  0.009103  ...  0.101937  0.010900  0.057891   \n",
       "49996  0.151994  0.003251  0.013472  ...  0.033227  0.268508  0.017099   \n",
       "49997  0.000756  0.020112  0.000000  ...  0.354629  0.000000  0.229080   \n",
       "49998  0.167790  0.086034  0.083120  ...  0.269349  0.010813  0.312314   \n",
       "49999  0.168107  0.040244  0.089973  ...  0.156524  0.038731  0.110496   \n",
       "\n",
       "           2042      2043      2044      2045      2046      2047  \\\n",
       "0      0.031925  0.008031  0.129452  0.450996  0.004531  0.008310   \n",
       "1      0.043662  0.057358  0.093535  0.103441  0.011182  0.052681   \n",
       "2      0.324949  0.021558  0.093964  0.012577  0.001841  0.246740   \n",
       "3      0.135283  0.010906  0.306520  1.403771  0.235088  0.000832   \n",
       "4      0.576424  0.294275  0.000000  0.080127  0.016386  0.423166   \n",
       "...         ...       ...       ...       ...       ...       ...   \n",
       "49995  0.205408  0.010592  1.026036  0.037057  0.005353  1.050666   \n",
       "49996  0.000700  0.121087  0.025590  1.007298  0.013507  0.169299   \n",
       "49997  0.020603  0.000000  0.809951  0.017986  0.058925  0.459367   \n",
       "49998  0.175110  0.289860  0.014819  0.304625  0.050381  0.012309   \n",
       "49999  0.076927  0.020674  0.203182  0.361507  0.031252  0.029597   \n",
       "\n",
       "       category_class  \n",
       "0                65.0  \n",
       "1               970.0  \n",
       "2               230.0  \n",
       "3               809.0  \n",
       "4               516.0  \n",
       "...               ...  \n",
       "49995           283.0  \n",
       "49996            26.0  \n",
       "49997           232.0  \n",
       "49998           982.0  \n",
       "49999           355.0  \n",
       "\n",
       "[50000 rows x 2049 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fSk6SGuO4YXd"
   },
   "outputs": [],
   "source": [
    "x_test=test_dataset.iloc[:,:-1].values\n",
    "y_test=test_dataset.iloc[:,-1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_kDEVB-64aY5"
   },
   "outputs": [],
   "source": [
    "y_test=y_test.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2K9GXeNX4cMU"
   },
   "outputs": [],
   "source": [
    "y_test1=np.eye(1000)[y_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DOCmcgBa5LnX",
    "outputId": "765df377-47ae-4c70-b13b-67244248b9d1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 2048)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "emmX2hcA4eJA",
    "outputId": "84b16f7b-9a1c-42cb-ba38-9b3d633b09b1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1563/1563 [==============================] - 4s 3ms/step - loss: 3.7264 - accuracy: 0.2675\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[3.726386785507202, 0.267520010471344]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp.evaluate(x_test,y_test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WENGkNOX6ApB",
    "outputId": "13a6a558-3b97-49ec-ad1f-531a52d4bc12"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000/50000 [==============================] - 116s 2ms/step - loss: 3.7264 - accuracy: 0.2675\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[3.7263801097869873, 0.267520010471344]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp.evaluate(x_test,y_test1,batch_size=1)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "reptile_AV_codebase_040422.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
